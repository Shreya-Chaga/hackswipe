[
  {
    "title": "Raspberry Pi Music Player",
    "summary": "The Raspberry Pi Music Player is a versatile audio streaming solution that leverages the capabilities of Raspberry Pi to play music from various sources, including local files and online platforms like YouTube Music. By integrating Python scripts and shell commands, the player provides an accessible interface for users to manage and enjoy their music seamlessly.",
    "description": "**What it does**\nThe Raspberry Pi Music Player is a voice-controlled music system that enables hands-free music playback through Siri integration. Here's what it can do: Voice Activation: Play music by simply saying \"Hey Siri, play my night music\"\nDual Music Sources: Local playback from USB storage (10 pre-loaded songs)\nCloud streaming from YouTube Music Automatic Timer: Music stops automatically after a set duration (perfect for sleep listening)\nHands-Free Operation: No need to touch your phone or computer\nRemote Control: Uses SSH commands triggered by iOS Shortcuts app\n\n**Inspiration**\nEach night I‚Äôd repeat the same annoying process of unlock‚ÄÇphone, connect to Bluetooth speaker, open my music app, find list of music etc. This was a process of around 5-6 steps, just to get music playing, and I didn‚Äôt like having the brightly lit screen of my phone while trying to drift off to sleep. I said to myself: ‚ÄúThere has got to be an easier way.‚Äù I wanted to make a tool that would be useful and solve an actual‚ÄÇproblem in my life. At that point I‚ÄÇknew what could solve it: A voice-controlled music player using a Raspberry Pi that plays music with just a simple Siri command.Yeah, instead of multiple steps we are now reduced to only one voice direction.\n\n**How we built it**\nHardware Setup\nThe physical setup consists of:\nRaspberry Pi 3 ‚Üí Audio Cable ‚Üí Amplifier ‚Üí Wired Speaker\nInitial attempt: I tried using a Bluetooth speaker for wireless convenience, but the connection kept dropping. Solution: Switched to a wired setup for stable, reliable audio output.\nSoftware Implementation\nStep 1: Local Music Library\nCreated a bash script for playing local music files:\nbash#!/bin/bash\ntimeout 1800 cvlc --play-and-exit /home/pi/Music/*.mp3 Copied 10 favorite songs via USB\nRenamed files systematically: 01.mp3, 02.mp3, ..., 10.mp3 to avoid typos\nUsed timeout command for automatic shutoff after 30 minutes (( 30\\ times 60 = 1800 \\ ) seconds) Step 2: YouTube Music Integration\nCreated a Python script (yt-music-tunes.py) for cloud streaming:\npython#!/usr/bin/env python3\nimport s\n\n**Challenges**\nBluetooth Connectivity Issues\nProblem: Bluetooth speaker randomly disconnected, even when appearing connected.\nSolution: Switched to wired connection. Sometimes the simpler solution is the better one‚Äîstability beats wireless convenience.\nFixed Duration Settings\nProblem: Changing the timer duration required manually editing code. To change from 30 to 45 minutes, I had to: Open the script\nChange 1800 to ( 45 \\times 60 = 2700 )\nSave and exit Impact: Not convenient for varying listening times. YouTube Download Lag\nProblem: Streaming from YouTube causes 10-20 second startup delay while audio downloads.\nTrade-off: Accepted this as the cost of accessing unlimited music library vs. instant playback from local files.\nManual URL Entry Per Song\nProblem: Each different song requires editing the Python\n\n**Accomplishments**\nSuccessfully reduced complexity: Transformed 5-6 phone operations into a single voice command.\nAchieved true hands-free control: No need to touch phone or computer to play music.\nBuilt a hybrid system: Combined local storage (instant playback) with cloud streaming (unlimited variety).\nImplemented automatic timer: Perfect for nighttime listening without worrying about manually stopping music.\nLearned hardware integration: Successfully connected and configured audio hardware components.\nMastered remote execution: Integrated multiple technologies (Raspberry Pi, iOS, SSH, Python, bash) into one cohesive system.\nCreated a practical solution: The system works reliably and has become part of my daily routine. The most rewarding moment was the first time I said \"Hey Siri, play my night music\" and\n\n**What we learned**\nTechnical Skills Hardware Integration: How to connect audio components and work with amplifiers\nBash Scripting: Writing shell scripts for system automation\nPython Programming: Using subprocess module and working with command-line tools\nNetwork Communication: Setting up SSH connections and executing remote commands\nAPI Integration: Connecting iOS Shortcuts with Raspberry Pi via SSH Problem-Solving Approaches Flexibility is key: When Bluetooth failed, switching to wired was the right choice\nSimplicity over complexity: Sometimes the most straightforward solution (wired connection, numbered file names) works best\nTrade-offs are necessary: Accepted download lag for YouTube streaming in exchange for unlimited music access\nIterative development: Started with local playback, then added streaming f\n\n**What's next**\nDynamic Voice Timer Control\nInstead of editing code, implement Siri-based duration input: User says: \"Play music for 45 minutes\"\nShortcut extracts the number (45)\nCalculates seconds: ( 45 \\times 60 = 2700 )\nPasses as parameter to the script Interrupt Capability\nCreate a \"Stop music\" Siri command:\nbashssh pi@192.168.x.x \"killall vlc\"\nDynamic URL Input\nSiri shortcut prompts: \"What's the YouTube URL?\" User provides URL via voice or paste\nShortcut passes URL as parameter to Python script\nNo more manual code editing Full Playlist Support\nModify Python script to handle YouTube playlist URLs: Automatically parse and download all songs in playlist\nPlay in sequence\nEnable \"shuffle\" option via voice command Advanced Features Web Interface\nBuild a simple web dashboard for easier configuration: Browse\n\n",
    "prize": "Winner Emerging Talent Award",
    "techStack": "bash, ffmpeg, ios, ios-shortcuts, python, raspberry-pi-os, realvnc, ssh, vlc, youtube-music, yt-dlp",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=VIDEO_ID",
    "demo": null,
    "team": "Rimi Yoshikawa",
    "date": "2026-01-04",
    "projectUrl": "https://devpost.com/software/raspberry-pi-music-player"
  },
  {
    "title": "VoxSol",
    "summary": "TECHNICAL HIGHLIGHTS: The project features a robust tech stack including Next.js for server-side rendering, Tailwind CSS for responsive design, and Three.js for immersive 3D audio visualizations. It also integrates ElevenLabs for AI-driven voice synthesis and utilizes Solana's web3.js library for seamless blockchain interactions, ensuring a smooth user experience.",
    "description": "**What it does**\nVoxSol is a voice-first Solana wallet copilot. You can: Check your portfolio balance by speaking\nExecute real token swaps on Solana devnet\nView live price charts\nGet AI-powered responses that understand DeFi context Everything is non-custodial - you connect your own wallet and sign every transaction yourself.\n\n**Inspiration**\nManaging a crypto wallet shouldn't require staring at complex interfaces. I wanted to create something that felt as natural as asking Siri or Alexa for help - but for Solana DeFi. What if you could just say \"swap 0.5 USDC to SOL\" and have it happen?\n\n**How we built it**\nFrontend: Next.js 14 with Tailwind CSS for a sleek, dark-themed UI\nAI Chat: Gemini API processes natural language and understands DeFi commands\nVoice Input: Web Speech API for speech-to-text\nVoice Output: ElevenLabs API for natural text-to-speech responses\nBlockchain: Solana devnet with @solana/web3.js for real on-chain transactions\nWallet: Phantom, Solflare, Backpack integration via wallet adapter\nSwap Execution: Custom vault that handles USDC ‚Üî SOL swaps at market rates\n\n**What we learned**\nHow to integrate voice interfaces with Web3 applications\nBuilding AI agents that understand financial context\nWorking with Solana transactions and token transfers\n\n**What's next**\nAI Agent Allowance: Set spending limits for autonomous trading\nYield Vaults: One-click deposits into yield strategies\nMainnet deployment with Jupiter integration\n\n",
    "prize": "Winner Best Use of Solana",
    "techStack": "css, elevenlabs, gemini, gsap, next.js, phantom, react, solana, solana/web3.js, tailwind, three.js, typescript",
    "github": "https://github.com/KaranSinghBisht/VoxSol",
    "youtube": "https://www.youtube.com/watch?v=3Mows_n952M",
    "demo": "https://vox-sol.vercel.app/",
    "team": "Karan Bisht",
    "date": "2026-01-04",
    "projectUrl": "https://devpost.com/software/voxsol"
  },
  {
    "title": "ComLab",
    "summary": "ComLab is a data-driven application designed to leverage the Snowflake API for enhanced data analytics and visualization. By combining technologies like MongoDB, Next.js, and React, the project aims to provide users with an interactive platform to explore and manipulate large datasets seamlessly.",
    "description": "Inspiration\n\nSome time ago, I was playing around in an online Google game, and an idea had suddenly crossed my mind. But how would I get that idea to Google, let alone a small team or person working on that game? Getting direct feedback from end users can be messy for some developers, and I came up with this app to help mediate that process.\n\nWhat it does\n\nComLab allows for both technical and non-technical users to submit tickets to different organizations that represent developers. Users can sign up or log in via email or Google OAuth. A developer can create an organization for users to submit tickets to.\n\nUsers can submit tickets with a title, description, and an image attachment to an organization. Developers can view these tickets and triage them by priority (powered by an LLM via Snowflake API) and summarize tickets (also powered by Snowflake API). Developers can sort tickets in multiple ways as well. Developers and users can also use the Reddit Digest tool to gain insight into a specific product (especially helpful for developers). Users can upvote other users‚Äô tickets if they feel it provides value.\n\nWith the app, users can directly communicate a bug, feature request, etc. to the developer, allowing for the developer to create and innovate faster, tailoring to the users' needs. For open-source projects, non-technical people can have a say in the development process too, which would be a little hard to do through GitHub Issues and Discussions.\n\nHow I built it\n\nI used a modern technology stack:  \n\n\nFrontend: Next.js, React, TailwindCSS\nBackend: Next.js Serverless Functions\nDatabase: MongoDB Atlas (with authentication)\nAI: Snowflake Cortex AI API (using gpt-5-mini)\n\n\nChallenges I ran into\n\nSince I was new to Snowflake, I initially had trouble with network and authentication policies, but in the end, I figured out how to make everything work. Developing the prompts for the LLMs to accurately output a relevant response also took some tinkering and trial and error.\n\nWhat I learned\n\nI learned how to use Snowflake‚Äôs API, something I was especially looking forward to, particularly for connecting LLMs into my app. I also learned about time management, a great life skill. Overall, I had a lot of fun experimenting with how an LLM can be used to help developers in the product sense.\n\nWhat‚Äôs next for ComLab\n\nI hope to continue developing ComLab for both learning and community purposes. I want to add a website verification system (something a little ambitious for a solo hacker at a two-day hackathon). I also want to incorporate more AI features so developers have an easier time understanding their user base. I would like for there to be a connection to GitHub for open-source projects to have a direct pipeline with my app.\n\n\n\n        \n    Built With\n\n    javascriptmongodbnext.jsreactsnowflaketypescriptvercel\n  \n\n        \n    Try it out\n\n    \n        \n  \n  GitHub Repo\n\n        \n  \n  comlab.aadittalele.com",
    "prize": "Winner Best Use of Snowflake API",
    "techStack": "javascript, mongodb, next.js, react, snowflake, typescript, vercel",
    "github": "https://github.com/aadittalele/comlab_app",
    "youtube": "https://www.youtube.com/watch?v=KK5LTfyfq14",
    "demo": "http://comlab.aadittalele.com/",
    "team": null,
    "date": "2026-01-04",
    "projectUrl": "https://devpost.com/software/comlab-an1uem"
  },
  {
    "title": "Story.ai",
    "summary": "TECHNICAL HIGHLIGHTS: Story.ai employs ElevenLabs for realistic voice generation and Gemini for natural language processing, enabling it to craft compelling narratives. The use of MongoDB for data storage ensures efficient management of user-generated content and seamless access to a rich database of stories.",
    "description": "**Inspiration**\nI was inspired by the magic of audiobooks and theater, and wanted to create a tool that could bring stories to life automatically. Reading a story is one thing, but hearing characters speak in unique, expressive voices makes it immersive and engaging. I asked myself: ‚ÄúWhat if a PDF story could become a full audio performance, with each character voiced differently?‚Äù That idea sparked this project. I also noticed that most existing text-to-speech tools use a single voice for all characters, which makes stories feel flat and hard to follow. I wanted to explore whether AI could understand the story context and generate distinct voices for each character, creating a richer, more human-like experience.\n\n**How we built it**\nStory.ai takes a PDF story and produces a multi-character narrated audio file, all accessible through a web interface. Here‚Äôs how it works: I used Python and PyPDF2 to extract clean, structured text from PDFs.\nI integrated the Google Gemini API to analyze the story and identify who is speaking in each line, distinguishing dialogue from narration and grouping consecutive lines by speaker to ensure smooth audio.\nI used ElevenLabs‚Äô Text-to-Speech API to generate realistic, expressive voices for each character and the narrator.\nI stitched all the generated audio clips together using Python libraries like pydub, producing a seamless audiobook.\nThe web interface allows users to upload PDFs, generate audio, and play it directly in the browser.\nI stored uploaded PDFs and generated audio files in M\n\n**Challenges**\nOver-segmentation: Initially, the AI split every sentence into separate chunks, making the audio choppy. I solved this by updating prompts to group consecutive lines by the same speaker.\nVoice assignment: Ensuring each character had a distinct, expressive voice while keeping the sequence natural.\nAudio stitching: Combining multiple audio files into a seamless story without gaps or mismatched timing.\nPDF variability: Handling different PDF formats and layouts while extracting clean text.\nWeb integration: Coordinating API calls and audio playback in a user-friendly web interface. Despite these challenges, Story.ai successfully transforms static stories into immersive, multi-character audio experiences powered entirely by AI using Python, Google Gemini, ElevenLabs, MongoDB Atlas, and a web in\n\n**What we learned**\nHow to extract structured text from PDFs and prepare it for AI processing.\nHow to prompt an LLM (Google Gemini) effectively to identify speakers while grouping consecutive lines together.\nHow to generate multiple AI voices and merge them into a coherent audio narrative using ElevenLabs.\nHow to think about user experience, including pacing, dialogue flow, and listener immersion.\nHow to leverage MongoDB Atlas for storage and retrieval, ensuring scalability and reusability.\nHow to integrate all components into a web interface, allowing users to interact with the system directly.\n\n**What's next**\nScene-level visuals: Adding illustrations for each scene to enhance immersion.\nSubtitle syncing: Highlighting dialogue in real-time for accessibility and engagement.\nCharacter consistency across stories: Maintaining the same voices for recurring characters.\nEnhanced web features: Letting users share generated stories directly from the web interface.\n\n",
    "prize": "Winner Best Use of ElevenLabs",
    "techStack": "elevenlabs, gemini, mongodb",
    "github": "https://github.com/Ayushsinghal05/Story.ai",
    "youtube": "https://www.youtube.com/watch?v=SccU1kl_7ng",
    "demo": null,
    "team": "Ayush Singhal",
    "date": "2026-01-04",
    "projectUrl": "https://devpost.com/software/story-ai-9tl7xf"
  },
  {
    "title": "DiaDoc",
    "summary": "DiaDoc is a web application designed to streamline document management by leveraging the Gemini API for enhanced functionality. The project aims to simplify the process of creating, sharing, and collaborating on documents, making it easier for users to manage their documentation needs.",
    "description": "**What it does**\nDiaDoc is a tool meant to help documentation writers come up with diagrams for their project‚Äôs file structure and to help hackers understand the file structure of existing software projects. Users can manually make file diagrams with a drag and drop interface, generate a file diagram given a public existing GitHub repository link, a link to a webpage containing project documentation or given the text draft of documentation for a software project, and receive the resulting file diagram in ASCII art. Users who want to go back to their file structures later on can download a .diadoc file of their file structure draft which can be opened by DiaDoc in the Manual Builder for future edits.\n\n**Inspiration**\nA couple of months ago, Lani was installing Magma, an open-source mobile core network solution, to her computer via its quick start documentation. It took her nearly 2 weeks to install it because it mostly contained command lines and she found herself so confused about the file structure of the project. Leah and Lani have faced similar issues when referring to documentation to install software, notably when installing the Flutter framework for app development and when encountering environment variables for the first time. It made them wonder if there could be a tool to aid in understanding a project‚Äôs file structure either step-by-step in documentation or given a project‚Äôs GitHub repository.\n\n**How we built it**\nDiaDoc was built in HTML, CSS and JavaScript with the React framework and uses the Gemini API for file structure generation in its ‚ÄúFrom Link‚Äù and ‚ÄúDocumentation Text‚Äù options. It has two repositories, one that is public, attached to this submission, and contains source code that can run the web application locally, and the other is a private repository cloned off of the public one by Vercel that holds .env files and a vercel.json file for web app hosting. It uses REST API requests to converse with the Gemini API and makes heavy use of the lucide-react, axios, and react-dom libraries and packages for clean icons, HTTP client, and document object model functions for React.\n\n**Challenges**\nBuilding the app was one thing, hosting the app was a whole other issue. In the past both of us have only ever hosted static websites, and with tools like GitHub Pages and surge.sh, there was hardly ever an issue getting our projects up. With an existing web application, Vercel creates a clone of its repository (but private) to hold additional information needed to host it publicly. While this is a great security practice, it inadvertently caused problems. Two notable errors include one where Gemini would work fine locally but output a single error supposedly caused by the server while hosted. It turned out that we only had the API key locally so adding it to Vercel‚Äôs settings fixed it. The other error came from a library (axios) that Leah was confident we had in our project but our produc\n\n**Accomplishments**\nMaking a custom file extension! Originally, we thought that file extensions had to have a direct correlation to how they were being used, but it turns out most files boil down to txt files or c files functionally and that file extensions can be thought of an addition to a file‚Äôs name. It was also satisfying to finally work with React and Vercel, as we have developed apps before in nearly any other framework or tool (Dart/Flutter, pure HTML/CSS/JS, Python/Flask, Java/Android etc.) but never what was mostly recommended to us: React. Finally getting to use it made us realize why it was recommended to us in the first place.\n\n**What we learned**\nLearned how to make a custom file extension and what file extensions really are\nExplored ways on how to incorporate the Gemini API into a web application\nFigured out how to create a drag-and-drop user interface for diagramming\nHow to create and send a file to one‚Äôs Downloads folder\n\n**What's next**\nThe file structure generation output is pretty inaccurate (for example, by giving DiaDoc a link to its own public repository, it will generate a file structure output a lot closer to more popular repositories from an organization also called DiaDoc) so figuring out ways on how to fine-tune the API for ASCII diagramming specific purposes would be our next main goal. We also came across a thread on Reddit where people shared similar thoughts on wanting a tool like this: https://www.reddit.com/r/datacurator/comments/1dzqtlp/what_tool_to_visualise_folder_structure/ and people have mentioned desiring features like being able to add metadata in addition to files and folder, the ability to add short descriptions for folder contents, and the ability to build templates for new projects (the latter\n\n",
    "prize": "Winner Best Use of Gemini API",
    "techStack": "api, css, gemini, html, javascript, react, tailwind, vercel, vite",
    "github": "https://github.com/LaniW/DiaDoc",
    "youtube": "https://www.youtube.com/watch?v=gYa0wN5x0Us",
    "demo": "https://diadocdeploy.vercel.app/",
    "team": null,
    "date": "2026-01-03",
    "projectUrl": "https://devpost.com/software/diadoc"
  },
  {
    "title": "Nexo: Turn Code into Stories You Can See and Hear",
    "summary": "Nexo is an innovative project that transforms code into engaging visual and auditory stories, allowing users to understand programming concepts through multimedia storytelling. By combining code analysis with visualization and audio narration, it aims to make coding more accessible and enjoyable for beginners.",
    "description": "Profile DropDown\n    \n\n              \n  \n\n\n\n            \n        \n          \n  \n\n        \n    \n      Landing Page\n    \n\n        \n          \n  \n\n        \n    \n      Search Analysis Page\n    \n\n        \n          \n  \n\n        \n    \n      Repo Analysis NEXO\n    \n\n        \n          \n  \n\n        \n    \n      Repo Analysis VsCode\n    \n\n        \n          \n  \n\n        \n    \n      Learning Content\n    \n\n        \n          \n  \n\n        \n    \n      ChatBot PopUp\n    \n\n        \n          \n  \n\n        \n    \n      Profile DropDown\n    \n\n              \n  \n\n\n\n            \n        \n          \n  \n\n        \n    \n      Landing Page\n    \n\n        \n          \n  \n\n        \n    \n      Search Analysis Page\n    \n\n        \n          \n  \n\n        \n    \n      Repo Analysis NEXO\n    \n\n        \n          \n  \n\n        \n    \n      Repo Analysis VsCode\n    \n\n        \n          \n  \n\n        \n    \n      Learning Content\n    \n\n        \n          \n  \n\n        \n    \n      ChatBot PopUp\n    \n\n        \n          \n  \n\n        \n    \n      Profile DropDown\n    \n12345678    \n\n\n\n      \n  Nexo: Turn Code into Stories You Can See and Hear üöÄ\n\nProject Story\n\nüí° What Inspired Us\n\nEvery developer knows the pain: you join a new project, and you're immediately drowning in a sea of unfamiliar code. Thousands of files, cryptic function names, and zero documentation. You spend days‚Äîsometimes weeks‚Äîjust trying to understand where to start.\n\nWe realized that while humanity has built incredible tools to write code, we've barely scratched the surface of tools to understand it. The inspiration struck us during a late-night debugging session: \"What if code could tell its own story?\"\n\nTraditional documentation is static, outdated, and frankly... boring. But what if we could transform cold, lifeless text into a living, breathing narrative that you could both see and hear? What if onboarding to a codebase felt less like reading a technical manual and more like listening to an engaging podcast while watching an interactive map unfold?\n\nThat's when Nexo was born‚Äîa revolutionary platform that turns any GitHub repository into a multi-modal learning experience, combining AI-powered visual dependency graphs with podcast-style audio narration.\n\n\n\nüéØ What Nexo Does\n\nNexo is an intelligent code documentation platform that transforms static repositories into interactive, multi-sensory experiences. Here's the magic:\n\n\nüîó Paste Any Repository URL\nSimply provide a GitHub link‚Äîno configuration, no SDK installation required.\nüß† AI-Powered Analysis\nOur Gemini-powered engine performs deep static analysis, extracting functions, classes, dependencies, and architectural patterns.\nüìä Visual Code Maps\nWatch as your codebase transforms into an interactive dependency graph‚Äîzoom, pan, and click to explore module relationships and data flows.\nüéß Code Podcasts\nListen to ElevenLabs-generated audio narrations that explain each file's purpose, logic, and integration points in natural, conversational language. Perfect for commuting, exercis",
    "prize": "Winner Best Beginner Hack",
    "techStack": "cloudflare, css, docker, elevenlabs, fastapi, gemini, github, mongodb, python, react, typescript, vite",
    "github": "https://github.com/Hacktown-BSB/Nexo",
    "youtube": "https://www.youtube.com/watch?v=WJ31fUvhq2c",
    "demo": null,
    "team": "Pedro Garcia Vilanova, Matheus Lemes Amaral, Tiago Bittencourt",
    "date": "2026-01-04",
    "projectUrl": "https://devpost.com/software/nexo-the-audio-visual-gps-for-your-codebase"
  },
  {
    "title": "EcoLens",
    "summary": "TECHNICAL HIGHLIGHTS: The project employs a robust tech stack, including HTML, JavaScript, Node.js, and React, to create a dynamic and responsive user experience. Notable implementations may include real-time data processing for ecological metrics, interactive data visualization components, and a user-friendly dashboard that simplifies complex environmental data into actionable steps.",
    "description": "**What it does**\nEcoLens is an AI-powered sustainability intelligence platform that helps individuals and organizations understand and reduce their carbon footprint. Analyzes user behavior such as travel, energy usage, and lifestyle patterns\nEstimates carbon emissions using intelligent modeling\nProvides personalized, actionable recommendations\nTracks progress through sustainability scores and visual dashboards\nEncourages eco-friendly habits through gamification and challenges\nIn short, EcoLens turns complex environmental data into simple, meaningful insights that drive real-world change.\n\n**Inspiration**\nSustainability is often discussed, but rarely understood at a personal level.\nMost people want to reduce their environmental impact, yet they don‚Äôt know where they are contributing the most or how to make meaningful changes.\nWe realized that sustainability data exists ‚Äî but it‚Äôs scattered, complex, and inaccessible to everyday users. This gap between intention and action inspired us to build EcoLens ‚Äî a platform that makes sustainability visible, measurable, and actionable for everyone.\n\n**How we built it**\nEcoLens was built as a software-first, scalable prototype using:\nFrontend: A responsive web interface displaying dashboards, charts, and insights\nBackend Logic: Rule-based and AI-simulated models to estimate carbon emissions\nData Layer: Mock datasets simulating travel, energy, and consumption behavior\nAI Engine: Generates personalized recommendations and predicts emission trends\nGamification Module: Badges, streaks, and sustainability scores to drive engagement\nWe focused on clean architecture, clarity, and storytelling ‚Äî ensuring the demo is intuitive and impactful even without real-world data integrations.\n\n**Challenges**\nTranslating abstract sustainability metrics into something understandable\nDesigning an AI system that feels intelligent without relying on real user data\nBalancing technical depth with simplicity for a hackathon demo\nMaking sustainability feel empowering, not overwhelming\nEach challenge pushed me to think more creatively and design with the user in mind.\n\n**Accomplishments**\nBuilt a complete end-to-end sustainability intelligence platform in a short time\nSuccessfully translated complex climate data into actionable insights\nDesigned a system that is scalable from individuals to institutions\nCreated a solution that blends AI, UX, and environmental impact seamlessly\nMost importantly, we built something that can realistically drive behavioral change.\n\n**What we learned**\nSustainability solutions must be intuitive to create real adoption\nAI is most powerful when it simplifies decision-making, not complicates it\nGamification significantly improves engagement in serious domains\nClear storytelling is just as important as strong technology\n\n**What's next**\nOur vision for EcoLens goes far beyond the hackathon:\nIntegrating real-world APIs (Google Maps, utility data, carbon databases)\nAdding enterprise-grade ESG reporting tools\nExpanding into campuses, smart cities, and corporate sustainability programs\nIntroducing community challenges and global impact leaderboards\nDeveloping a public sustainability score API\nEcoLens aims to become the standard platform for understanding and improving environmental impact.\n\n",
    "prize": "Winner Best Hack for Hackers",
    "techStack": "html, javascript, node.js, react",
    "github": "https://github.com/sagnikchatterjee450/ecolens",
    "youtube": "https://www.youtube.com/watch?v=VjEPoZH7p3U",
    "demo": "https://ecolens-delta.vercel.app/",
    "team": null,
    "date": "2026-01-03",
    "projectUrl": "https://devpost.com/software/ecolens-vyokrh"
  },
  {
    "title": "NexusWatch",
    "summary": "TECHNICAL HIGHLIGHTS: The project showcases notable technical implementations, such as the use of Flask for RESTful API development, efficient data handling with Node.js, and a robust frontend built with TypeScript that ensures type safety and enhanced maintainability. Additionally, its deployment on Vultr demonstrates effective cloud resource utilization.",
    "description": "**What it does**\nNexusWatch is a global performance intelligence platform that tests website performance from multiple regions across the world and provides AI-powered insights and recommendations. It allows users to: Test any website's performance from 6 global regions simultaneously\nVisualize network paths with animated connection lines on an interactive map\nReceive AI-generated performance scores and reliability metrics\nGet actionable recommendations for optimizing global performance\nUnderstand how their service performs from different user perspectives\nThe platform transforms raw performance data into strategic business intelligence, helping companies make informed decisions about their global infrastructure.\n\n**Inspiration**\nIn today's digital-first world, website performance directly impacts user experience and business success. We were inspired by the challenge that companies face when trying to understand how their services perform across different geographical regions. Existing solutions are either expensive enterprise tools or simplistic single-point tests that don't capture the global picture. We wanted to create something that leverages Vultr's global infrastructure to provide comprehensive performance insights with the power of AI.\n\n**How we built it**\nWe built NexusWatch using a distributed architecture that leverages Vultr's global infrastructure: Backend Infrastructure:\nDeployed 6 Vultr cloud instances across Sydney, Amsterdam, Chicago, Frankfurt, Los Angeles, and Tokyo\nCreated a central orchestrator in Sydney that coordinates all performance tests\nUsed systemd for robust service management to ensure reliability\nTesting Technology:\nSwitched from ICMP ping to HTTP-based testing for more accurate web performance measurements\nBuilt custom Python agents that measure Time to First Byte (TTFB) for any website\nImplemented proper error handling and timeout management\nFrontend Experience:\nDeveloped a React TypeScript application with a modern dark theme\nCreated an interactive map using Leaflet with animated connection lines\nBuilt responsive co\n\n**Challenges**\nChallenges we ran into Service Reliability: Initially, our agent processes would terminate when we closed SSH connections. We solved this by implementing systemd services for robust process management.\nTesting Major Services: We discovered that many large websites like Netflix and Amazon block ICMP ping requests. We pivoted to HTTP-based testing which is more relevant for web services and works with virtually any website.\nAPI Integration: Integrating the Vultr Inference API required careful prompt engineering to ensure the AI provided structured, useful responses rather than generic text.\nUI/UX Design: Creating a visualization that was both informative and visually appealing required multiple iterations to get the right balance between data density and readability.\nCross-Region Communicati\n\n**Accomplishments**\nAccomplishments that we're proud of Complete Distributed System: We successfully built and deployed a distributed system spanning 6 global regions in just a few hours.\nReal AI Integration: We went beyond hardcoded recommendations to implement genuine AI-powered insights using Vultr's Serverless Inference.\nProfessional UI: We created a polished, responsive interface with smooth animations and thoughtful interactions that rivals commercial monitoring tools.\nRobust Architecture: Our system is built with production-quality practices including systemd services, proper error handling, and fallback mechanisms.\nSolving a Real Problem: We created a tool that addresses a genuine business need that companies typically pay thousands of dollars for with enterprise solutions.\n\n**What we learned**\nGlobal Infrastructure: We gained deep insights into how global infrastructure affects web performance and the importance of geographic proximity to users.\nDistributed Systems: We learned firsthand about the challenges and solutions for building reliable distributed systems.\nAI Integration: We developed skills in prompt engineering and integrating AI services into web applications.\nModern Web Development: We honed our React TypeScript skills and learned to create complex visualizations with libraries like Leaflet.\nSystem Reliability: We learned the importance of robust service management and the difference between a quick hack and a production-ready system.\n\n**What's next**\nHistorical Tracking: We plan to add historical performance tracking to show trends over time and identify performance regressions.\nAlert System: We want to implement a notification system that alerts users when performance drops below specified thresholds.\nExpanded Testing: We're looking to add more testing locations and additional metrics beyond just latency, such as uptime monitoring and route tracing.\nCustom AI Models: We plan to fine-tune AI models specifically for performance analysis to provide even more accurate and domain-specific insights.\nIntegration with CI/CD: We envision integrating NexusWatch into development pipelines to automatically test performance during deployments.\nPublic API: We're considering offering a public API so other services can integrate NexusWatch's global p\n\n",
    "prize": "Winner Best Use of Vultr",
    "techStack": "flask, linux, node.js, typescript",
    "github": "https://github.com/YuvrajSHAD/NexusWatch",
    "youtube": null,
    "demo": "https://nexuswatch.yuvrajsingh302.workers.dev/",
    "team": null,
    "date": "2026-01-03",
    "projectUrl": "https://devpost.com/software/pentestlab"
  },
  {
    "title": "Current AI",
    "summary": "Current AI is a project designed to integrate AI capabilities into Discord, allowing users to interact with advanced AI models through a bot. It leverages various APIs to enhance user experiences, providing intelligent responses and functionalities that cater to the needs of the Discord community.",
    "description": "**What it does**\nCurrent-AI is an AI-powered project management system that lives inside your Discord server. Just chat naturally, and watch the magic happen:\n\n**Inspiration**\nAfter attending over 21 hackathons, I've witnessed the same problem again and again: teams spend more time organizing than building. Picture this: It's 2 AM at a hackathon. Your team is exhausted. Someone asks, \"Who's working on the database?\" Silence. Another person chimes in, \"I thought you were doing that? I've been building the API!\" Now you have duplicate work, wasted hours, and mounting frustration. Sound familiar? The reality is brutal: üî¥ 6+ hours wasted just figuring out task assignments\nüî¥ Critical features forgotten because nobody tracked them\nüî¥ Team members duplicate work due to poor communication\nüî¥ Discord messages buried in endless scrolling\nüî¥ \"Who's doing what?\" becomes the most asked question I realized that Discord is where hackers live, but it's terrible for project ma\n\n**How we built it**\nCurrent-AI is a full-stack monorepo combining cutting-edge technologies:\n\n",
    "prize": "Winner Best Use of MongoDB Atlas",
    "techStack": "discord-api, discord.py, github-api, google-gemini-ai, mongodb, next.js, python, typescript",
    "github": "https://github.com/sherwinvishesh/Current-AI",
    "youtube": "https://www.youtube.com/watch?v=Y9Xt2FuiO6U",
    "demo": null,
    "team": "Sherwin Vishesh Jathanna",
    "date": "2026-01-04",
    "projectUrl": "https://devpost.com/software/current-ai"
  },
  {
    "title": "Renal Wellness:  3-Day Student Detox",
    "summary": "The \"Renal Wellness: 3-Day Student Detox\" project aims to promote kidney health and overall wellness among students through a structured detox program that leverages artificial intelligence. By integrating personalized nutrition and preventive care strategies, the initiative seeks to empower students to make informed lifestyle choices that support renal health.",
    "description": "Real Life Smoothie Day2\n    \n\n              \n  \n\n\n\n            \n        \n          \n  \n\n        \n    \n      Thumbnail\n    \n\n        \n          \n  \n\n        \n    \n      AI Food Analysis\n    \n\n        \n          \n  \n\n        \n    \n      3Day Healing Roadmap SS1\n    \n\n        \n          \n  \n\n        \n    \n      3Day Healing Roadmap SS2\n    \n\n        \n          \n  \n\n        \n    \n      Marketplace Suggestions\n    \n\n        \n          \n  \n\n        \n    \n      Real Life Smoothie Day2\n    \n\n              \n  \n\n\n\n            \n        \n          \n  \n\n        \n    \n      Thumbnail\n    \n\n        \n          \n  \n\n        \n    \n      AI Food Analysis\n    \n\n        \n          \n  \n\n        \n    \n      3Day Healing Roadmap SS1\n    \n\n        \n          \n  \n\n        \n    \n      3Day Healing Roadmap SS2\n    \n\n        \n          \n  \n\n        \n    \n      Marketplace Suggestions\n    \n\n        \n          \n  \n\n        \n    \n      Real Life Smoothie Day2\n    \n1234567    \n\n\n\n      \n  My 3-Day Wellness Journey With Renal-AI\n\nBalancing health with academic obligations can be challenging as a university student, and kidney health was something I had never given much thought to before. For this hackathon, I used Renal-AI to role-play as a healthy student with an emphasis on long-term wellness and prevention. Through this experience, I gained a deeper understanding of how small lifestyle decisions can have a significant impact on kidney health and overall well-being.\n\nI started by adding basic ingredients that were already available in my kitchen, including rice, dal, apples, cucumber, yogurt, lemon, carrots, and potatoes. Renal-AI evaluated these ingredients immediately and provided feedback on their alkaline qualities, balance, and kidney safety. What impressed me most was that the AI did not recommend complicated or impractical meals. Instead, it created a realistic and achievable plan using items I already had.\n\nDay 1\n\nDay 1 meals included baked potatoes with peas and yogurt, rice pudding with cinnamon, and dal with a lemon-flavored cucumber salad. Along with food guidance, Renal-AI suggested light physical activities such as stretching and short walks, while also emphasizing the importance of maintaining a consistent sleep schedule. These recommendations were simple to implement and blended naturally into my daily routine.\n\nDay 2\n\nDay 2 featured a refreshing yogurt, apple, and orange smoothie. I prepared this smoothie in real life, and it was easy, delicious, and energizing. This experience helped me realize that healthy eating does not have to be tedious or time-consuming. Lunch and dinner consisted of vegetable stir-fried rice and a light vegetable soup. The gentle activity plan helped me stay active without feeling overwhelmed.\n\nDay 3\n\nBy Day 3, the routine no longer felt forced but instead became organic and sustainable. Meals such as lemon-herb roasted potatoes with dal and carrot and pea rice pilaf reinforced the importance of moderation and balance. The most importan",
    "prize": "Winner Renal-AI Grand Prize Winner",
    "techStack": "ai, artificialintelligence, healthcare, nutrition, preventivecare, renal-ai, studenthealth, wellness",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=QdbRx0ztb6U",
    "demo": "https://drive.google.com/drive/folders/1xWBm5AtiAPSJVm3yiFBrjXjbJFM8vJdx?usp=drive_link",
    "team": "Ayesha Riaz",
    "date": "2025-12-30",
    "projectUrl": "https://devpost.com/software/renal-wellness-3-day-student-detox"
  },
  {
    "title": "CultureAlive",
    "summary": "TECHNICAL HIGHLIGHTS: The application is built using Python Flask for the backend, ensuring a robust and scalable framework. It incorporates modern web technologies such as Bootstrap for responsive design, CSS for styling, and JavaScript for interactivity. The use of dotenv for environment variable management enhances security, while groq and groq-llm suggest advanced data handling capabilities, potentially integrating AI for personalized cultural recommendations.",
    "description": "**What it does**\nCultureAlive is a web application that uses AI-powered storytelling to explain cultural topics in simple and easy-to-understand language.\nUsers can enter a topic (such as a temple, festival, tradition, or historical concept), and the AI responds as a cultural storyteller, making complex information approachable and meaningful.\n\n**Inspiration**\nCulture is the soul of any community, yet many local traditions, temples, festivals, and stories remain undocumented or difficult to understand for younger generations. I was inspired to build CultureAlive to preserve and present cultural knowledge in a simple, engaging, and accessible way using AI.\nThe idea was to combine technology with heritage‚Äîallowing users to explore cultural topics through natural language explanations.\n\n**How we built it**\nBackend: Python with Flask AI Model: Groq LLM (LLaMA 3.1) Frontend: HTML, CSS, JavaScript Deployment: Render Environment Management: dotenv The Flask backend handles API requests and sends user input to the Groq LLM.\nThe AI processes the prompt using a cultural storytelling context and returns a clear, human-like explanation.\nThe project is deployed on Render, making it accessible publicly via the web.\n\n**Challenges**\nDeployment issues: Handling environment variables securely on Render API authentication errors: Debugging invalid API key issues Template handling: Fixing Flask template path errors (TemplateNotFound) Error handling: Ensuring smooth user experience when the AI API fails Each challenge helped me better understand backend debugging, deployment workflows, and production-level configuration.\n\n**What we learned**\nHow to integrate LLMs into real-world applications Deploying Flask apps on cloud platforms Managing environment variables securely Debugging production errors Designing AI prompts for meaningful, user-friendly responses\n\n**What's next**\nMultilingual cultural storytelling Voice-based narration User-contributed cultural stories Image and map integration for cultural sites\n\n",
    "prize": "Winner Excellence in Social Impact Award",
    "techStack": "backend-python-flask, bootstrap, css, dotenv, groq, groq-llm, html5, javascript, render",
    "github": "https://github.com/prajktanandurkar09/CultureAlive_Hack",
    "youtube": "https://www.youtube.com/watch?v=8nXVZgDNt1g",
    "demo": "https://culturealive.onrender.com/",
    "team": "Prajkta Nandurkar",
    "date": "2025-12-30",
    "projectUrl": "https://devpost.com/software/culturealive"
  },
  {
    "title": "Scholarship Finder",
    "summary": "TECHNICAL HIGHLIGHTS: The implementation of the Google Web Speech API stands out as a notable technical achievement, enabling seamless voice recognition. Additionally, the use of HTML, JavaScript, JSON, and Python allows for effective front-end and back-end integration, ensuring smooth data retrieval and user interaction.",
    "description": "**Challenges**\nUnreliable AI output formatting\nSolved using strict JSON Schemas and JSON-only prompts. Alert fatigue\nSolved by classifying change severity and filtering low-impact updates. AI overreach\nSolved by limiting the model to recommendations only; execution remains in n8n.\n\n**What we learned**\nHow to safely integrate LLMs into automated workflows\nThe importance of schema validation for AI reliability\nHow MCP improves explainability and auditability\nHow orchestration tools turn AI agents into production systems\n\n**What's next**\nDashboard for historical change tracking\nSlack or webhook notifications\nMulti-language email support\nUser-defined alert rules\nDatabase-backed scholarship history\n\n",
    "prize": "Winner Browser Use",
    "techStack": "google-web-speech-api, html, javascript, json, python",
    "github": null,
    "youtube": null,
    "demo": null,
    "team": "Moksha Shah",
    "date": "2025-12-23",
    "projectUrl": "https://devpost.com/software/scholarship-finder-59vkwp"
  },
  {
    "title": "Guidely",
    "summary": "Guidely is an innovative project designed to leverage the capabilities of the Gemini API alongside Google Cloud to provide personalized guidance and recommendations for users. It utilizes a React frontend to create a dynamic and user-friendly interface, enhancing the user experience and making the delivery of insights seamless.",
    "description": "**What it does**\nGuidely is an AI-powered career mentor app that helps users build CVs, find jobs matching their skills, connect with nearby mentors, track active learning time, and complete daily tasks and challenges. It provides both text and voice guidance, gamifies learning experiences, and motivates users to achieve their career goals.\n\n**Inspiration**\nMany young professionals and students in Kigali struggle to find mentors, career guidance, and personalized learning opportunities. I wanted to create an AI-driven platform that empowers users to grow their careers through actionable guidance, mentorship, and skill development.\n\n**How we built it**\nWe used Google AI Studio and integrated multiple AI models: Gemini 3 Pro for guidance and mentorship, Imagen 3 for visual assets, and Nanobana for task and challenge generation. The app supports voice and text recognition, social login, user profiles, and a personalized task system to guide users step by step.\n\n**Challenges**\nDesigning an intuitive UI for complex AI features, ensuring relevant mentor and job matches, integrating real-time voice guidance, and gamifying career development were the main challenges.\n\n**Accomplishments**\nWe created a platform that acts like a personal mentor, offering actionable guidance, gamified challenges, skill tracking, and mentorship connections‚Äîall in one app.\n\n**What we learned**\nWe gained experience integrating AI models, designing user-friendly interfaces for complex features, and creating systems that motivate and engage users effectively.\n\n**What's next**\nWe plan to expand mentor and job networks, add more gamified learning experiences, and enhance AI personalization to make the platform even more effective in guiding users‚Äô careers.\n\n",
    "prize": "Winner Best Use of Gemini API",
    "techStack": "gemini, google-cloud, react",
    "github": "https://github.com/aibrahiam/Guidely",
    "youtube": "https://www.youtube.com/watch?v=NfyhKd3D2to",
    "demo": null,
    "team": "Ahmed Ibrahim, Mr Adwok",
    "date": "2025-12-23",
    "projectUrl": "https://devpost.com/software/guidely"
  },
  {
    "title": "InterLink",
    "summary": "InterLink is a collaborative platform designed to facilitate real-time communication and project management among users. It harnesses various web technologies to enable seamless interactions, allowing teams to work together effectively regardless of their physical locations.",
    "description": "123456789    \n\n\n\n      \n  Inspiration\nWe live in an era where we can video chat with someone across the ocean, yet we often don't know the person living right next door. We realized that every neighborhood is an untapped skill economy‚Äîfull of experts, mentors, and tradespeople‚Äîbut there was no marketplace to connect them.\n\nWe were inspired to redesign learning by transforming it into a mutual exchange. We didn't just want a classroom; we wanted a platform for skill bartering, where a retired piano teacher could trade lessons with a neighbor who knows how to fix a leaky faucet. Our goal was to break down the barriers of isolation and language, allowing neighbors to \"pay\" for knowledge with their own unique skills, making learning accessible and community-driven.\n\nHow we built it\nWe built InterLink as a full-stack real-time application using Next.js 16 and React 19 for a responsive, modern frontend, and a Node.js/Express backend.\n\nAI-Powered Reciprocal Matching: We utilized Google's Gemini model to go beyond simple keyword matching. The AI analyzes user profiles to identify mutual benefits, generating personalized \"match explanations\" that highlight the trade (e.g., \"Sarah can teach you piano and specifically needs the pet sitting you offer\"), making the connection feel fair and relevant.\n\nReal-Time Translation: To truly democratize this exchange, we built a live translation engine using Gemini. It detects languages on the fly, allowing a Spanish-speaking mentor to barter skills seamlessly with an English-speaking neighbor.\n\nGeospatial Privacy: We implemented MongoDB geospatial queries to find nearby neighbors for practical exchanges but added a \"fuzzy location\" layer to our models. This allows users to find help \"near 5th Ave\" without exposing their exact home address until they trust the connection.\n\nGamification: To incentivize the \"barter,\" we built a custom scoring engine (+10 for connections, +20 for completed exchanges) that rewards users for actively contributing to the community pool of skills.\n\nChallenges we faced\nHandling API Rate Limits: One of our biggest hurdles was managing the Gemini API rate limits while keeping the app responsive. We solved this by implementing a custom LRU (Least Recently Used) Caching System (aiCache.js) that stores generated match descriptions and translations, significantly reducing redundant API calls and latency.\n\nReliability vs. Real-World Chaos: We knew that for a demo (and real life), APIs can fail. We spent significant time building \"Intelligent Fallbacks\" where the system seamlessly switches to rule-based matching if the AI service becomes unavailable, ensuring the app never crashes during a critical interaction.\n\nWhat we learned\nAI is a UX Tool, not just a Chatbot: We learned that Generative AI shines when it works in the background‚Äîsorting lists, explaining trade compatibility, and translating text‚Äîrather than just being a chat interface.\n\nThe Power of Reciprocity: In testing our \"Emergency\" and excha",
    "prize": "Winner People's Choice",
    "techStack": "css, express.js, gemini, javascript, mongodb, next.js, node.js, radix, react, socket.io, tailwind, typescript",
    "github": "https://github.com/neil-agarwal24/InterLink",
    "youtube": null,
    "demo": null,
    "team": "Neil Agarwal, Ayush Iyer",
    "date": "2025-12-21",
    "projectUrl": "https://devpost.com/software/interlink-md3ouz"
  },
  {
    "title": "FluxDiagram",
    "summary": "TECHNICAL HIGHLIGHTS: Built with a robust tech stack including React and TypeScript, FluxDiagram utilizes CSS3 for responsive design and antigravity for enhanced visualization capabilities. The integration of these technologies ensures smooth performance and a visually appealing layout.",
    "description": "**What it does**\nFluxDiagram is a native VS Code extension that transforms your editor into a powerful flowchart builder. Core Features:\nüé® Intuitive Canvas ‚Äî Drag-and-drop nodes with smooth pan & zoom\nüîó Smart Connections ‚Äî Create edges between nodes with automatic path routing\nüìê Auto Layout ‚Äî Automatically organize messy diagrams with one click\nüó∫Ô∏è Minimap Navigation ‚Äî Navigate large diagrams effortlessly\nüåó Theme Aware ‚Äî Automatically adapts to your VS Code light/dark theme\nüì§ Multi-format Export ‚Äî PNG, SVG, and JSON export for any use case\n‚Ü©Ô∏è Full Undo/Redo ‚Äî Configurable history up to 200 steps\n‚å®Ô∏è Keyboard First ‚Äî Power-user shortcuts for every action\nDiagrams are saved as .fluxdiagram files that live alongside your code and can be version-controlled with Git.\n\n**Inspiration**\nDevelopers constantly need to visualize their logic ‚Äî whether it's documenting an algorithm, planning a system architecture, or explaining a workflow to teammates. But the current options are frustrating: switch to a browser-based tool like Lucidchart, lose context, and break your flow. Or use clunky desktop apps that don't integrate with your dev environment. We asked ourselves: Why can't we build flowcharts where we already spend most of our time ‚Äî inside VS Code? FluxDiagram was born from the belief that diagramming should be as seamless as writing code. No context switching. No exports to manage. Just open a file and start creating.\n\n**How we built it**\nFluxDiagram is built entirely with TypeScript and leverages the VS Code Custom Editor API for deep integration.\nArchitecture Highlights:\nLayer                                         Technology\nExtension Host                         VS Code Extension API\nState Management                   Custom EventBus + StateManager pattern\nCanvas Rendering                           HTML5 Canvas with custom renderers\nLayout Engine                          Graph-based auto-layout algorithm\nBuild System                                    esbuild for fast bundling\nTesting                                        Jest with TypeScript support Key Design Decisions:\nSeparation of concerns ‚Äî Core graph logic is decoupled from VS Code APIs, making it testable and portable\nEvent-driven architecture ‚Äî All state chan\n\n**Challenges**\nVS Code Webview Sandboxing\nWebviews run in an isolated context with strict CSP policies. We had to carefully architect the communication bridge between the extension host and the canvas without compromising security.\nCanvas Performance at Scale\nRendering hundreds of nodes with smooth 60fps panning required implementing viewport culling ‚Äî only drawing elements visible on screen ‚Äî and optimizing our render loop.\nUndo/Redo with Graph State\nImplementing reliable undo/redo for a mutable graph structure was tricky. We solved it with immutable state snapshots and a custom StateManager that handles deep cloning efficiently.\nAuto-Layout Algorithm\nCreating a layout engine that produces visually pleasing results for any graph topology took multiple iterations. We implemented a layered approach inspir\n\n**Accomplishments**\n‚úÖ Zero external runtime dependencies ‚Äî The extension is lightweight and fast to load ‚úÖ Seamless UX ‚Äî Opening a .fluxdiagram file feels as natural as opening a code file ‚úÖ Production-ready code quality ‚Äî Full TypeScript, ESLint, Prettier, and Jest test coverage ‚úÖ Minimap implementation ‚Äî A fully functional minimap that mirrors the main canvas in real-time ‚úÖ Export flexibility ‚Äî Users can export diagrams as PNG (for docs), SVG (for web), or JSON (for processing) ‚úÖ First-class keyboard support ‚Äî Every action has a shortcut, making it accessible and efficient\n\n**What we learned**\nVS Code's Extension API is incredibly powerful ‚Äî Custom editors open up possibilities far beyond text editing\nState management matters ‚Äî Investing early in a clean EventBus pattern saved us from countless bugs\nPerformance requires intention ‚Äî Canvas rendering at scale doesn't \"just work\" ‚Äî you have to design for it\nDeveloper experience is UX ‚Äî Small touches like grid snapping and keyboard shortcuts dramatically improve usability\nTesting graph logic is non-trivial ‚Äî We developed patterns for testing stateful, interconnected data structures\n\n**What's next**\nüöÄ Roadmap:\nFeature                             Description\nCollaboration               Real-time multi-user editing via VS Code Live Share\nAI-Powered Diagrams     Generate flowcharts from natural language or code analysis\nSwimlanes                       Organize nodes into lanes for process diagrams\nImport Support              Import from Mermaid, Draw.io, and other formats\nCode Integration                Link nodes to specific lines of code for documentation We believe FluxDiagram can become the go-to diagramming tool for developers who live in VS Code. The journey is just beginning! üéØ\n\n",
    "prize": "Winner CERTIFICATE",
    "techStack": "antigravity, css3, html5, react, typescript",
    "github": "https://github.com/sitharaj88/flux-diagram",
    "youtube": "https://www.youtube.com/watch?v=FNBSyVYvidk",
    "demo": "https://marketplace.visualstudio.com/items?itemName=sitharaj.flux-diagram",
    "team": null,
    "date": "2025-12-29",
    "projectUrl": "https://devpost.com/software/fluxdiagram"
  },
  {
    "title": "HOUP APP",
    "summary": "TECHNICAL HIGHLIGHTS: Notable technical implementations likely include advanced AI algorithms for user interaction and experience personalization, alongside the integration of antigravity technology, which may involve novel physics simulations or user interface designs that defy conventional applications. This combination could lead to groundbreaking user experiences.",
    "description": "**What it does**\nHOUP helps you record what you're working on every hour and automatically saves it to Google Sheets. Think of it as:\nA digital work diary that reminds you to update your progress\nAn automatic timesheet that logs your daily activities\nA productivity tracker that keeps a record of everything you accomplish\n\n**How we built it**\nusing antigravity ai\n\n**Challenges**\nmany bug so i tripleshoot\n\n**What we learned**\nAI and promting ,hoe apk work,how puplish apk\n\n",
    "prize": "Winner Participation; Winner InterviewCake Licenses; Winner .xyz Domains",
    "techStack": "ai, antigravity",
    "github": "https://github.com/naveenrajgit23/houp.git",
    "youtube": "https://www.youtube.com/watch?v=gGEAa8aBSek",
    "demo": "https://expo.dev/artifacts/eas/mH14uN4DWGFwjpCne9TFKD.apk",
    "team": "Naveen raj Malaikannan",
    "date": "2025-12-20",
    "projectUrl": "https://devpost.com/software/houp-app-xgeq9k"
  },
  {
    "title": "Sous Chef",
    "summary": "Sous Chef is an innovative cooking assistant application designed to enhance the culinary experience by providing users with personalized recipe suggestions, cooking tips, and ingredient management. It leverages advanced AI and machine learning to adapt to user preferences and dietary restrictions, making meal preparation more efficient and enjoyable.",
    "description": "**What it does**\nBridges the gap between student struggles and instructors field of view into how well their students are ingesting knowledge.\n\n**Inspiration**\nHad a discussion with whom i'd like to call Mentor. The Education Vertical is underserved as it is. We are preparing for our future, why not equip them with the best shot at making a better future.\n\n**How we built it**\nWe built it with Claude Code, React, typescript and vite. Claude AI for Chat and MemMachine for persistent memory.\n\n**Challenges**\nImplementing the real-time streaming response and speed while saving/pulling memory at the same time.\n\n**Accomplishments**\nCreating this vision for students to get more personalized not generalized education and the instructors can save time and be able to assist all of their students and see where the hidden gaps are to provide a more robust curriculum.\n\n**What we learned**\nThat Memory can be applied and 100x your workflows.\n\n**What's next**\nThe Ability to upload curriculum uploads and an area for the students to keep structure notes from what they spoke to sous about plus their personal notes.\n\n",
    "prize": "Winner Third Prize",
    "techStack": "claude, memmachine, react, typescript, vite",
    "github": "https://github.com/MuddySheep/SousChef",
    "youtube": "https://www.youtube.com/watch?v=et9tQk2GUis",
    "demo": null,
    "team": null,
    "date": "2025-12-18",
    "projectUrl": "https://devpost.com/software/sous-chef-5czyd0"
  },
  {
    "title": "CleanMCP - MCP and Context Management made Easier",
    "summary": "CleanMCP is a project designed to simplify the management of Model-Context Protocol (MCP) and contextual data in applications. It aims to streamline the development process by providing an intuitive interface and efficient data handling capabilities, enabling developers to easily integrate and manage context within their applications.",
    "description": "**What it does**\nClean is an MCP server that intercepts tool calls from your other MCP server calls from coding agents mid-request. Every response gets captured and stored in a Neo4j graph database, tagged by team, user, MCP, and timestamp. The frontend then visualizes it and programmers can now have clean context provided straight from a history of MCP toolcalls saved in a vector DB. On top of that, we built a full dashboard using Reagraph in 2D mode where you can visualize your MCPs as nodes. Click on any MCP and see all the context it has fetched. The Context page lets you search, filter, and clean up outdated entries. We also integrated Mem Machine to power a chatbot that can answer questions about your stored context. Ask it things like \"What MCPs have we used on this project?\" or \"What context do we\n\n**Inspiration**\nWhen we started using AI coding agents like Claude Code and Cursor, we fell into a rabbit hole. There are over 12,000 MCP servers out there, and we had no idea which ones to use or what context our agents were actually fetching. But the bigger problem hit us when we worked as a team. One person's agent was using Tailwind v4, another's was on v3. Styles clashed. Context was fragmented. There was no shared memory and no visibility into what anyone's agent had actually seen. We knew we had to fix that.\n\n**How we built it**\nClean runs as an MCP server that intercepts other MCP tool calls mid-request. Every response is captured and tagged with team ID, user ID, MCP name, tool name, and timestamp. The normal agent-to-MCP flow stays intact; Clean just sits in the middle and mirrors everything. All captured context is stored in Neo4j as a graph: Team ‚Üí User ‚Üí MCP ‚Üí Tool Call ‚Üí Context. This gives us a single source of truth for what context has ever been fetched across the team. We built a 6-page dashboard in Next.js: Home, MCP, Context, Team, Settings, Chat. The Home page uses Reagraph in 2D mode to show a central MCP node with individual MCP nodes around it. Clicking an MCP reveals the context nodes associated with that MCP. The Context page has search, freshness indicators, and bulk cleanup tools. The MCP page\n\n**Challenges**\nClaude Code does not let you simply route MCPs through a proxy. We had to figure out how to actually intercept tool calls in the middle of a request, which required a lot of trial and error since MCP documentation is sparse. Translating Neo4j graph nodes into something that renders well in the browser was painful. We had to map relationships carefully so the 2D visualization made sense and performed smoothly. Wiring Mem Machine to the right slice of context from Neo4j took iteration. We needed the chatbot to stay relevant without dumping the entire database into every query. Smithery was not working, and there is no official API for scraping MCP directories. We ended up building our own MCP catalog from scratch in Neo4j. MCP servers are bleeding-edge technology. Learning how they work, bui\n\n**Accomplishments**\nGot mid-request interception working so Clean actually captures tool calls as they happen\nBuilt our own MCP catalog from scratch in Neo4j instead of relying on external directories\nShipped a polished 6-page dashboard with Reagraph 2D visualization that we would actually want to use\nIntegrated Mem Machine into a chatbot that can answer questions about captured context\nTested cross-team context sharing on our own project and it works: one person fetches, everyone sees it\nLearned Neo4j, Mem Machine, Reagraph, and MCP architecture from scratch as freshmen in two days\n\n**What we learned**\nMCP tooling is young and documentation is thin. We had to reverse-engineer patterns from examples and just try things until they worked.\nDesigning a graph schema in Neo4j that maps cleanly to a product UI takes thought. We spent hours on architecture before coding and it saved us from rewriting everything later.\nInterception is harder than proxying. Claude Code does not want you in the middle, so you have to work around constraints instead of fighting them.\nWith AI assistance, nothing is too far-fetched. We built something we did not think was possible at the start of the weekend.\n\n**What's next**\nToken optimization is first. We want to use cosine similarity to fetch only relevant context per prompt instead of everything, and summarize stored context to reduce token costs. We also want smarter MCP recommendations that suggest which MCPs to install based on project type and what the team is working on. We are already talking to a YC company who wants to be our first customer. Goal is to ship, get paying users, and apply to the next YC batch.\n\n",
    "prize": "Winner Second Prize",
    "techStack": "claude-code, cypher-query-language, date-fns, eslint, framer-motion, gemini-api, gsap, lucide, mcp-(model-context-protocol), mem-machine, neo4j, next.js-16, node.js-20+, prettier, radix-ui, react-19, react-hook-form, reagraph, shadcn/ui, tailwind-css-v4, tanstack/react-table, typescript, vercel, websocket, zod",
    "github": "https://github.com/clarsbyte/cleanMCP-Hack.git",
    "youtube": "https://www.youtube.com/watch?v=1h5pDpASSlU",
    "demo": null,
    "team": "Pavan Kumar NY, Clarissa Saputra, Nikhil Prabhu",
    "date": "2025-12-19",
    "projectUrl": "https://devpost.com/software/clean-mcp-management-and-context"
  },
  {
    "title": "LifeGraph",
    "summary": "TECHNICAL HIGHLIGHTS: LifeGraph leverages several cutting-edge technologies, including FastAPI for building a robust backend, Neo4j for graph database management, and React for an interactive frontend. The integration of TailwindCSS ensures a responsive and aesthetically pleasing design, while Vercel and Vite enhance deployment and development efficiency.",
    "description": "**What it does**\nLifeGraph is a habit intelligence platform with two superpowers: 1. Graph-Based Pattern Discovery (Neo4j) Your daily habits (health, learning, creativity) are stored as connected nodes in a knowledge graph\nThe SAME_DAY_AS relationship links activities that happened together\nThe AI agent traverses these connections to find non-obvious correlations:\n\n\n\"On days with HRV > 55, your learning focus is 23% higher\"\n\"Heavy lifting + 8hrs sleep predicts creative satisfaction\"\n\"Afternoon coffee correlates with -23% sleep quality\" 2. Time-Traveling Memory (MemVerge) At strategic moments (Day 15, Day 45), the agent saves memory checkpoints\nUsers can \"restore\" to any checkpoint and the agent shows only what it knew then\nThe Day 45 discoveries (cold exposure, protein optimization) literally disappear whe\n\n**Inspiration**\nWe've all been there: you track your sleep in one app, your workouts in another, your learning in a third. Each app shows you isolated metrics, but none can answer the questions that actually matter: \"Why was I so productive last Tuesday?\"\n\"Does my morning routine actually help my afternoon focus?\"\n\"What patterns predict my best creative sessions?\" The insight is obvious: your life is interconnected, but your tools aren't. Then we thought about AI assistants. They're great at conversations, but they have amnesia - every session starts fresh. What if an AI could not only see connections across your life domains, but also remember what it learned about you and even travel back in time to understand how its knowledge evolved? That's when LifeGraph was born: an AI agent that models your life a\n\n**How we built it**\nArchitecture: React Dashboard (Vercel)\n        ‚Üì\nFastAPI Backend (GCP Cloud Run)\n        ‚Üì\n   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê\nNeo4j Aura   MemVerge MMCloud\n(Graph DB)   (Memory Checkpoints) Neo4j Graph Schema: 5 node types: Streak, Log, Day, Concept, Sentiment\n9 relationship types including the crucial SAME_DAY_AS\n215 nodes, 432 relationships representing 90 days of synthetic habit data MemVerge Integration: Checkpoints store: discovered patterns, confidence scores, agent thoughts, active node count\nRestore operation returns the agent's state at that exact moment\nFrontend replaces the \"Agent Thoughts\" panel with checkpoint-specific insights AI Agent Logic: MATCH (health:Log)-[:SAME_DAY_AS]-(learn:Log)\nWHERE health.sleep_hours >= 8 AND health.workout = 'Zone 2 Cardio'\nRETURN AVG(learn.focus_score) as avg_focus\n\n**Challenges**\n1. Designing the Graph Schema\nHow do you model \"life\" as a graph? The breakthrough was the SAME_DAY_AS relationship - it connects activities across different domains without complex temporal joins. This simple edge enables powerful cross-streak queries. 2. Defining \"AI Agent Memory\"\nMemVerge typically checkpoints VMs. But what IS an AI agent's memory? We defined it as: Discovered patterns (with confidence scores)\nAgent thoughts (the reasoning stream)\nActive knowledge (which nodes it's aware of)\nNOT just raw data - the derived understanding 3. Making Time Travel Feel Real\nThe UI had to clearly show that the agent \"forgot\" later discoveries. We solved this by: Completely replacing the Agent Thoughts panel (not just filtering)\nShowing different confidence levels at different checkpoints\nAddin\n\n**Accomplishments**\n1. The Time Travel Actually Works\nWhen you restore to Day 15, the cold exposure insight genuinely disappears. The agent shows lower confidence scores. It feels like you're looking at a younger, less experienced version of the AI. 2. Cross-Streak Insights Are Real\nThe graph structure enables discoveries that spreadsheets can't: \"Frustrated tech sessions ‚Üí darker haiku themes same evening\". The SAME_DAY_AS edge makes this query trivial. 3. Clean Separation of Concerns Neo4j = What happened (data)\nMemVerge = What the agent knew (memory)\nThis pattern could apply to ANY AI agent application 4. Production-Ready Demo\nLive at lifegraph-hackathon.vercel.app with real API calls to Neo4j Aura and MemVerge-style checkpoints.\n\n**What we learned**\nAbout Neo4j: Graph databases make relationship queries first-class citizens\nCypher pattern matching is incredibly intuitive for discovery\nAura's free tier handled our 215-node graph effortlessly About MemVerge: Memory checkpointing applies beyond VMs - it's a pattern for any stateful system\n\"Time travel\" is a compelling UX metaphor that users immediately understand\nAgent state includes derived knowledge, not just raw data About AI Agents: Agents need TWO types of memory:\n\n\nWorking memory: Current context and recent interactions\nEpisodic memory: What they learned and when they learned it\n\nGraphs are natural for agent reasoning about interconnected domains\nPersistent memory transforms agents from assistants to companions\n\n**What's next**\nShort Term: Connect to real data sources (Apple Health, Whoop, Notion)\nNatural language queries (\"What makes me productive?\")\nMore granular checkpoints (weekly, monthly)\nAutomatic checkpoint creation when significant patterns emerge Long Term: Predictive insights (\"Based on today's data, try X tomorrow\")\nMulti-user anonymized patterns (collective intelligence)\nLLM integration for conversational graph exploration\nMobile app for daily logging\nExport/share your graph with coaches or doctors The Vision:\nEvery person deserves an AI that truly knows them - not just their last conversation, but their patterns, their growth, their journey. LifeGraph is a step toward AI agents with genuine long-term memory.\n\n",
    "prize": "Winner First Prize",
    "techStack": "fastapi, memory-machine, memverge, neo4j, neo4j-aura, python, react, recharts, tailwindcss, typescript, vercel, vite",
    "github": "https://github.com/ooiyeefei/lifegraph-hackathon",
    "youtube": "https://www.youtube.com/watch?v=Jwn7kOJdvsw",
    "demo": "https://lifegraph-hackathon.vercel.app/",
    "team": "Yee Fei Ooi",
    "date": "2025-12-18",
    "projectUrl": "https://devpost.com/software/lifestreak"
  },
  {
    "title": "Upside down survival",
    "summary": "\"Upside Down Survival\" is an innovative project designed to simulate survival scenarios in an inverted world, where traditional survival strategies are challenged. The project leverages advanced AI to create dynamic environments, requiring users to adapt their approaches in real-time, ultimately enhancing critical thinking and problem-solving skills in unconventional situations.",
    "description": "**What it does**\nUpsidometer monitors temperature, air quality, and sound to detect abnormal environmental conditions linked to Vecna‚Äôs presence. It converts this data into a real-time danger level, triggers visual alerts on the screen, and plays music when the user is in serious danger to help them escape.\n\n**Inspiration**\nIn Stranger Things, danger in the Upside Down is sensed through environmental changes rather than direct sight. We were inspired by this idea and wanted to build a system that could detect those subtle signs early and help people react before it‚Äôs too late.\n\n**How we built it**\nWe used multiple sensors to collect real-time data, processed each input to detect anomalies, and combined them into a single danger index. This index is displayed visually through the Upsidometer and controls alerts like screen flashes and music playback. We used react, node, express,typescript to make this.\n\n**Challenges**\nThe main challenge was balancing sensitivity and accuracy while avoiding false alerts from normal environmental changes.It was difficult to integrate the Gemini Api Chatbot. It was also a challenge to integrate the music when the danger was critical.\n\n**Accomplishments**\nThe successful integration of Gemini Api Chatbot, inclusion of music when the person is in critical danger, real time sensor reading such as audio, temperature and air quality index. ALso very proud\n\n**What we learned**\nLearned how to integrate Gemini Api Chatbot. Learned how to compute real time values using sensors.We learned the importance of sensor fusion, real-time data processing, and designing systems that are easy to understand during high-stress situations.\n\n**What's next**\nNext, we plan to improve accuracy with adaptive thresholds, add more sensors, and expand the system into a portable or wearable solution for continuous protection.\n\n",
    "prize": "Winner Best use of Gemini AI",
    "techStack": "ai, express.js, node.js, python, react, typescript",
    "github": "https://github.com/alwinalbert/gorgan",
    "youtube": "https://www.youtube.com/watch?v=MFh-un-HZPs",
    "demo": null,
    "team": "harshadithyan, Devika Vinod, Alwin Albert, Sreeram Athrij",
    "date": "2025-12-17",
    "projectUrl": "https://devpost.com/software/upside-down-survival"
  },
  {
    "title": "Compass",
    "summary": "Compass is a data-driven application designed to provide users with insights and recommendations based on complex datasets. By leveraging machine learning and advanced data visualization, it aims to guide users in decision-making processes across various domains.",
    "description": "**What it does**\nCompass is a daily check-in and simulation app that is a personal model of how your habits influence your mood and then lets you interact with that model. Each day, you log a compact set of signals. You log your mood score, how long you slept, how much you exercised, how much time you spent with other people, how much you were on screens late at night, and an optional free-text journal. Behind the scenes, Compass treats that as a time series and learns how ‚Äúyesterday‚Äù predicts ‚Äútoday.‚Äù It then surfaces the three habits that appear to move your mood the most for you which turn coefficients into human-readable statements like ‚Äúfor you, each extra hour of sleep is associated with about +0.6 mood tomorrow‚Äù or ‚Äúextra late-night screen time tends to pull mood down the next day.‚Äù The app includes\n\n**Inspiration**\nWe kept coming back to one simple but important question: ‚ÄúFor me, personally, what actually improves my mood?‚Äù Most mental health apps give the same advice. They tell you to sleep more, exercise, go outside, and reduce screen time. Helpful, but vague. None of them can answer whether sleep sleep actually the strongest lever for me? None of them can answer whether how much doomscrolling after 11pm hurt my next-day mood? Compass was created from the idea of precision mental health. No more generic tips, but a personal map of cause‚Äìeffect patterns learned from your own life. We wanted something that feels like a flight simulator for behavior change, where you can test ‚Äúwhat if I slept 8h and halved my screen time?‚Äù before you actually do it.\n\n**How we built it**\nCompass is a three-part system. It is a Next.js frontend, a Supabase/Postgres data layer, and a Python/FastAPI.\n\n**Challenges**\nOne of the biggest challenges was designing a model that felt both ‚Äúsmart‚Äù and interpretable. A deep neural net on the time series would have been a black box, but a naive ‚Äújust linear regression‚Äù felt too weak. Therefore, we had to find middle ground that stayed mathematically possible, explainable, and shippable. We also had to solve the cold-start problem, that is where new users have almost no history, so a purely personal model is too noisy, yet we still want to give them useful feedback. Another subtle challenge was handling the time-series and ‚Äúcausal‚Äù story responsibly. Through using lagged predictors (yesterday ‚Üí today), the model is more causal-ish than same-day correlations, but it‚Äôs still observational. On top of all that we had to make sure everything fit properly across Next.\n\n**Accomplishments**\nI'm proud that Compass is a real end-to-end system. I'm equally proud that the stack (Next.js, Supabase, FastAPI, scikit-learn, Hugging Face transformers) is actually deployable and debuggable. Lastly, I'm grateful that this is something that people can use to help themselves.\n\n**What we learned**\nI primarily learned that large language models are strongest as interpreters, not oracles. I constrained the LLM to operate only on structured summaries of the numeric model. Thus, the prediction is within reproducible math while using the model‚Äôs strengths to generate explanations, contextual guidance, and gentle disclaimers. That separation gave us both safety and clarity.\n\n**What's next**\nI see Compass as a foundation for more modeling, including fully Bayesian hierarchical and time-varying coefficient models, so every effect size comes with uncertainty and the system can track how your sensitivity to different habits changes over time. We also want to embed explicit N-of-1 experiments, carefully integrate passive signals from wearables and phones, add clinician-facing and exportable views, and extend beyond mood to related dimensions like anxiety, energy, and stress resilience.\n\n",
    "prize": null,
    "techStack": "fastapi, lucide, next.js, numpy, numpyro, pandas, recharts, scikit, supabase, tailwind, transformers, typescript",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=6ADpWIHV5Q4",
    "demo": "https://compass-3.vercel.app/",
    "team": "Murari Ambati",
    "date": "2025-12-17",
    "projectUrl": "https://devpost.com/software/compass-tmyfx6"
  },
  {
    "title": "10xCoders",
    "summary": "10xCoders is a web application designed to enhance the coding skills of users by providing interactive coding challenges and collaborative learning environments. It connects developers of varying expertise levels, facilitating mentorship and skill development through gamified experiences.",
    "description": "**What it does**\n10xCoders is an all-in-one platform designed to help users learn, practice, and grow in the world of technology. It offers: Personalized Learning Paths ‚Äî Tailored roadmaps to fit each learner‚Äôs goals.\nCoding Practice ‚Äî Hundreds of real-world coding challenges to sharpen problem-solving skills.\nCareer Agent ‚Äî Smart recommendations and job matches based on user skills and interests.\nInterview Preparation ‚Äî Practice real interview questions from top tech companies.\nKanban Board ‚Äî Stay organized with a personal task manager.\nResume Maker & Enhancer ‚Äî Create and optimize tech-focused resumes with AI-powered feedback.\nTyping Speed Enhancer ‚Äî Build faster coding speed with interactive exercises. Everything you need ‚Äî from learning to landing your dream job ‚Äî is in one place.\n\n**Inspiration**\nThe journey to becoming a great programmer can often feel overwhelming ‚Äî scattered resources, unclear learning paths, and lack of mentorship make it hard for aspiring coders to grow consistently. We wanted to create one unified platform that bridges the gap between learning, practicing, and advancing a tech career ‚Äî helping learners become true 10x developers with structured guidance and real-world practice.\n\n**How we built it**\nWe built 10xCoders using: Frontend: React.js with Vite for lightning-fast performance and responsiveness.\nBackend: Node.js and Express.js for efficient API handling and authentication.\nDatabase: MongoDB for storing user data, progress, and resources.\nUI/UX: Tailwind CSS and ShadCN/UI for a modern, minimal, and adaptive design.\nAI Integration: OpenAI API for resume enhancement, personalized learning insights, and interview preparation. The platform‚Äôs architecture focuses on scalability, speed, and personalization.\n\n**Challenges**\nIntegrating multiple features while maintaining a smooth user experience.\nDesigning a personalized roadmap system that adapts to different user goals.\nEnsuring accurate AI-driven feedback for resumes and interview prep.\nBalancing data storage efficiency with real-time updates for progress tracking.\n\n**Accomplishments**\nSuccessfully developed a modular and scalable platform integrating learning, career, and practice tools.\nBuilt a clean and responsive UI that works seamlessly across all devices.\nImplemented AI-powered features that add real-world value to users‚Äô growth.\nCreated an ecosystem that motivates learners to level up daily and stay consistent.\n\n**What we learned**\nHow to merge education, productivity, and career development into one cohesive platform.\nThe importance of personalization and user experience in edtech tools.\nLeveraging AI and automation to deliver intelligent career insights and faster learning outcomes.\nTeam collaboration and version control using GitHub and agile workflows with Kanban boards.\n\n**What's next**\nLaunching mobile app versions for Android and iOS.\nAdding community discussions and mentorship programs for peer learning.\nIntroducing real-time code collaboration and AI-powered coding tutor.\nPartnering with tech companies for direct job placement opportunities.\nExpanding to include Web3, Data Science, and DevOps career tracks.\n\n",
    "prize": "Winner 2nd Runner up",
    "techStack": "express.js, javascript, mongodb, node.js, react.js, tailwind",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=L7xEhdAI4Lo",
    "demo": "https://10x-coders-main.vercel.app/",
    "team": "Sameer .",
    "date": "2025-12-15",
    "projectUrl": "https://devpost.com/software/10xcoders-yx83dt"
  },
  {
    "title": "CppShell",
    "summary": "TECHNICAL HIGHLIGHTS: The project utilizes C++ for its performance and efficiency, integrates Git for version control, and is developed on a Linux platform, indicating a focus on a robust environment. The implementation of the readline library suggests advanced features like command line editing and history management, enhancing the usability of the shell.",
    "description": "**What it does**\nCppShell is a fully functional command-line interpreter that replicates core shell behaviors: REPL Cycle: Processes user input in real-time.\nInfinite Pipelines: Chains commands (e.g., cmd1 | cmd2 | cmd3) via kernel pipes.\nI/O Redirection: Supports output redirection (>) and append mode (>>).\nPersistent History: Saves and loads session history automatically.\nAdvanced Parsing: Handles single quotes, double quotes, and backslash escapes correctly.\n\n**Inspiration**\nI wanted to stop trusting the \"magic\" of the terminal and understand the machinery. For the Build From Scratch hackathon, I challenged myself to build a POSIX-compliant shell without high-level shortcuts like system(). My goal was to use the raw system calls that power the OS.\n\n**How we built it**\nBuilt in C++ on Linux using three main architectural pillars: The Parser: A custom state-machine tokenizer that handles complex quoting and escape sequences character-by-character.\n Process Management: Uses fork() to clone the shell and execvp() to perform a \"brain transplant,\" replacing the process memory with the target program.\n Pipeline Logic: A \"bucket brigade\" loop that creates pipe() channels and uses dup2() to surgically rewire STDOUT and STDIN between independent processes.\n\n**Challenges**\nZombie Processes: Commands would finish but remain in the process table. I had to implement careful waitpid loops to reap them.\nDeadlocks: In multi-stage pipelines, forgetting to close a single write-end caused the reader to hang forever.\nParsing: Distinguishing between literal strings inside 'single quotes' vs escapable strings inside \"double quotes\" required multiple rewrites.\n\n**Accomplishments**\nZero High-Level Dependencies: Built entirely with raw POSIX syscalls (unistd.h, sys/wait.h, fcntl.h).\nRobust Pipelines: Can handle arbitrarily long command chains.\nUsability: With persistent history and auto-complete, it feels like a real tool.\n\n**What we learned**\nFiles are Integers: I gained a deep appreciation for how Linux treats File Descriptors.\nCost of Forking: Learned how expensive yet powerful cloning a process state is.\nComplexity of Text: Interpreting a simple string correctly is harder than it looks.\n\n**What's next**\nSignal Handling: Implementing Ctrl+C handling.\nEnvironment Variables: Adding export and variable expansion ($VAR).\nScripting: Adding support for .sh script execution.\n\n",
    "prize": "Winner 1st Place ‚Äî Build From Scratch Champion",
    "techStack": "c++, git, linux, readline",
    "github": "https://github.com/vb8146649/codecrafters-shell-cpp",
    "youtube": "https://www.youtube.com/watch?v=W3uO1kWf1Rg",
    "demo": null,
    "team": "Vishal Vishal",
    "date": "2025-12-25",
    "projectUrl": "https://devpost.com/software/linux-terminal"
  },
  {
    "title": "Wastewise",
    "summary": "TECHNICAL HIGHLIGHTS: Key technical implementations include the use of AI algorithms to analyze user data and generate personalized waste management tips, as well as a responsive front-end developed with HTML, CSS, and JavaScript. The integration with Gemini technology likely enhances user interaction and experience.",
    "description": "**What it does**\nWasteWise is a comprehensive AI-powered platform that transforms waste management into an engaging, rewarding experience:\n\n**Inspiration**\nIn a world drowning in electronic waste and struggling with proper waste disposal, we saw an opportunity to make a difference. Every year, 50 million tons of e-waste are generated globally, yet only 20% is properly recycled. This results in $62.5 billion worth of recoverable materials lost annually, while toxic materials contaminate our soil and water. WasteWise was born from a simple question: What if recycling could be as easy as shopping online? We envisioned a platform that doesn't just tell you to recycle ‚Äì it shows you how much your waste is worth, connects you with verified recyclers, educates you about environmental impact, and rewards you for making sustainable choices. Our mission is to bridge the gap between environmental responsibility and everyday convenience, making it easier\n\n**How we built it**\nFrontend Stack: Pure HTML5, CSS3, and vanilla JavaScript for lightweight performance\nResponsive design with custom animations and modern card-based UI\nGlassmorphism effects and animated loading states for enhanced UX\nDynamic form validation and progressive enhancement patterns AI & Machine Learning: Google Gemini API (gemini-2.0-flash-exp model) via REST endpoints\nImage recognition for waste classification\nNatural language processing for chatbot interactions\nCustom prompt engineering for accurate waste identification and pricing insights Data Architecture: LocalStorage for user preferences, chat history, and estimate tracking\nSessionStorage for form state management\nComprehensive pricing database with 50+ item categories\nHigh-value items database with per-piece pricing logic\nCondition mult\n\n**Accomplishments**\n‚úÖ Fully Functional AI Integration: Three distinct AI-powered features (classifier, estimator, chatbot) working seamlessly with real Gemini API calls and intelligent fallbacks ‚úÖ Sophisticated Pricing Engine: Realistic calculations with 50+ categories, high-value item recognition, condition assessment, and quantity bonuses ‚Äì featuring proper number formatting with thousands separators ‚úÖ Resilient Architecture: Built-in rate limiting, error handling, and local fallback responses ensure users always get value even when APIs are unavailable ‚úÖ Exceptional UX: Animated loading states, typing indicators, floating chatbot buttons, dynamic form validations, and glassmorphism effects create an engaging, modern experience ‚úÖ Production-Ready Code: Clean event handling, proper state management, comprehe\n\n",
    "prize": null,
    "techStack": "ai, css, gemini, html, javascript",
    "github": "https://github.com/Ranojitdas/Wastewise-Ecommerce-Website",
    "youtube": "https://www.youtube.com/watch?v=ey3tgQPmqYI",
    "demo": "https://edunest.me/Wastewise-Ecommerce-Website/index.html",
    "team": "Ranojit Das",
    "date": "2025-12-13",
    "projectUrl": "https://devpost.com/software/wastewise-eaws74"
  },
  {
    "title": "Location Scout",
    "summary": "Location Scout is a web application designed to help users discover and recommend ideal locations for various activities, utilizing Yelp's extensive database of businesses and services. By combining user preferences with location data, it aims to streamline the process of finding suitable venues for events, outings, or gatherings.",
    "description": "**What it does**\nLocation Scout helps small business owners identify strong brick-and-mortar locations. Users enter a business type and city, such as ‚Äúcoffee shop‚Äù and ‚ÄúBoston, MA.‚Äù The app analyzes Yelp data to map competitor density, scores neighborhoods using AI for saturation and market gaps, and ranks top opportunities. It also gives detailed analysis of recommended neighborhoods with interactive maps, demographic insights, competitor comparisons, review summaries, and service recommendations. This turns complex data into a clear checklist for differentiation.\n\n**Inspiration**\nLocation Scout was inspired by how messy and guesswork-heavy site selection is for small businesses. Most business owners rely on intuition, scattered Google searches, or expensive enterprise tools that are not practical for a single-location cafe, restaurant, or retail shop. The goal was to create a focused, self-serve tool that uses Yelp‚Äôs business data to give clear guidance on where to open and how to stand out.\n\n**How we built it**\nThe app was built with: Frontend: Next.js, React, TypeScript\nMaps: Leaflet for pins, clustering, and interactive bounds\nAPIs: Yelp Business Search API and Yelp AI Search API\nAI Processing: Nvidia LLM for parsing and structuring data\nUI: shadcn/ui and Tailwind CSS\nPerformance: Client-side caching to keep exploration fast\n\n**Challenges**\nPost-processing AI responses for the UI\nThe app needed structured outputs such as neighborhood scores and gap summaries, so I created parsing and validation steps to keep results consistent across different business types.\nOptimizing API calls and caching\nEach analysis required multiple Yelp and Yelp AI calls. Local caching with smart invalidation helped maintain a balance between speed, freshness, and API usage limits.\n\n**Accomplishments**\nA smooth flow from data to decisions\nUsers can move from a single search to neighborhood scoring, competitor mapping, and clear action steps within minutes.\nTurning Yelp AI into practical intelligence\nSimple API calls now produce insights that support real location decisions for small business owners.\nA fully responsive design\nThe app works just as well for detailed desktop analysis as it does for on-the-go exploration on mobile.\n\n**What we learned**\nSimple APIs can create meaningful impact\nWith the right structure, Yelp AI can provide the type of insights that were once available only to larger companies.\nClarity matters more than volume\nPulling large datasets is easy, but turning that information into useful guidance takes careful formatting and prioritization.\nThoughtful UX ties everything together\nThe flow from the landing page to the city overview, then into neighborhood detail views and personalized insights, feels natural even though the underlying logic is complex.\n\n**What's next**\nCustom analysis filters\nOptions for saturation, competitor density, foot traffic, budget ranges, and lease preferences.\nUser accounts and saved analyses\nA logged-in experience with history tracking and cross-city comparisons.\nAI-powered launch playbooks\nAutomatically generated 90-day plans for hours, pricing, marketing, and service strategy based on neighborhood needs.\nTeam collaboration\nShared analyses, comments, and collaborative launch planning.\nMobile-first exploration\nNative iOS and Android apps with geolocation to support real-time scouting. Yelp AI API client ID - I7BEU_lXBeNf7EfUKEbCvA\n\n",
    "prize": "Winner Second Place",
    "techStack": "nextjs, typescript, yelp",
    "github": "https://github.com/rahuls98/location-scout",
    "youtube": "https://www.youtube.com/watch?v=x1utF9G-AHE",
    "demo": null,
    "team": "Rahul Suresh",
    "date": "2025-12-15",
    "projectUrl": "https://devpost.com/software/location-scout"
  },
  {
    "title": "AuditShield.ai",
    "summary": "TECHNICAL HIGHLIGHTS: Notable implementations include the use of Node.js and PostgreSQL for robust backend development, React for a responsive frontend, and TanStack Query for efficient data fetching. The integration of Supabase for real-time databases and Tailwind CSS for a sleek user interface contributed to a seamless user experience.",
    "description": "12    \n\n\n\n      \n  AuditShield.ai: Transparent Navigation for Cashless Healthcare\n\nTeam Name: AuditShield\nContact Email(s): raushan22882917@gmail.com\nTeam Members:\n\n\nRaushan (age 19‚Äì24), University student\nMausam Kumari (age 19‚Äì23), University student\nCountry / Region: India (Bihar)\n\n\n\n\nProblem Statement\n\nWho is affected\n\n\nPatients and families seeking major but non-emergency medical procedures (such as knee or joint replacements) in private hospitals‚Äîespecially middle- and lower-income households unfamiliar with hospital billing practices and government health schemes.\nElderly patients and their caregivers are the most affected, as decisions are made under emotional and medical pressure.\n\n\nWhere / when it happens\n\nThis problem occurs during:\n\n\nHospital selection\nPre-surgery consultations\nPost-surgery billing\nEspecially in private hospitals where pricing, inclusions, and exclusions are not disclosed clearly and upfront.\n\n\nEvidence the problem is real (real incident)\n\nOn 14 November, I visited Nageshwar Ortho & Trauma Centre in Bhagalpur, Bihar with my father, whose knee condition had become extremely severe. After examination, the doctor recommended an immediate knee replacement surgery.\n\nI clearly asked about:\n\n\nThe total cost of surgery and hospital stay\nWhether Ayushman Bharat (PM-JAY) would apply\n\n\nThe doctor quoted ‚Çπ1,65,000 and mentioned that medicines and treatment would be managed by the hospital, implying that extra costs would be minimal.\n\nBased on this, we agreed.\n\nHowever, after surgery‚Äîwhen ‚Çπ1,65,000 was already paid‚Äîwe were told:\n\n\nMedicine charges were ‚Çπ7,000/day\nBed charges were ‚Çπ1,500/day\nAll injections, saline, and consumables would be charged separately\n\n\nAt this point, we had no choice; the surgery was already done. Due to extended stay and unexpected charges, the total cost increased drastically, and I was forced to take a loan of nearly ‚Çπ4,00,000.\n\nThis happened not due to lack of treatment but due to missing cost transparency and missing process guidance.\n\nWhy this reflects a systemic issue\n\n\nHospital ‚Äútotal cost‚Äù quotes often exclude critical components\nPatients do not know what is included vs. excluded\nGovernment scheme eligibility is unclear\nOnce surgery starts, decision-making power is lost\nThis is a widespread healthcare navigation and transparency failure, not an isolated incident.\n\n\n\n\nPurpose\n\nHealthcare decisions involve trust, urgency, and vulnerability. Patients deserve complete information before treatment begins, not after.\n\nThis matters because:\n\n\nMedical debt can financially damage families for years\nIncomplete information undermines informed consent\nExisting systems favor institutional knowledge over patient understanding\n\n\nThis project is driven by personal lived experience and aims to prevent families from facing financial distress due to unclear hospital processes.\n\n\n\nProposed Solution\n\nAuditShield.ai helps patients access transparent and affordable healthcare by guiding them through:\n\n\nChoosing the right",
    "prize": "Winner 1st",
    "techStack": "gemini, google, node.js, postgresql, react, react-router, shadcn/ui, supabase, tailwind-css, tanstack-react-query, typescript, vite",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=3_9FBeWEHX8",
    "demo": "https://fair-charge-ai.vercel.app/",
    "team": null,
    "date": "2025-12-12",
    "projectUrl": "https://devpost.com/software/auditshield-ai"
  },
  {
    "title": "HYROX Pulse",
    "summary": "HYROX Pulse is a fitness-oriented digital application that leverages augmented reality and interactive elements to enhance user engagement in workout regimes. By integrating advanced technologies, it aims to provide users with real-time performance metrics and personalized training experiences.",
    "description": "**What it does**\nHYROX Pulse adopts Spectacles technology to digitize the physical grind. We turn any open space into a smart, connected arena‚Äîno judges or training partners required. KEY FEATURES Autopilot Coaching (The Use Case): Take Station 4 (80m Burpee Broad Jumps). The AR layer actively counts every rep and tracks distance covered. The moment you hit 80m, the Spectacles \"nudge\" you immediately into the run transition. No counting, no cheating‚Äîjust performance. Ghost Mode (The Competition): Leveraging HYROX's standardized format, we visualize the leaderboard. Train against AR Avatars of friends abroad, World Champion pros, or your own \"Personal Best\" shadow. Split time projection in your physical environment. THE IMPACT We bridge the gap between physical exertion and digital feedback, turning a solo\n\n**Inspiration**\nHYROX is the \"World Series of Fitness.\" It‚Äôs a standardized global race format blending functional strength and endurance. The formula is simple but brutal: 8km of running separated by 8 different functional workouts (like sled pushes, rowing, and wall balls). Because every race globally uses the same weights and distances, it creates a perfectly level playing field‚Äîa massive, unified leaderboard where seconds separate the amateur from the elite. Chandran, Roland and Min having a conversation about sports.. marathon running.. before we settled on HYROX. We wanted to introduce  AR  into, and with a bit of imagination, one can use Hyrox in a practice and training environment without a judge, to feel as though we are competing with others.\n\n**How we built it**\nScripts with help of Github Copilot and Cursor\nLens Studio Lens for Snap Spectacles\n\n**Challenges**\nIntegration of different parts in the final project\n\n**Accomplishments**\nHand and head recording and replay\nSplit time projection integrated into your physical environment\nBurpee counter detects phases of your burpee broad jump exercise\nGPS + IMU based  performace indicators for long distance running\n\n**What we learned**\nWorking with mixed reality like Spectacles\nHow to mix physical with digital\n\n**What's next**\nSupabase integration for persistent storage\nLeaderboards selection of people to train with\nMore complete body tracking including context replace of weights, slide etc.\nAutomated wall ball judging with sphere detection\n\n",
    "prize": "Winner 1st Place",
    "techStack": "lensstudio, typescript",
    "github": null,
    "youtube": null,
    "demo": null,
    "team": "Roland Smeenk, Chandran N, min lai",
    "date": "2025-12-13",
    "projectUrl": "https://devpost.com/software/hyrox-ar"
  },
  {
    "title": "NutriCare Agents",
    "summary": "TECHNICAL HIGHLIGHTS: Notable technical implementations include the use of Next.js for server-side rendering and optimized performance, Firebase for real-time database management, and integration with Google Maps API for location-based services. The use of React and Tailwind CSS also ensured a responsive and visually appealing user interface.",
    "description": "Logo\n    \n\n              \n  \n\n\n\n            \n        \n          \n  \n\n        \n    \n      Logo\n    \n\n              \n  \n\n\n\n            \n        \n          \n  \n\n        \n    \n      Logo\n    \n12    \n\n\n\n      \n  Inspiration\n\nThe core inspiration for NutriCare Agents was to bridge the gap between scientific nutritional knowledge and culturally relevant, personalized advice. We wanted to build a system that goes beyond generic recommendations, offering tailored meal suggestions based on individual health conditions, dietary preferences, and budget constraints.\n\nOur vision was to make personalized nutrition guidance accessible to everyone, including people who face financial or physical barriers to traditional nutrition services. Ultimately, NutriCare Agents aims to improve public health and quality of life through evidence-based, culturally aware nutrition intelligence.\n\n\n\nWhat We Learned\n\nThroughout the development of NutriCare Agents, we gained valuable insights into AI orchestration, multi-modal systems, and the challenges of integrating advanced technologies into real-world applications.\n\nAI Integration\n\nOrchestrating multiple AI agents and enabling them to collaborate seamlessly was both challenging and rewarding. We learned to fine-tune AI models for specialized tasks ‚Äî such as meal recommendation or nutrition analysis ‚Äî and design a communication layer that keeps them synchronized.\n\nGraph Neural Networks (GNN)\n\nWe explored GNNs to personalize meal recommendations using user health data and preferences. By modeling relationships between Vietnamese dishes, ingredients, and dietary constraints, GNNs helped us uncover meaningful patterns that power our recommendation engine.\n\nCultural Sensitivity\n\nNutrition is deeply cultural. We realized the importance of integrating Vietnamese culinary traditions, regional differences, and local health concerns into the logic of our recommendations. This ensured our system remained relevant and trusted by real users.\n\nMulti-Modal Design\n\nSupporting text, voice, and potentially image/video inputs pushed us to think more broadly about AI‚Äìhuman interaction. Creating an intuitive, accessible experience‚Äîespecially for users with disabilities‚Äîwas a major design priority.\n\n\n\nHow We Built the Project\n\nNutriCare Agents was built by integrating a powerful stack of Google AI technologies and open-source frameworks. The system architecture includes:\n\nFrontend\n\n\nBuilt with Next.js + React\nStyled with Tailwind CSS for a modern, responsive UI\nFirebase Authentication for secure user management\n\n\nBackend\n\n\nFirebase Cloud Functions for a scalable, serverless backend\nFirebase Realtime Database for storing user data and preferences\nGoogle Cloud Storage for managing large media files (e.g., images)\n\n\nAI & Machine Learning\n\n\nMulti-agent framework built using LangChain + Google GenAI SDK\nSpecialized agents for:\n\n\nNutritional grounding\nLogical inference\nMeal recommendation\n\nGraph Neural Networks (GNNs) used to model Vietnamese dishes and",
    "prize": null,
    "techStack": "api, firebase, gemini, google, google-cloud, javascript, maps, next.js, openai, places, react, tailwind",
    "github": "https://github.com/technoob05/NutriCare_Agents",
    "youtube": "https://www.youtube.com/watch?v=6d3UbajcSWw",
    "demo": "https://nutricareagents.learningaiwithlosers.com/",
    "team": "HUY QU√ù MINH, Dao Sy Duy  Minh",
    "date": "2025-12-11",
    "projectUrl": "https://devpost.com/software/nutricare-agents-oefn4b"
  },
  {
    "title": "Mintly",
    "summary": "Mintly is a digital platform designed for seamless authentication and interaction using QR codes, enabling users to manage and verify their identities in real time. By leveraging a robust database and a user-friendly interface, Mintly aims to simplify user authentication processes and enhance security in various applications.",
    "description": "Home Screen\n    \n\n              \n  \n\n\n\n            \n        \n          \n  \n\n        \n    \n      Receive Payment QR Code (Participant)\n    \n\n        \n          \n  \n\n        \n    \n      Create Event Step 1\n    \n\n        \n          \n  \n\n        \n    \n      Create Event Step 2\n    \n\n        \n          \n  \n\n        \n    \n      Organizer's Dashboard\n    \n\n        \n          \n  \n\n        \n    \n      Log In \n    \n\n        \n          \n  \n\n        \n    \n      Participant's Walet\n    \n\n        \n          \n  \n\n        \n    \n      Participant Join Event\n    \n\n        \n          \n  \n\n        \n    \n      Home Screen\n    \n\n              \n  \n\n\n\n            \n        \n          \n  \n\n        \n    \n      Receive Payment QR Code (Participant)\n    \n\n        \n          \n  \n\n        \n    \n      Create Event Step 1\n    \n\n        \n          \n  \n\n        \n    \n      Create Event Step 2\n    \n\n        \n          \n  \n\n        \n    \n      Organizer's Dashboard\n    \n\n        \n          \n  \n\n        \n    \n      Log In \n    \n\n        \n          \n  \n\n        \n    \n      Participant's Walet\n    \n\n        \n          \n  \n\n        \n    \n      Participant Join Event\n    \n\n        \n          \n  \n\n        \n    \n      Home Screen\n    \n123456789    \n\n\n\n      \n  The idea for Mintly was inspired by large-scale events like Tomorrowland, which use their own digital currencies to create fast, cashless, and immersive experiences. We realized that while these systems exist for major festivals, smaller communities‚Äîschools, student clubs, robotics competitions, charity events, and local fairs‚Äîstill rely on cash or improvised solutions that are inefficient and impossible to track.\nWe wanted to give everyone the power to create their own currency and experience how a real economy works, even on a small scale. That vision led to Mintly: a platform where anyone can mint a temporary digital currency and run a complete micro-economy for an event in just minutes.\n\nMintly is a full-stack web application that allows organizers to create events with custom-branded currencies, define exchange rates, set event durations, and distribute starting balances to participants. Attendees join by scanning a QR code and instantly receive a digital wallet‚Äîno account required. They can pay vendors, send funds to other participants, and earn badges, while organizers track everything in real time through a live analytics dashboard.\n\nThe platform supports secure transactions, real-time balance updates, vendor management, reward systems, and automatic event expiration, ensuring that each currency exists only for the duration of the event.\n\nWe built Mintly using React, TypeScript, and Tailwind CSS on the frontend, with Supabase and PostgreSQL powering the backend. Supabase Authentication and Row Level Security ensure that only authorized users can access sensitive data, while all financial operations run through secure database functions to guarantee atomic transactions and prevent balance inconsistencies. Real-time updates are",
    "prize": null,
    "techStack": "authentication, context, css, database, definer, dom, eslint, figma, framer, functions, github, level, lovable, motion, postgresql, qrcode.react, query, react, realtime, rls), router, row, security, shadcn/ui, sql, tailwind, typescript, vite, yudiel/react-qr-scanner, zod",
    "github": "https://github.com/CristosMpi/mintlyfin.git",
    "youtube": "https://www.youtube.com/watch?v=T2XR2lIDmw8",
    "demo": "https://mintlyfin.lovable.app/",
    "team": "Christos Mpirmpos",
    "date": "2025-12-17",
    "projectUrl": "https://devpost.com/software/mintly"
  },
  {
    "title": "Aegis.ai",
    "summary": "TECHNICAL HIGHLIGHTS: The project is built with a combination of CSS3 and HTML5 for a modern user interface, while the core functionality relies on natural language processing techniques. The integration of these technologies into a web extension format highlights its practicality and user-friendliness.",
    "description": "Inspiration\n\nThe internet has become increasingly unsafe, especially for students, job seekers, freelancers, and everyday users who face fake internships, phishing emails, investment scams, and fraudulent financial pages. These threats often look legitimate on the surface, making them easy to fall for.\nWe wanted to create a simple, instant, and trustworthy layer of protection that helps people understand whether a link, post, or webpage is safe before they interact with it. That idea became Aegis.ai.\n\nWhat it does\n\nAegis.ai is a browser extension that scans webpages, emails, job postings, and financial links to detect warning signs of scams or fraud.\nIt analyzes:\n\n\nSuspicious keywords, red-flag language, and impersonation patterns\nPhishing indicators\nFake job/internship templates\nFinancial risks such as unrealistic investment claims or unsafe payment pages\nMetadata and structural anomalies often found in scam sites\n\n\nThe result is shown as a simple rating:\nSafe ‚úî | Suspicious ‚ö† | Dangerous ‚úñ\n\nA Premium concept version adds deeper scanning, scam history tracking, and early alerts for new fraud patterns.\n\nHow we built it\n\nThe extension combines:\n\n\nJavaScript, HTML/CSS\nBrowser extension APIs\nAI/NLP models for language analysis\nCustom heuristics for financial and phishing detection\nA modular structure that allows rapid updates as scam patterns evolve\n\n\nAll processing is lightweight, fast, and privacy-focused, running directly in the browser.\n\nTracks:\n‚úî Artificial Intelligence & Machine Learning\n‚úî FinTech & Digital Economy\n\nChallenges we ran into\n\n\nDesigning a detection system that works reliably across different types of webpages.\nKeeping the extension lightweight without compromising risk analysis.\nEnsuring clear communication of risk without overwhelming users.\nNavigating constraints around publishing and distributing browser extensions.\n\n\nAccomplishments that we're proud of\n\n\nBuilt a functional, user-friendly safety tool within a very short timeframe.\nAchieved consistent detection of phishing cues and suspicious financial content.\nDesigned an interface suitable for both tech and non-tech users.\nCreated a solution that blends AI, safety, and digital trust in a practical way.\n\n\nWhat we learned\n\n\nHow AI can be applied to real-world scam and fraud detection.\nHow online threats evolve, especially in job markets and digital finance.\nHow important UI clarity is for trust and adoption.\nHow browser extension architecture can be adapted quickly for security use cases.\n\n\nWhat's next for Aegis.ai\n\n\nPublishing on the Chrome Web Store after meeting all required policies.\nExpanding the Premium feature set with deeper analysis and dashboards.\nAdding OCR-based scanning for email attachments and images.\nIntroducing mobile support and integration with messaging apps.\nBuilding a community-driven database of newly reported scams.\n\n\n\n\n        \n    Built With\n\n    css3extensionhtml5natural-language-processing\n  \n\n        \n    Try it out\n\n    \n        \n  \n  GitHub Repo",
    "prize": null,
    "techStack": "css3, extension, html5, natural-language-processing",
    "github": "https://github.com/BruderTaubeWitchhut/aegis-ai.git",
    "youtube": "https://www.youtube.com/watch?v=HXqxVz5VN6I",
    "demo": null,
    "team": "Private user, Private user",
    "date": "2025-12-10",
    "projectUrl": "https://devpost.com/software/aegis-ai-8q3aph"
  },
  {
    "title": "Inkphony",
    "summary": "TECHNICAL HIGHLIGHTS: The project was built using advanced tools including Depth for spatial awareness, Meta for augmented reality experiences, PCA for data analysis, Quest for VR capabilities, and Unity for game-like interactivity. This combination allowed for a rich, multi-sensory experience that enhanced user engagement.",
    "description": "**What it does**\nInkphony allows you to write music on any surface around you and create endless instruments that you can play your music with until you find the right sound. Easily iterate using the handtracking note writing and scrubbing gesture to delete.\n\n**Inspiration**\nFrom our musical and XR backgrounds, we decided to create a highly creative tool to learn and experiment with instruments generated from your environment, the world becomes an instrument!\n\n**How we built it**\nUsing the latest Meta PCA (v81), which features time-stamped and stereo camera access together with Gemini API, ElevenLabs, and our advanced filtering/cleaning system, we were able to build an asynchronous, continuous background recognition that seamlessly generates icons and instruments dynamically from your surroundings.\n\n**Challenges**\nThe usual image recognition data-to-world aligning, which was solved by storing the depth frame together with the camera capture, to later align properly when the Gemini API result was ready. This allows us to get correct results aligned to the real world, even if we are moving our heads.\n\n**Accomplishments**\nThe seamless background object detection system, the smooth hand-tracking implementation that opens up many possibilities compared to being limited to the Logitech pen.\n\n**What we learned**\nDepth-align a camera frame to a depth frame with timestamps, using stereo camera access to create mind-bending deformations in real time!\n\n**What's next**\nThe top 1 next feature that we are implementing before publishing is saving and loading your songs and instruments. \nThen, have the capability to have multi-instrument compositions: \"Make the bass with an apple sound, the rhythm with a bottle sound, and the melody with a cup!\"\nAfter that, the ability to share songs with other users, and being able to listen to other people's compositions and instruments!\n\n",
    "prize": "Winner Best Lifestyle Experience",
    "techStack": "depth, meta, pca, quest, unity",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=T6NyCAvU0sk",
    "demo": "https://www.meta.com/s/8YgVAmyJM",
    "team": "PCA-AI integration, stretching",
    "date": "2025-12-09",
    "projectUrl": "https://devpost.com/software/inkphony"
  },
  {
    "title": "OpenSoundLab",
    "summary": "TECHNICAL HIGHLIGHTS: Key technical implementations include the use of C and C# for robust audio processing, integration with Unity for creating engaging user interfaces, and innovative use of passthrough technology to blend virtual and real-world audio experiences. The mirror component might suggest a unique way of reflecting sound dynamics based on user interaction.",
    "description": "**What it does**\nOpenSoundLab (OSL) is a spatial modular synthesizer and creative toolkit for Meta Quest headsets, designed to facilitate immersive sound creation and exploration in virtual reality. It offers a range of features including over 30 modular devices and spatial sound design, enabling users to craft rich auditory experiences and music within mixed-reality environments.\n\n**Inspiration**\nMy inspiration for this project lies in many years of producing music, audio installations, working with Eurorack modular synthesizers and teaching creative technologies at the Academy of Art and Design Basel (HGK). The start was the pandemic and a funding that I received for creating a VR sound laboratory based on SoundStage VR for educating students in sound design via video tutorials at home.\n\n**How we built it**\nIt is an open-source fork of SoundStage VR since that app was abandoned in 2017. OpenSoundLab is built with Unity in C# and audio DSP codes in C/C++ with Meta APIs and Mirror Networking for the multiplayer.\n\n**Challenges**\nAdding the multiplayer features were definitely the most challenging aspects, especially syncing the audio engine in low latency over the network.\n\n**Accomplishments**\nOffering a mixed-reality first, colocative, spatialized sound design and media arts experience that now offers hands as an input modality thanks to this hackathon!\n\n**What we learned**\nToo much to list... :)\n\n**What's next**\nStore release in Q1 2026, adding more modules and more video tutorials, looking forward to seeing what people do with it! And providing instructions for other developers to create their own modules and modify OpenSoundLab, with a special focus on agentic coding. We will also continue to play gigs with OpenSoundLab and enhance the tool for new art pieces, now with a focus on hands interactions. http://www.sphericals.io\n\n",
    "prize": "Winner Judge‚Äôs Choice",
    "techStack": "c, c#, mirror, passthrough, unity",
    "github": "https://github.com/SphericalLabs/OpenSoundLab/tree/feature/hands",
    "youtube": "https://www.youtube.com/watch?v=230hncihTxI",
    "demo": null,
    "team": "Ludwig Zeller",
    "date": "2025-12-09",
    "projectUrl": "https://devpost.com/software/opensoundlab"
  },
  {
    "title": "The Tarot Experience VR - AI Edition",
    "summary": "The Tarot Experience VR - AI Edition is an innovative virtual reality application that combines tarot card reading with artificial intelligence, providing users with personalized tarot insights and immersive experiences. By leveraging AI-driven interactions, the project enhances traditional tarot readings, making them more dynamic and accessible in a virtual environment.",
    "description": "All Card Answer\n    \n\n              \n  \n\n\n\n            \n        \n          \n  \n\n        \n    \n      MR Passthrough - Final\n    \n\n        \n          \n  \n\n        \n    \n      Intro Portal\n    \n\n        \n          \n  \n\n        \n    \n      Platform Elevation\n    \n\n        \n          \n  \n\n        \n    \n      Final Flying Interaction\n    \n\n        \n          \n  \n\n        \n    \n      Portal\n    \n\n        \n          \n  \n\n        \n    \n      Present Portal Sun Card\n    \n\n        \n          \n  \n\n        \n    \n      All Card Answer\n    \n\n              \n  \n\n\n\n            \n        \n          \n  \n\n        \n    \n      MR Passthrough - Final\n    \n\n        \n          \n  \n\n        \n    \n      Intro Portal\n    \n\n        \n          \n  \n\n        \n    \n      Platform Elevation\n    \n\n        \n          \n  \n\n        \n    \n      Final Flying Interaction\n    \n\n        \n          \n  \n\n        \n    \n      Portal\n    \n\n        \n          \n  \n\n        \n    \n      Present Portal Sun Card\n    \n\n        \n          \n  \n\n        \n    \n      All Card Answer\n    \n12345678    \n\n\n\n      \n  The Tarot Experience VR began with a simple question: can a symbolic and introspective practice like tarot exist meaningfully in virtual reality? Tarot relies on imagery, atmosphere and presence. VR offered the possibility of creating a space where someone could pause, step inward and explore meaning in a way that feels personal, intentional and uninterrupted. Early testers responded strongly to the quiet pacing of the original release and shared that the experience offered something rarely found in VR: a contemplative space free from urgency or performance. That response shaped the direction and confirmed that stillness and depth could be enough.\n\nThe experience guides the user through a three card tarot reading representing Past, Present and Future. It begins in enhanced mixed reality, where the user enters one personal question. That question becomes the intention for the session and the interpretive lens for the reading. Once confirmed, the user transitions into an immersive symbolic environment where each card is revealed slowly and thoughtfully. The new AI Edition offers a personalised interpretation of each card based on the symbolism of the card, its position in the spread and the user‚Äôs question. When the reading reaches its conclusion, the experience returns gently to the user‚Äôs physical surroundings, allowing the meaning of the session to settle.\n\nThe experience was built in Unity for Meta Quest devices. The new AI component was integrated into the existing structure with care, ensuring it supported the reflective tone rather than overwhelming it. Much of the development process involved balancing pacing, presence and emotional tone so the AI responses felt natural, supportive and aligned with the original experience.\n\nOne of the challenges was ensuring that the AI spoke in a way that encouraged reflection rather than certainty or prediction. Another challenge involved timing. Even minor",
    "prize": "Winner Best Lifestyle Experience Runner-up",
    "techStack": "ai, ar, c#, chatgpt, immersive, llm, metaxr, openai, passthrough, quest, unity, vr",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=DnuDY5wk3rE",
    "demo": null,
    "team": "Adam James Malone",
    "date": "2025-12-09",
    "projectUrl": "https://devpost.com/software/the-tarot-experience-vr-ai-edition"
  },
  {
    "title": "Wishy Washy",
    "summary": "Wishy Washy is a creative project that likely focuses on the visualization or modeling of artistic concepts, possibly in the realm of 3D design or animation, given the tools used. The project may integrate portrait mode features, enhancing user interaction or visual appeal, which aligns with its recognition in best implementation awards.",
    "description": "**Inspiration**\nI set out to build something fast, simple to start, and instantly fun, a vertical, portrait-mode experience that anyone could jump into in seconds. I learned how crucial small feedback loops are in a mobile-friendly game: every movement, tap, and reward needed to feel immediate and satisfying. Throughout development, I also deepened my understanding of Noesis UI, custom scripting workflows, and integrating AI-assisted tools without losing the handmade charm.\n\n**Challenges**\nThe biggest challenge was time. Building a fast-paced game requires iteration, but limited playtesting windows made it tough to refine mechanics, observe players, and adjust systems on the fly.\n\n**What's next**\nI plan to release seasonal updates, expand progression systems, and introduce a customization store where players can personalize their characters. I‚Äôll continue refining window-cleaning and row-clearing rewards as the world grows.\n\n",
    "prize": "Winner Best Portrait Mode Implementation",
    "techStack": "blender, horizon-desktop-editor, noesis, photoshop, substance-painter, vs-code, zbrush",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=XxgwNhRKAm8",
    "demo": "https://horizon.meta.com/world/10237615339870207/?locale=en_US",
    "team": null,
    "date": "2025-12-08",
    "projectUrl": "https://devpost.com/software/wishy-washy"
  },
  {
    "title": "Pack Attack",
    "summary": "TECHNICAL HIGHLIGHTS: Pack Attack was built using advanced tools such as Unreal Engine 5, which allowed for high-quality graphics and immersive gameplay experiences. The use of software like Adobe, Blender, Photoshop, and Substance Painter contributed to the game's visual appeal and detailed asset creation.",
    "description": "**What it does**\nPack Attack is a cooperative party game that encourages players to work as a single unit. It brings people together by removing the friction of traditional multiplayer setups. The core loop is simple but challenging: you have to manage a futuristic delivery line. \nThe MR player checks what the delivery robots need, tells it to their couch-colleagues who have to pack the order and send it away. The parcel will land in the MR drop zone so the player in the headset can pick it up, drop it into the canon and shoot it towards the correct robot.\nIt encourages constant (and often loud) communication between all participants. It‚Äôs designed to be chaotic, but when you finally get into a flow state with your team, that shared sense of triumph is unmatched.\n\n**Inspiration**\nWe‚Äôve always been huge fans of couch co-ops and social games, nothing beats the feeling of screaming instructions at your friends, whether you‚Äôre in the same room or on a call. We wanted to capture that frantic, high-stakes energy found in games like Overcooked or Keep Talking and Nobody Explodes, where the gameplay is 10% mechanics and 90% pure communication. The specific theme came from watching the absolute chaos of the holiday season. We started wondering: with Christmas delivery times getting tighter and logistics getting crazier, what is actually happening behind the scenes? And more importantly, what will the chaotic, automated warehouse of the future look like? That‚Äôs where the idea for Pack Attack was born.\n\n**How we built it**\nWe wanted the game to look stylized and with history, so we started by creating all of the models in Blender and texturing them in Substance Painter to give them that vibrant look. The heavy lifting is done by Unreal Engine 5, which handles the game logic and environment. To ensure a low barrier to entry, we didn't want players to have to download a companion app. Instead, we built a responsive web interface using React. Players simply join the game via a website on their phones, turning their mobile device into a controller instantly: https://pack-attack.app !\n\n**Challenges**\nThe biggest hurdle was definitely the \"invisible\" stuff: networking and UX. Onboarding: We wanted it to be seamless. Ensuring data was sent instantly between the mobile web client and the headset without lag was tricky.\nControl Feel & Asymmetry:\nDesigning for asymmetric gameplay on the Quest was a unique challenge. We spent a lot of time iterating on how the MR player interacts with the world versus how the mobile players assist them. The Quest controls needed to feel physical and immersive through actually lifting boxes and firing cannons, while remaining intuitive enough to coordinate with teammates on 2D screens. If the MR mechanics fight the player, the game stops being \"fun-chaotic\" and just becomes frustrating.\n\n**Accomplishments**\nWe are incredibly proud of the accessibility. The fact that the MR player can allow their friends to join a session with ease, and be playing inside the game session within seconds without installing anything is a huge win for us. Seeing the seamless session setup and reliable performance between a high-fidelity engine like UE5 and a standard mobile browser feels like magic every time it works.\n\n**What we learned**\nCross-device communication: We learned a massive amount about WebSockets and optimizing data packets to keep latency low between the website and the main MR game.\nThe art of chaos: We learned that chaos needs to be designed. You can't just throw random things at players; you have to give them tools to solve problems, then slowly turn up the heat.\n\n**What's next**\nWe have a big roadmap ahead! This simple game loop has potential for a lot of exciting directions: Story Mode: We want to add a single-player, story-driven mode to explore the lore of this futuristic delivery company.\nMore Chaos: We plan to implement events, think machinery breaking down, conveyor belts reversing, or power outages that force players to adapt on the fly.\nMobile Depth: We want to give the mobile players more specific, tactile tasks on their screens (like rewiring circuits or stamping packages) to make their role feel even more distinct.\n\n",
    "prize": "Winner Best Social Game Honorable Mention",
    "techStack": "adobe, blender, photoshop, substance-painter, ue5, unreal-engine",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=YkWO2qdyeME",
    "demo": "https://pack-attack.app/",
    "team": "Andrew Douglas, Ines Hilz",
    "date": "2025-12-09",
    "projectUrl": "https://devpost.com/software/pack-attack"
  },
  {
    "title": "Fluxglove and VR",
    "summary": "Fluxglove and VR is an innovative project that integrates a gesture-controlled glove with virtual reality experiences. By utilizing sensors and programming, it allows users to interact with virtual environments in a more immersive and intuitive way, enhancing user engagement and interactivity.",
    "description": "**What it does**\nFluxGlove is a low-cost, haptic-enabled smart glove that translates natural hand movements into digital actions in real-time. Gestural Control: Users can control a mouse cursor, navigate web pages, or play games (like BGMI) simply by moving their hand in the air and bending their fingers.\nHaptic Feedback: The glove vibrates to simulate touch when interacting with virtual objects, adding a layer of immersion.\nAccessibility Mode: It acts as a Human-Computer Interface (HCI) for users who cannot grip a traditional mouse, allowing them to execute complex commands with simple finger flexes.\nWeb Integration: Through our custom platform, the glove connects directly to the browser for calibration and testing without heavy driver installation.\n\n**Inspiration**\nThe inspiration for FluxGlove (born from our initiative Sparsh Mukthi) came from two starkly different realities. On one hand, we saw the explosion of VR and the Metaverse, locked behind paywalls of hardware costing thousands of dollars. On the other, we witnessed the daily struggles of individuals with motor impairments who find standard keyboards and mice nearly impossible to use. We asked ourselves: Why can't the same technology used for gaming be used to give freedom to those with disabilities? We wanted to build a bridge‚Äîa device that offers \"Touchless Freedom.\" Our goal was to democratize motion capture, bringing high-end haptic feedback and precise tracking down to a sub-$50 price point, making it accessible for both the gamer in a dorm room and the patient undergoing rehabilitation\n\n**How we built it**\nWe adopted a hybrid hardware-software approach, leveraging our background in Electronics & Communication Engineering (ECE).\n\n**Challenges**\nThe \"Jitter\" Problem: The raw values from the flex sensors and MPU6050 were incredibly noisy. The cursor would shake uncontrollably. We spent days tuning the Complementary Filter and adding dead-zones in the code to stabilize the input.\nHardware Constraints: Mounting sensors on a flexible glove without breaking connections was a challenge. We had to innovate with cable management to ensure the user had full range of motion without snapping wires.\nLatency: Minimizing the delay between a physical hand movement and the screen action was critical for gaming. We had to optimize our baud rate and streamline the serial packet parsing logic.\n\n**Accomplishments**\nThe Price Point: We successfully built a functional prototype for under $50, whereas industry standards cost hundreds or thousands.\nWeb Integration: Getting the glove to talk to a web browser via WebSerial was a huge technical win for us.\nTeam Synergy: As a team of first-year students, coordinating hardware (soldering, circuit design) and software (full-stack web dev) remotely and efficiently was a massive achievement.\nLive Demo: Creating a site where judges can visually see the glove's tracking logic in real-time.\n\n**What we learned**\nSensor Fusion: We gained a deep understanding of how to mathematically combine data from different sensors to create a reliable \"truth.\"\nProduct Design: We learned that \"functional\" isn't enough; the device has to be comfortable. The physical form factor is just as important as the code.\nEmbedded Communication: We mastered UART and serial communication protocols, which are foundational for our ECE degree.\n\n**What's next**\nWireless Evolution: Migrating from Arduino Uno to ESP32 to introduce Bluetooth Low Energy (BLE) and remove the cables entirely.\nAI Integration: Implementing an LSTM (Long Short-Term Memory) neural network to recognize complex, custom gestures (like sign language) automatically.\nGame Development: Building a dedicated Unity SDK so game developers can easily add FluxGlove support to their titles.\nClinical Trials: Partnering with rehabilitation centers to test the \"Sparsh Mukthi\" accessibility features with patients recovering from hand injuries.\n\n",
    "prize": null,
    "techStack": "arduino, css, flex, html5, javascript, mpu6050, python",
    "github": "https://github.com/anuragkr-14/FLUXGLOVES",
    "youtube": "https://www.youtube.com/watch?v=AncF9D27v9A",
    "demo": "https://fluxglove.vercel.app/",
    "team": "Keshav Agrawal, Saket Kumar, Anshul Kumar, Anurag Kumar, Mohak Gupta",
    "date": "2025-12-08",
    "projectUrl": "https://devpost.com/software/fluxglove-and-vr"
  },
  {
    "title": "Into The Mind",
    "summary": "\"Into The Mind\" is an immersive virtual experience developed to explore and visualize complex mental landscapes, allowing users to navigate the intricacies of thought processes and emotions. By utilizing advanced technologies, the project aims to create a unique interactive journey that enhances understanding of mental health and cognitive experiences.",
    "description": "123    \n\n\n\n      \n  Into the Mind is a narrative/interactive adventure for up to 7 users on Quest 3, using hand tracking. \n\nNOTE :  Use pinch for teleport navigation\n\nIt is an ambitious work of fiction, both technically and artistically, a fantastical journey between reality and imagination that transports users into a team of adventurers selected to take part in a unique scientific experiment. Their host, Professor Genet, is the inventor of the ‚ÄúOmega‚Äù device, a vessel that allows navigation inside the human body at a microscopic scale. Users participate in this revolutionary program, designed to carry out the final tests of his technology before it is introduced to the public. For the first time, human beings will be able to travel inside Genet‚Äôs microscopic system and explore the human brain. The journey begins in the circulatory system and must take several detours before reaching the neural circuits, the climax of the voyage. However, unforeseen events will disrupt the exploration, leading the adventurers not only into Genet‚Äôs brain, but also to the heart of his imagination and his dreams.\n\nThe goal of this journey is to immerse users in a thrilling adventure with an educational dimension. Commentary from the scientists accompanying them aboard the Omega will provide the latest information on current research into the human body and the brain. Users can interact with the various consoles on board the vessel to monitor the professor‚Äôs vital signs or intervene at key moments in the story to trigger specific functions of the ship.\n\nA distinctive feature of the concept is the final shift into Professor Genet‚Äôs dream world, which takes users out of the vessel and into his imagination, creating a narrative loop reminiscent of Inception, where spectators may wonder, by the end of the experience, whether their journey inside the vessel wasn‚Äôt simply the professor‚Äôs childhood dream.\n\nBy placing scientific exploration and the imagination that inspires it on equal footing, both born from the same human brain, the story offers an allegory and a tribute to human genius, to its creative power whether applied to the arts or to science. Into the Mind shows how imagination nourishes science, and vice versa. It pays homage to the child who daydreams and who, as an adult, will change the world with their creations.\n\nWith Into the Mind, our goal is to strike a balance between scientific credibility and the fantastical aesthetic of the environments, in order to deliver a powerful sense of immersion and spark wonder throughout the experience. The vessel‚Äôs minimalist design, whose structure can even fade away, and the more discreet presence of characters during large-scale sequences are meant to highlight the spectacular microscopic landscapes, which will be dense, richly textured, and vibrant with color.\n\nOur ambition is to push the limits of what Quest 3 can deliver, drawing on our team‚Äôs expertise and our deep knowledge of the Unreal engine to develop advanced",
    "prize": "Winner Judge‚Äôs Choice",
    "techStack": "blender, c++, unreal-engine",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=A2s7b85T348",
    "demo": null,
    "team": null,
    "date": "2025-12-09",
    "projectUrl": "https://devpost.com/software/into-the-mind"
  },
  {
    "title": "Hardy World Poker",
    "summary": "Hardy World Poker is a digital poker platform designed to enhance the gaming experience through innovative technology and engaging features. Leveraging the Noesis framework, it aims to create a more interactive and immersive poker environment for players.",
    "description": "**What it does**\nHardy World Poker is a complete Texas Hold'em poker game featuring: Multi-tiered Buy-In System: Players can choose from 6 different cities with varying buy-in amounts (1K, 5K, 10K, 25K, 50K, 100K), each representing a major world city (Las Vegas, New York, London, Tokyo, Dubai, Monaco)\nFull Poker Gameplay: Complete Texas Hold'em implementation with betting rounds, blinds, side pots, all-in scenarios, and hand evaluation\nAI Opponents: Intelligent AI players with different personalities (Tight, Aggressive, Loose, Balanced) that make realistic decisions based on hand strength\nPersistent Statistics: Player stats are tracked and saved, including hands played, wins, losses, and chip counts\nGlobal Leaderboard: Top 10 players are ranked globally based on chip count, stored in world-level persisten\n\n**Inspiration**\nHardy World Poker was inspired by the desire to create an immersive, multiplayer poker experience within Horizon Worlds. The goal was to build a fully-featured Texas Hold'em poker game that combines classic card game mechanics with modern UI/UX design, complete with persistent player statistics, leaderboards, and multiple buy-in tiers representing major cities around the world.\n\n**How we built it**\nThe project was built using: TypeScript: Core game logic, state management, and AI decision-making\nNoesisGUI/XAML: UI framework for creating responsive, data-bound interfaces\nHorizon Worlds API: Network events, persistent storage, and world management\nObject-Oriented Architecture: Modular design with separate classes for:\n\n\nPokerOverlay: Main game controller and UI management\nPokerGameState: Game state management (players, pot, betting rounds)\nPokerActions: Hand evaluation and game controller logic\nPokerAI: AI decision-making with personality-based behavior\nServerManager: Server-side logic for persistent storage and leaderboard management\nPokerImageLoader: Optimized parallel image loading with exponential backoff\nPokerTimer: Turn timer with countdown and expiration handling Key Technical F\n\n**Accomplishments**\nComplete Poker Implementation: Successfully built a full-featured Texas Hold'em game with all standard rules and edge cases handled\nPerformance Optimizations: Achieved significant performance improvements through algorithm optimization and lazy loading\nRobust State Management: Implemented reliable game state management that handles complex scenarios like multiple all-ins and side pots\nBeautiful UI/UX: Created an intuitive, visually appealing interface with smooth animations and clear feedback\nPersistent Data System: Built a reliable system for saving player stats, chips, and leaderboard data that survives world restarts\nAI Intelligence: Developed AI players that feel realistic and challenging without being predictable\nCrash Protection: Implemented a robust system to prevent data loss even\n\n**What we learned**\nPerformance Matters: Pre-computation and caching can dramatically improve performance in computationally intensive operations like hand evaluation\nType Safety is Critical: Careful type validation prevents subtle bugs like string concatenation instead of numeric addition\nNetwork Optimization: Debouncing and batching network events significantly reduces server load and improves user experience\nState Management Complexity: Poker game state management requires careful handling of edge cases, especially with all-in scenarios and side pots\nUI/UX Design: Creating intuitive interfaces requires balancing information density with clarity, and animations can greatly enhance user experience\nPersistent Storage Patterns: World-level vs player-level storage requires different strategies, and careful desi\n\n**What's next**\nTournament Mode: Implement tournament-style gameplay with multiple tables and progressive elimination\nPlayer Customization: Allow players to customize their table themes, and card designs\nAchievement System: Add achievements and badges for various accomplishments (winning streaks, big pots, etc.)\nSocial Features: Implement friend lists, private tables, and chat functionality\nAdvanced Statistics: More detailed stats tracking including VPIP, PFR, aggression factor, and hand history\nMobile Optimization: Optimize UI for mobile VR devices\nSound Design: Add immersive sound effects and background music\nTutorial System: Interactive tutorial for new players to learn poker rules\nDaily Challenges: Daily challenges and missions to keep players engaged\nCross-World Leaderboards: Expand leaderboard syste\n\n",
    "prize": "Winner Best World that Leveraged Noesis",
    "techStack": "typescript",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=i31rOC24T1k",
    "demo": "https://horizon.meta.com/world/25301194902846864/?locale=en_US",
    "team": null,
    "date": "2025-12-08",
    "projectUrl": "https://devpost.com/software/hardy-world-poker"
  },
  {
    "title": "The Nanauts",
    "summary": "The Nanauts is an immersive experience project that leverages advanced web technologies to create an engaging environment using hand tracking and passthrough camera access. By integrating the Immersive Web SDK and WebXR, it allows users to interact with virtual elements in a seamless manner, enhancing the overall user experience.",
    "description": "**What it does**\nAs earth‚Äôs goodwill ambassador to The Nanaunts, you help three cute little robot explorers understand our culture and way of life by exploring your environment in mixed reality, answering their questions, and aiding in their quest for understanding, with the Professor guiding your experience.\n\n**Inspiration**\nMy game was inspired by Walt Disney‚Äôs example of trying to harness the latest in emerging technology to create characters and stories with real heart. Therefore my goal lately has been to incorporate AI services for unique gameplay.\n\n**How we built it**\nfrom a base Immersive Web SDK project for its many common XR components\nused Immersive Web Emulator for rapid iteration in the browser\nused Meta Quest scene understanding to dynamically construct a navmesh for the robot agents to navigate according to crowd sim, flocking and game rules\nused Passthrough Camera Access to generate custom environment maps\nGave the player the ability to speak directly to the characters, using a serverless Meta Llama 3.3 endpoint hosted on AWS, which can judge the intent and emotional sentiment of speech given certain game conditions\nTwo interaction modes using either controllers or hand tracking with natural microgestures.\nUsed  the WebXR Hit Test API to get real depth to open portals on tracked surfaces.\nNavigation-driven procedural animations to impart charac\n\n**Challenges**\nI attempted to leverage SAM 3 and SAM-3D-Objects AI models (released mid-November) to recognize and react to more objects in the real-world (https://www.linkedin.com/feed/update/urn:li:activity:7399101129554419712/) but camera intrinsics on Quest browser are still experimental, and I ran into challenges with accuracy, and on a short timescale, I ultimately pivoted the concept to rely on core Quest technology + the more reliably and widely-available Llama.\n\n**Accomplishments**\nI completed all technical and creative work on my own in only three weeks, starting November 21. I‚Äôm proud of not only have having that design and creative intuition, but being an engineer who can see it through.\n\n**What we learned**\nI learned more about how to develop and design AI services for innovative gameplay versus solely production efficiencies.\n\n**What's next**\nI‚Äôd love to win the contest and have a good reason to keep investing in the technology and title, as I see tremendous possibilities for work and play in interacting with increasingly-intelligent virtual beings in your real-world environment as new AR glasses and computer vision solutions become available in coming years. I could see this becoming a virtual pet title, like a \"mixed reality Tamagotchi,\" encouraging replayability. I want to design for that future.\n\n",
    "prize": "Winner Best Immersive Experience Built with Immersive Web SDK",
    "techStack": "handtracking, hittest, iwsdk, llama3.3, passthroughcameraaccess, webaudioapi, webxr",
    "github": "https://github.com/jameskane05/nanauts",
    "youtube": "https://www.youtube.com/watch?v=7r-29fp4Nbg",
    "demo": "https://jameskane05.github.io/nanauts/",
    "team": null,
    "date": "2025-12-09",
    "projectUrl": "https://devpost.com/software/nanauts"
  },
  {
    "title": "Falcon",
    "summary": "Falcon is an innovative social gaming project that leverages advanced technology to create an engaging and interactive gaming experience. It focuses on fostering community interaction and collaboration among players, potentially integrating features that enhance social connectivity within the gaming environment.",
    "description": "**Challenges**\nAchieving stable colocation for multiple players in small physical spaces.\nBuilding engaging controls for many diffrent categories of vehicles.\nEnsuring synchronized physics across multiplayer sessions.\nCreating track-building tools that remain accessible to casual players.\n\n**Accomplishments**\nA highly responsive MR flight system that feels natural and tactile.\nDistinct gameplay modes offering both competitive and cooperative experiences.\nA creative system for building custom race tracks inside the headset.\nA polished, social experience supported by expressive avatars and intuitive interactions. What we learned We learned how crucial spatial consistency is for social MR experiences, and how small variations in real-world environments can influence perceived gameplay quality.\nWe also gained valuable experience in building scalable multiplayer architecture, designing intuitive hand-based controls, and optimizing physics for mixed-reality conditions.\nMost importantly, we saw how creativity tools empower players and significantly increase engagement. What's next for FALCON We plan t\n\n",
    "prize": "Winner Best Social Game Runner-up",
    "techStack": "blueprints, c++, eos, meta, unreal-engine",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=CwuS1JffbFs",
    "demo": null,
    "team": "Wild Vision Games Wild Vision Games",
    "date": "2025-12-09",
    "projectUrl": "https://devpost.com/software/falcon-shgxn9"
  },
  {
    "title": "Boopi",
    "summary": "Boopi is an innovative project that leverages artificial intelligence and generative AI technologies to enhance portrait photography. By utilizing advanced algorithms and APIs, it aims to provide users with tools for creating stunning portrait images, likely with automated enhancements and artistic features.",
    "description": "**What it does**\nIn this endless runner, you help Boopi the frog jump from lily pad to lily pad by swiping up on your screen. Eat flies you find along the way to purchase cosmetics in the shop. Don‚Äôt fall into the water though, scary things lurk beneath, waiting for a yummy little snack!\n\n**Inspiration**\nWhen I was a kid, the most memorable game I played on my first-ever iPod Touch was those paper-throw games where you flick your finger to toss a crumpled paper ball into a trash bin. I‚Äôve always wanted to recreate that concept with my own twist, but this time, with a frog.That idea gave me the freedom to go wild with cute and funny creature animations, and to build a childlike, dreamy world that captures the nostalgia I associate with those early gaming memories.\n\n**How we built it**\nWe set out to make a stylized game with simple shapes and memorable characters. Since we wanted to appeal to casual players and teens, we focused on creating vivid, dreamy visuals to grab the viewer‚Äôs attention and a simple yet replayable game loop that keeps them coming back.\n\n**Challenges**\nWe mostly faced technical challenges along the way. For example, importing 3D assets and dynamically switching between animation states wasn‚Äôt fully supported, so we had to rely on a hacky workaround. Additionally, a few recent Horizon Worlds Editor updates broke some of our logic with Noesis and TypeScript, which forced us to rework parts of the code.\n\n**Accomplishments**\nWe were super excited after figuring out how to implement the hop mechanism, which is core to our gameplay. We managed to factor in the swipe angle and accurately translate it into the frog‚Äôs jump direction. I‚Äôm also proud of the visuals we developed using Blender‚Äôs texture-baking techniques to store realistic light and shadow information directly onto our textures. Early on, we also figured out instantiation for spawning endless scene elements, which helped shape the foundation of our gameplay loop.\n\n**What we learned**\nWe learned a lot about properly importing assets, textures, and animations from other 3D apps into the editor. We also gained valuable experience with the Noesis editor‚Äîsetting up leaderboards, dynamic quests, and the in-game store, all featuring playful animations.\n\n**What's next**\n1 - We really want the game to feel more social, so we plan to introduce a proper lobby system where players can walk around and hang out before the game begins.\n2 - Additionally, along with a game-changing multiplayer mode where you compete with friends to see who can hop the farthest, we also plan to add multiple biomes, with unique enemy types,  that you can seamlessly travel through. This keeps the visuals constantly interesting and makes players want to keep hopping for more!\n3 - We also plan to add a more fleshed out cosmetic store, add more quest variety, introduce more gameplay altering mechanics through obstacles, power ups, skill tree.\n\n",
    "prize": "Winner Best Portrait Mode Implementation",
    "techStack": "ai, genai, horizon-apis, noesis, typescript",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=sv3uZyZ-_TA",
    "demo": "https://horizon.meta.com/world/902445966279753/?locale=en_GB",
    "team": null,
    "date": "2025-12-08",
    "projectUrl": "https://devpost.com/software/boopi"
  },
  {
    "title": "MarketView VR",
    "summary": "TECHNICAL HIGHLIGHTS: MarketView VR was developed using Android Studio with Kotlin and Compose, showcasing modern Android development practices. The use of VR capabilities enhances user interaction, while the efficient architecture ensures smooth performance, even in resource-intensive scenarios.",
    "description": "**What it does**\nMarketView VR is a VR‚Äënative stock analysis workspace for Meta Horizon OS. The current P0 build focuses on three panels: a Portfolio Overview Panel showing daily price and percentage changes across tracked stocks, a Stock Detail Chart Panel for exploring intraday and historical trends, and a News Panel surfacing relevant headlines in context. There is no trading or execution; the app is intentionally research‚Äëonly, designed for calm, focused market monitoring in VR.\n\n**Inspiration**\nStock analysis has always been a multi‚Äëscreen activity: serious investors juggle charts, news, and multiple tickers at once, but are constrained by flat monitors and limited screen space. MarketView VR explores how virtual reality can turn that workload into an immersive, spatial environment, giving portfolio management the same ‚Äúmission control‚Äù feel that professionals get from multi‚Äëmonitor desks‚Äîwithout the hardware overhead.\n\n**How we built it**\nStarted with Figma Make for initial mockups, refined in Figma for detail. Connected to Meta Horizon MCP server to incorporate all VR design recommendations. Built as an Android app targeting SDK 36 (compatible with Android 24‚Äì36), using Jetpack Compose for UI. Divided into three layers: UI, business logic, and data‚Äîfetching real stock data via Alpha Vantage (historical prices) and Finhub (stock info/news). Multiple panels implemented via Android activities with default/recommended sizes, plus responsive resizing that auto-adjusts layout. Tested on Meta Quest 3S using Meta Quest Developer Hub. Future versions will migrate data logic to our own backend.\n\n**Challenges**\nThe challenge we ran into was designing specifically for VR, which we found very important and thus spent a lot of time on. Balancing information density with VR comfort was tough‚Äîtraditional finance tools compress too much text and data, which doesn't translate well to a headset. It took several iterations to perfect panel sizing, text scale, and contrast for readability without overwhelming users, while also resisting feature creep to stay disciplined for a true P0 release.\n\n**Accomplishments**\nThe result is a minimal yet capable P0 app that feels ‚Äúnative‚Äù to VR rather than a flat app ported into 3D. The three‚Äëpanel layout already supports a meaningful daily workflow for monitoring a stock portfolio, and the design leaves clear room for future expansion. The app respects VR ergonomics and interaction patterns while still feeling like a professional‚Äëgrade finance tool.\n\n**What we learned**\nDesigning for VR forced a rethinking of common finance UI patterns: typography, spacing, and hierarchy all need to be more generous, and interactions must work equally well with hands and controllers. It also highlighted how powerful spatial memory can be‚Äîusers quickly associate specific information with specific positions in their virtual workspace, which is a strong advantage over traditional setups.\n\n**What's next**\nThe next planned release will add portfolio filters (such as top movers and worst performers) and additional chart types, including candlestick, bar, and comparison views. Future iterations will introduce historical performance tracking based on purchase price and quantity, plus a voice‚Äëdriven AI assistant so users can ask questions like ‚ÄúWhat caused this drop in June?‚Äù while looking at a chart. Over time, the goal is for MarketView VR to grow from a focused research tool into a full spatial ‚ÄúBloomberg of VR‚Äù for individual investors.\n\n",
    "prize": "Winner Best Android Utility App",
    "techStack": "android, android-studio, compose, kotlin, mcp",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=XFBR9wV8kLA",
    "demo": null,
    "team": null,
    "date": "2025-12-09",
    "projectUrl": "https://devpost.com/software/marketview-vr"
  },
  {
    "title": "‚Äî Mise ‚Äî",
    "summary": "Mise is an immersive experience project designed to leverage spatial computing technologies to create engaging virtual environments. Utilizing the Meta Spatial SDK and Oculus hardware, the project integrates advanced AI capabilities through OpenAI, aiming to enhance user interaction and exploration within these digital spaces.",
    "description": "**Inspiration**\nWe have all had that moment: opening the fridge, staring at a random assortment of ingredients, and thinking, \"I have nothing to eat.\" I wanted to eliminate that decision fatigue entirely. I also wanted to solve a problem every home cook faces: trying to scroll through a recipe on a phone while your hands are covered in flour or sauce. Mise (short for Mise en place) bridges the gap between your physical kitchen and digital guidance. By leveraging the Meta Spatial SDK, I built an assistant that lives inside your kitchen without taking up physical space, making cooking effortless, clean, and genuinely fun.\n\n**How we built it**\nWe built Mise as an AR-first experience tailored for the Meta Quest 3/3S. We heavily utilized the Meta Spatial SDK to ensure high performance (60 FPS+) and reliable tracking.\n\n**Challenges**\nSpatial UI Alignment: Getting VR UI panels to feel naturally placed in a real kitchen without obstructing the user's view of dangerous items (knives/stoves) required careful design and calibration.\nLatency vs. UX: AI recipe generation and visual analysis can be slow. We optimized caching and \"debouncing\" AI calls so the user never feels stuck waiting inside the headset.\nInstruction Translation: Translating flat text recipes into 3D animated guidance was a major UX hurdle. We iterated several times to make the instructions feel native to spatial computing rather than just floating text.\n\n**What's next**\nWe are just getting started. Here is our roadmap for the next updates: [ ] Smart Grocery Lists: Auto-generation based on low inventory.\n[ ] Hardware Sync: Integration with smart fridges and ovens for auto-timers.\n[ ] Community Cookbook: Share your generated \"Recipe Disks\" with other users. Cooking shouldn't be stressful. With Mise, it's the future.\n\n",
    "prize": "Winner Best Immersive Experience Built with Spatial SDK",
    "techStack": "metaspatialsdk, oculus, openai",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=VQT4SE5cFgw",
    "demo": null,
    "team": "Private user",
    "date": "2025-12-09",
    "projectUrl": "https://devpost.com/software/mise-9wvaib"
  },
  {
    "title": "AptX",
    "summary": "AptX is an innovative project designed to streamline user authentication and enhance online learning experiences through a seamless integration of various technologies. By leveraging Firebase and Firestore for real-time data management, AptX aims to provide a robust platform for users to access selective online courses efficiently.",
    "description": "**What it does**\nAptX is a holistic web application designed as a complete learning and support ecosystem for students with Down syndrome, their teachers, and their guardians.\n\n**Inspiration**\nStudents with Down syndrome often thrive in learning environments that are visually engaging, patient, and present information in simple, single-concept steps. We noticed a gap in digital tools that are specifically designed to meet these needs from the ground up. Our mission began with a simple question: \"How can we use AI to create a truly adaptive and joyful learning experience for every child?\" We were inspired by: *Study that out of 7 children in a group 2 have down syndrome , only one or none can access standard education The unique learning profiles of students with Down syndrome, who benefit from repetition, multi-sensory input (visual, audio), and positive reinforcement.\n  The dedication of teachers and guardians who spend countless hours adapting standard curriculum.\n  The potent\n\n**Challenges**\nDesigning a UI that is extremely simple and accessible without feeling restrictive.\n  Fine-tuning AI prompts to generate content that is genuinely appropriate and encouraging for students with Down syndrome.\n  Integrating multiple user roles (Student, Teacher, Guardian) with distinct but interconnected experiences.\n  Ensuring real-time data synchronization between student actions and the guardian dashboard.\n\n**Accomplishments**\nBuilding a truly multi-user platform that serves the entire support system around a student.\n  Creating a user experience that prioritizes calmness, clarity, and positive reinforcement.\n  Successfully integrating multiple GenAI capabilities (text simplification, image generation, TTS, chat) into a single, cohesive application.\n  Designing an application that goes beyond academics to include emotional well-being as a core feature.\n\n**What's next**\nWe plan to grow the prototype into a full learning companion with: Deeper personalization of lesson content based on individual progress.\n  Gamified learning milestones and rewards to boost engagement.\n  Teacher-facing analytics to identify classroom-wide learning trends.\n  Offline mode for lessons to ensure accessibility in all environments.\n  Integration with school information systems (SIS).\n\n",
    "prize": "Winner Leaf Courses - Selective online course; Winner Finalist Certificate; Winner CodeCrafters",
    "techStack": "authentication, css, firebase, firestore, gemini, google-cloud, javascript, node.js, react",
    "github": "https://github.com/QueenMary100/AptX_",
    "youtube": "https://www.youtube.com/watch?v=4GWKm7U9Auc",
    "demo": "https://apt-x-nu.vercel.app/",
    "team": "Mary Mbithe",
    "date": "2025-12-07",
    "projectUrl": "https://devpost.com/software/aptx"
  },
  {
    "title": "BeanJam - Clinique Voudou",
    "summary": "BeanJam - Clinique Voudou is an innovative project that combines artistic expression with interactive technology, likely focusing on themes related to culture and creativity. While specific details on functionality are not provided, the project seems to engage users in a unique and immersive experience, potentially through a game or interactive platform.",
    "description": "**What it does**\nPlaying as Voudou Jacques the doctor, ghost patients arrive and ask for potion treatments. Potion ingredients appear on screen, Voudou Jacques has to collect the correct amount of each from the shelves and put it in the toad cauldron to create the potion. When the patient's potions are ready, they pay and leave. Rats‚Äô tails are a key ingredient which can be replenished by squishing rats which scurry across the screen.\n\n**Inspiration**\nWitch doctors practising magical voodoo medicine in the Louisiana Bayou in 1870s.\n‚ÄòOvercooked‚Äô game where the player has to prepare orders for customers using ingredients.\n\n**How we built it**\nUsed GitHub and Google Drive to collaborate.\nGame created in Godot.\nAll art created by hand in Aseprite.\nMusic composed on Roland synthesiser using Logic.\nLore created by team imagination and research of the setting, with help from Gemini.\n\n**Challenges**\nWe wanted to have some more dialogue between Voudou Jacques and the ghost patients, but instead put more attention on smooth and immersive gameplay.\n\n**Accomplishments**\nWe have an energetic playable character in an immersive and graphically appealing setting\nAtmospheric music has been composed by the team\nRat squishing is fun for all!\n\n**What we learned**\nAs a cohesive and collaborative team, we understood each others‚Äô strengths from the outset and supported each other to build on these over 24 hours and bring everything together. \nRed Bull seems like a great idea at the time.\n\n**What's next**\nBrackeys Gamejam in February, Uni of Kent hackathon in March 2026.\n\n",
    "prize": "Winner [HackSussex] The Palette of power!: Best Artistry in a project",
    "techStack": "gemini, github, godot, google-drive",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=5AAHgG30zzc",
    "demo": "https://drive.google.com/file/d/1yIeAGW1tjxrFyPJhuisdUfIMWRaCDoIq/view?usp=drive_link",
    "team": "Amelia Parsons",
    "date": "2025-12-07",
    "projectUrl": "https://devpost.com/software/beanjam-clinique-voudou"
  },
  {
    "title": "Mind Your Mana",
    "summary": "TECHNICAL HIGHLIGHTS: The game leverages C# programming for robust game logic and scripting, while navmeshplus enhances pathfinding capabilities, allowing for smooth character interactions and navigation across complex terrains. The integration of Unity's features contributes to a visually appealing and interactive gameplay experience.",
    "description": "**What it does**\nMind Your Mana is primarily about utilising specific spells to clear sections of a dungeon, by drawing the correct sigils on a hexagonal grid using the mouse. To clear an area, the player must defeat all the enemies in the area, then use the sigil shown on the exit door to cast the correct spell, which opens the door and allows them to progress. There are four spells used - a bolt spell, an explosion spell, a healing spell, and a teleport spell. Each spell consumes mana, which regenerates over time. The explosion and healing spell are 'area of effect' spells - the explosion can deal damage to both the player and any enemies in the blast range, and the healing spell can heal both enemies and the player. The bolt spell is a ranged attack, which deals significant damage should it make contact\n\n**Inspiration**\nMind Your Mana's general style was inspired by several top-down dungeon crawlers, with the artstyle influenced by games such as A Link to the Past. The specific idea of drawing sigils to cast different spells came from both the Minecraft Hex Casting Mod, as well as the 2016 Google Doodle Halloween game.\n\n**How we built it**\nOne of our stretch goals which we did not have time to achieve was allowing for multiplayer. While we did not achieve this goal, this approach lead us to design a very modular game, which is well suited for a multiplayer implementation in the future. Enemy navigation was especially aided by the use of a small library named NavMeshPlus, which provides some convenience Unity convenience functions for generating navigation meshes from tilemaps. Everything else however, was of our own creation. The sigil drawing system uses some relatively simple mathematics and line rendering to draw a path, which is reduced down to a sequence of integers. This sequence of integers is then used to determine the spell which was intended. The enemies make use of an MMORPG style enmity system, implemented as par\n\n**Challenges**\nThe main challenges we faced were with setting up the spellcasting system, as well as the shaders and other effects used in the spellcasting. Luckily, these issues were overcome, which allowed us to continue with completing the game. The creation of the navigation meshes was also a challenge, as none of us have had much experience with that in the past!\n\n**Accomplishments**\nWe learned new skills in developing with navigation meshes.\nWe managed to develop what we think is a very well-rounded game, with good visuals, considering the time allocated to us.\nManaging to keep our development processes in check, so that we could even consider attempting our stretch goals.\n\n**What we learned**\nWe learned more about how to function effectively as a team, and manage difficult problems and situations. We were able to ensure we developed our code base in the correct way to keep it clean and maintainable. We also learned a number of new skills in the Unity Game Engine, such as navigation meshes, tilemapping, and more advanced particle effects.\n\n**What's next**\nAs mentioned, one of our stretch goals was to implement multiplayer. With the code base we have developed, this should not be too difficult to implement in the future. We also have a few ideas for more spells, such as a spell which allows you to swap positions with an enemy.\n\n",
    "prize": "Winner Grand Prize",
    "techStack": "c#, navmeshplus, unity",
    "github": "https://github.com/Nathcat/HackSussex-GameJam-2025",
    "youtube": "https://www.youtube.com/watch?v=2ZFRnfujm7k",
    "demo": null,
    "team": null,
    "date": "2025-12-07",
    "projectUrl": "https://devpost.com/software/mind-your-mana"
  },
  {
    "title": "SunShiftAI",
    "summary": "SunShiftAI is a project that utilizes artificial intelligence to optimize solar energy usage by analyzing weather data and user preferences. The application aims to enhance energy efficiency for households or businesses by providing tailored recommendations for solar energy management.",
    "description": "I‚Äôm Ayushmaan Bellum, a sophomore from Folsom, CA, and I built SunShiftAI to help communities save energy and reduce carbon emissions by running appliances at the cleanest, lowest-impact times. Inspired by California‚Äôs grid stress and the hidden carbon cost of timing electricity use, I learned how solar irradiance, cloud cover, and TOU pricing interact and designed a SunShift Score:\n\nSunShiftScore[h] = 0.4 √ó solarScore[h] + 0.3 √ó priceScore[h] + 0.3 √ó gridScore[h]\n\nI built the app with Flutter and Dart for iOS, Android, and web, using OpenWeather API for hourly weather, SharedPreferences for local storage, Provider for state management, and Google Gemini API for AI explanations and a chatbot. The biggest challenges were normalizing scores, sliding variable-length appliance windows, and making AI explanations beginner-friendly, but in the end, SunShiftAI transforms complex grid data into simple, actionable recommendations that anyone can follow to save energy and reduce emissions.\n\n\n\n        \n    Built With\n\n    dartfluttergeminiapiopenweatherapiprovidersharedpreferences\n  \n\n        \n    Try it out\n\n    \n        \n  \n  GitHub Repo",
    "prize": "Winner Leaf Courses - Selective online course; Winner Finalist Certificate; Winner CodeCrafters",
    "techStack": "dart, flutter, geminiapi, openweatherapi, provider, sharedpreferences",
    "github": "https://github.com/Ayushmaan-PCG",
    "youtube": "https://www.youtube.com/watch?v=0J-wbK8xSao",
    "demo": null,
    "team": "Ayushmaan Bellum",
    "date": "2025-12-07",
    "projectUrl": "https://devpost.com/software/sunshiftai"
  },
  {
    "title": "Dear Diary",
    "summary": "Dear Diary is an innovative digital journaling application that allows users to document their thoughts and experiences in a seamless and engaging manner. By leveraging modern web technologies, it offers a user-friendly interface and interactive features that enhance the journaling experience.",
    "description": "UI\n    \n\n              \n  \n\n\n\n            \n        \n          \n  \n\n        \n    \n      Onboarding Page\n    \n\n        \n          \n  \n\n        \n    \n      Analytics Page\n    \n\n        \n          \n  \n\n        \n    \n      Mockup\n    \n\n        \n          \n  \n\n        \n    \n      Settings\n    \n\n        \n          \n  \n\n        \n    \n      UI\n    \n\n              \n  \n\n\n\n            \n        \n          \n  \n\n        \n    \n      Onboarding Page\n    \n\n        \n          \n  \n\n        \n    \n      Analytics Page\n    \n\n        \n          \n  \n\n        \n    \n      Mockup\n    \n\n        \n          \n  \n\n        \n    \n      Settings\n    \n\n        \n          \n  \n\n        \n    \n      UI\n    \n123456    \n\n\n\n      \n  DearDiary\n\nJournaling app with real-time sentiment analysis and adaptive color UI.\n\n\n\n\n\n\nWhy I Built This\n\nI've been journaling on and off for years, but I always struggled with consistency. The problem wasn't lack of motivation - it was that most journaling apps felt... lifeless. They were just blank pages that stared back at me. No personality, no feedback, nothing to make the experience feel alive.\n\nOne night after a particularly rough day, I was writing in my journal and noticed how my handwriting changed as my mood shifted. The letters got heavier when I was frustrated, lighter when I felt hopeful. It got me thinking - what if a digital journal could do something similar? What if it could respond to what I was feeling?\n\nI wanted to build something that felt less like a tool and more like a companion. Something that could mirror back the emotions in my writing without being intrusive. So I spent a few nights experimenting with Groq's AI models and sentiment analysis. The first version was honestly terrible - colors flashing everywhere like a broken disco ball. But after a lot of tweaking (debouncing, color mapping, transition curves), it started to feel right.\n\nThe breakthrough came when I realized the app shouldn't just detect emotions - it should embody them. Gold for joy. Crimson for anger. Blue for sadness. Not as data points, but as atmosphere. The UI became this living thing that breathed with my thoughts. Writing angry? The whole screen turns red, and somehow that acknowledgment makes it easier to get the words out.\nI added the analytics dashboard later because I got curious about my own patterns. Apparently I'm way more anxious on Mondays than I thought. Seeing those mood trends laid out didn't just satisfy my curiosity - it actually motivated me to work on my mental health. Turns out having a visual history of your emotional state is pretty powerful.\nThe neobrutalism design came from a desire to make something bold and honest. No gradients hiding behind corporate polish. Just raw colors, thick borders, and interfaces that don't apologize for existing. It matches how journaling should feel - direct, unfiltered, real.\nSome entries are two sentences. Some are essays. But every time, the colors shift and the app listens. If this helps even one other pe",
    "prize": null,
    "techStack": "framer, grok, next.js, recharts, sanity, shadcn",
    "github": "https://github.com/somewherelostt/DiaryEmo",
    "youtube": "https://www.youtube.com/watch?v=7GwJDLrbnFM",
    "demo": "https://deardiary.maazx.dev/",
    "team": null,
    "date": "2025-12-06",
    "projectUrl": "https://devpost.com/software/dear-diary-u7gt6v"
  },
  {
    "title": "OctoSense",
    "summary": "OctoSense is an innovative mobile application designed to enhance user engagement through real-time data insights. By leveraging APIs and a combination of modern programming frameworks, OctoSense provides users with an intuitive interface to access and analyze data relevant to their specific needs.",
    "description": "12345678910    \n\n\n\n      \n  Inspiration\n\nStudents today juggle classes, notes, assignments, exams, and learning resources across dozens of apps. It feels like having eight arms still isn‚Äôt enough. We wanted to build something that simplifies a student‚Äôs entire academic workflow ‚Äî a smart assistant that thinks, organizes, and learns alongside them.\n\nThat idea inspired OctoSense: a multitasking AI study buddy modeled after the adaptability and intelligence of an octopus.\n\nWhat it does\n\nOctoSense is an AI-powered academic assistant that helps students stay organized, learn faster, and manage their workload ‚Äî all in one app.\n\nKey Features\n\nAI Note Summarizer: Upload a PDF, image, or paste text ‚Äî OctoSense generates a clean summary + key points.\n\nAI Study Chat: Ask questions, clarify concepts, or get explanations instantly using Gemini Flash 2.5.\n\nTask & Deadline Manager: Add assignments, exams, and reminders. AI can categorize and prioritize them.\n\nStudy Planner Generator: Enter a topic and timeframe ‚Äî OctoSense creates a personalized day-by-day study schedule.\n\nSaved Notes & Summaries: All summaries and plans are stored locally for quick revision.\n\nIn short: it‚Äôs a full academic command center in your pocket.\n\nHow we built it\n\nFrontend: Flutter framework (clean architecture + Riverpod) for speed, modularity, and native-quality performance.\n\nAI Engine: Google‚Äôs Gemini Flash 2.5 model powers summarization, chat responses, classification, and study plan generation.\n\nStorage: Hive / local DB for persisting notes, tasks, and preferences.\n\nArchitecture: Feature-first folder structure with dedicated providers and controllers for each module.\n\nWorkflow:\n\nBuild logic first (AI, task manager, planner).\n\nAdd placeholder UI for testing.\n\nUse Figma-based screens for the final polish.\n\nIntegrate everything into a smooth end-to-end experience.\n\nChallenges we ran into\n\nHandling PDFs and images for accurate AI summarization without crashing or slowing down the app.\n\nDesigning a multi-module AI workflow (chat + summarizer + planner) that feels simple and intuitive to users.\n\nOptimizing Gemini responses to produce consistent summaries and structured study plans.\n\nBalancing scope vs. time ‚Äî deciding what to include in the MVP without overbuilding.\n\nState management complexity, especially syncing tasks, notes, and AI outputs across screens.\n\nAccomplishments that we‚Äôre proud of\n\nBuilt a fully functional AI study assistant in a short timeframe.\n\nAchieved smooth integration of Gemini Flash 2.5 across multiple features.\n\nCreated a clean, modular Flutter architecture that‚Äôs easy to extend.\n\nDesigned an ecosystem of tools (summaries, planner, tasks, chat) that feel unified and purposeful.\n\nCrafted a product that genuinely helps students learn smarter, not harder.\n\nWhat we learned\n\nHow to structure large AI-powered apps cleanly in Flutter.\n\nHow to prompt Gemini Flash 2.5 for reliable, structured outputs.\n\nThe importance of user flow simplification when multiple complex feature",
    "prize": null,
    "techStack": "android-studio, api, dart, flutter, gemni, supabase",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=OALGfSFFpWk",
    "demo": "https://drive.google.com/file/d/15NrOOLGn6vCuBxPlqEQqeNQhsgPZWb2H/view?usp=drivesdk",
    "team": "Private user",
    "date": "2025-12-06",
    "projectUrl": "https://devpost.com/software/octosense-af0vhw"
  },
  {
    "title": "Trash Talkers",
    "summary": "TECHNICAL HIGHLIGHTS: Trash Talkers utilized HTML for the front-end interface, along with Leaflet for interactive mapping capabilities. The integration of vector layers and PostGIS allowed for robust geographic data handling, enabling real-time updates and detailed mapping of trash locations.",
    "description": "**What it does**\nTrash Talker is a simple-to-use, appealing website designed to minimize trash and broken community equipment, keeping our streets safe and clean. The website is incredibly user-friendly, as volunteers can help out in three ways. Report an issue. If someone spots something that needs attention, they can capture a photo of the issue and select a category for it, followed by an optional description (e.g., ‚ÄúBroken glass near the playground‚Äù). They can use either their live location or select a location on our map to show us exactly where the issue is.\nJoin our crew of dedicated volunteers and make a direct impact on our community. They can use the Google Form linked on the website to sign up for community clean-up initiatives and choose the tasks, timings, and roles that work best for them. Th\n\n**Inspiration**\nWe started our brainstorming ideas by reading the rules and reviewing the hackathon guide. We needed to choose a specific focus area for the project (that is solvable, of course), and find common ground since we both know different language models. We scrolled through a catalog of problems to find the 80,000 Hours website, but our personal fits varied at first. While one partner was excited about combating engineered pandemics using biotechnology, another was intrigued to study the moral status of digital minds. We wanted to create a project that aligned with our personal vision, so we kept going through the brainstorming process until we found something we both enjoyed‚Äîcleaning our communities. This problem is solvable, neglected, and is something that closely aligns with our social and m\n\n**How we built it**\nWe used GitHub and followed a standard HTML5 structure. We used standard meta tags to make the site mobile-friendly as well (using viewport) and used Google Fonts (Poppins) by importing the font family. Font Awesome was used for icons like the camera. CSS Styles\nRoot variables in our CSS styles defined the basic color palette and organized it in one common place, so we can easily change it later. The Navbar is fixed to the top with a glass-like blur effect. There are four feature cards at the top (Report, Volunteer, Leaderboard, Donate) that animate/hover up when you move your mouse over them. The Navigation Bar\nTrash Talker is displayed in a stacked design, and each link has a unique hover color corresponding to its section in Nav Links (e.g., Pink for Report, Purple for Leaderboard). The\n\n**Challenges**\nOur primary challenge while completing this project was time. We were challenged to brainstorm a problem, create a prototype, test our website, and present it as well‚Äîall in such a short time frame of a few days. We also used an API to track the location of the user, and this took a lot of time to design. We had to examine a lot of websites and their designs to draw inspiration for the design of our own. Finally, uploading the logo to our website was also a challenge. We designed the thumbnail for our project submission using Canva, and we were also planning on using Canva to make a logo to attach to our website. While we tried downloading both JPG and PNG files to upload to GitHub, neither worked, even though we had no errors in our code. This was a disappointing challenge for us that we\n\n**Accomplishments**\nThe interface allows users to report community civic issues in real-time, anywhere they are! It works in every area of the world (provided there is stable internet connectivity). We are proud of the responsiveness of our interface. It can be accessed on all devices, both desktop and mobile. It is a user-friendly public communication system that allows for real change to happen, relying on the power of numbers. It is also interactive. Buttons and cards lift up when hovered over, which creates real and engaging user interaction. Finally, the same colors and fonts defined in the CSS variables are consistently used throughout the page, giving the page a clean, aesthetic look.\n\n**What we learned**\nWe learned many useful things from this project. Talking to each other has improved our communication skills, as we have consistently needed to text and schedule times for calls this weekend. We also learned valuable copyright policies and figured out how to license our project under the MIT License. Finally, we have learned to find a balance between our technical skills and our presentational skills, as we need to excel in both areas for a successful submission.\n\n**What's next**\nTrash Talkers needs a consistent supply of donors and volunteers to keep it running. While it is likely that there will be many issue submissions, we expect to have a lack of donors and volunteers. Currently, we have a leaderboard for top donors and top volunteers on a weekly, monthly, and all-time basis. We hope this system will incentivize more Trash Talkers to join our community. We have edited our ‚ÄúDonate‚Äù tab to automatically update the currency of money based on the location of the user to encourage global participation. However, our website‚Äôs data on leaderboards, as well as the donate tab, are currently just sample statistics used for demo purposes. We need a clearer, more efficient system to truly expand our interface globally across all communities.\n\n",
    "prize": "Winner Finalist Certificate",
    "techStack": "html, leaflet-vector-layers-postgis",
    "github": "https://github.com/Archit992008/TrashTalkers?tab=readme-ov-file",
    "youtube": "https://www.youtube.com/watch?v=LqLju7wULh4",
    "demo": null,
    "team": null,
    "date": "2025-12-07",
    "projectUrl": "https://devpost.com/software/trash-talkers"
  },
  {
    "title": "The Attack of the Spoopy Little Guys",
    "summary": "\"The Attack of the Spoopy Little Guys\" is a game that likely combines elements of traditional role-playing and magic systems within a whimsical, spooky theme. Players probably engage with charming yet mischievous characters, navigating challenges that involve strategy and creativity.",
    "description": "**What it does**\nThe player controls a wizard in third person, and has to fight different types of ghosts using spells that they unlock along the way. Damage dealt from each attack is dependant on the roll of a dice, similar to D&D. After each wave of ghosties, the player can choose to roll a buff dice, a d12 that randomly picks a buff or debuff for the player or all of enemies.\n\n**Inspiration**\nOur main inspiration was Google's 2016 Halloween game, which had a cat wizard fighting different ghosts, using the mouse to draw simple shapes. We then found some ghost assets on the Unity Store and decided to expand upon the game in 3D with a roguelike structure. As well as this, we were heavily inspired by Dungeons and Dragons (AKA D&D), and decided to use some of the systems in that game to make our project more complex. We were also inspired by survival.io with the ongoing battle element.\n\n**How we built it**\nWe built our project in Unity, using assets and animations from the Unity Store. Only the dice were created by us - we do not claim any other art, assets, or animations as our own. We also used GitHub for version control and to share our project with each other whenever necessary.\n\n**Challenges**\nA couple of group members had other plans on the Sunday so they had to leave early. This meant that we were on an extreme time-crunch! As well as this, our lack of an artist on the team meant that we had to find, download and figure out how to use free assets available on the Unity Store. Another challenge was getting the character and the terrain to interact correctly. We had created the player on a plane and stupidly created the terrain in a different scene, so when merging we had to recreate the player. However, we didn't realise the the terrain was way too big for the character, causing movement problems when scaling the player up, and looking slightly ridiculous when at the right size. We fixed this by trying to resize the terrain, but figuring this out and then fixing it took about 3\n\n**Accomplishments**\nWe completed this game in 3D, despite that being significantly harder and more work. Also, we managed to create our own dice asset in Blender for the combat system. We are also really proud of how pretty the environment turned out to be: we feel like we really encapsulated the enchanted and slightly spooky vibe that we were going for. The lighting really makes this project shine! No pun intended...\n\n**What we learned**\nWe learned how to better avoid merge collisions when pushing to GitHub, and generally improved our skills in Unity - especially debugging. In addition, we learned how to stay calm under pressure when working as a team with a strict deadline, successfully avoiding any fallings out. We also learned how to do a lot of things individually, most notably lighting, switching scenes, and Blender.\n\n**What's next**\nDuring the summer, we may add some additional features like more spells, more complicated fighting systems, and cutscenes. Currently, we don't have a dedicated artist on this project, so creating cutscenes for a backstory is quite difficult. If we had more time, it would be great to create all the assets and models ourselves in Blender.\n\n",
    "prize": "Winner [HackSussex] Roll 20 for a win: Best traditional/rpg magic",
    "techStack": "blender, c#, unity",
    "github": "https://github.com/ChweeBee/GameJam25",
    "youtube": "https://www.youtube.com/watch?v=3w80mFvIFCU",
    "demo": null,
    "team": null,
    "date": "2025-12-06",
    "projectUrl": "https://devpost.com/software/ghosts-in-the-woods"
  },
  {
    "title": "AI Code Mentor",
    "summary": "TECHNICAL HIGHLIGHTS: The project leverages Flutter for a seamless cross-platform experience, utilizing Dart for development and incorporating essential libraries like Riverpod for state management. The integration of AI technologies such as Gemini and OpenRouter enhances the application‚Äôs ability to provide intelligent code suggestions and mentorship.",
    "description": "**What it does**\nAI Code Mentor provides instant, intelligent code analysis across four domains: üêõ Bug Fixer\nDetects syntax errors, logic flaws, runtime exceptions\nSide-by-side diff view with beginner/intermediate/expert explanations\nüîí Security Scanner\nIdentifies SQL injection, XSS, hardcoded secrets\nOWASP-compliant fixes with risk scores\n‚ö° Performance Analyzer\nDetects O(n¬≤) algorithms, memory leaks, bottlenecks\nVisualizes complexity with charts\nüîß Code Refactor\nModernizes legacy code with clean architecture\nSuggests design patterns, removes code smells\nApp Screenshots\n\n**Inspiration**\nI've wasted countless hours debugging simple errors that AI could catch instantly. I noticed 70% of security vulnerabilities are preventable, yet most developers lack accessible tools. I wanted to create an intelligent coding companion that fixes bugs and teaches why they occurred‚Äîthe mentor I wish I had when learning to code.\n\n**Challenges**\nProblem:\n\n**Accomplishments**\n‚ú® Multi-Provider Orchestration - Seamless failover between 3 AI providers\n‚ú® Educational Impact - 3-tier explanations adapt to skill level\n‚ú® Zero-Crash Architecture - Graceful error handling, no raw exceptions shown\n‚ú® Speed - Average analysis time: 2.1s (10x faster than manual review)\n‚ú® Cross-Platform - One codebase runs on Android, iOS, Web, Desktop\n\n**What we learned**\n1. AI Engineering ‚â† Prompt Engineering\nProduction AI apps need structured outputs, fallback parsing, and response validation‚Äînot just good prompts. 2. JSON Parsing Edge Cases\nDifferent AI models return wildly different formats. Built a universal sanitizer handling unescaped newlines, control characters, and invalid escapes. 3. Security First\nGitHub secret scanning taught me to use .env files properly: 4. UX > Technical Complexity\nRewrote error messages from:\n\n**What's next**\nQ1 2026: VS Code Extension üîå\n\n",
    "prize": null,
    "techStack": "android, cross-platform, dart, flutter, flutter-dotenv, gemini, groq, ios, macos, material-design-3, openrouter, riverpod, sharedpreferences, website, windows",
    "github": "https://github.com/assassinaj602/ai_code_mentor",
    "youtube": null,
    "demo": "https://ai-code-mentor-my7etjczx-asadullahaj602-9700s-projects.vercel.app/",
    "team": null,
    "date": "2025-12-06",
    "projectUrl": "https://devpost.com/software/ai-code-mentor-ghpdf7"
  },
  {
    "title": "HandCraft XR: Winter Edition",
    "summary": "TECHNICAL HIGHLIGHTS: Notable technical implementations include the use of Havok for physics simulations, enabling realistic interactions with crafted objects. The integration of WebXR allows for a seamless virtual reality experience across different devices, while the utilization of Three.js and TypeScript enhances the rendering of 3D graphics and improves code maintainability.",
    "description": "**What it does**\nHandCraft XR is a calm Mixed Reality sandbox where you use natural hand gestures to build a festive miniature Christmas Market on a virtual table. You can: Pick blocks from a floating tablet\nPinch to spawn them in your hand\nScale, rotate, and position them intuitively\nSnap pieces onto the table or attach decorations to other objects\nGrab and throw away unwanted pieces\nWatch snow fall and clouds drift overhead\nBuild a peaceful winter village that automatically saves and loads It‚Äôs simple, tactile, and relaxing - a tiny holiday world you create with your hands.\n\n**Inspiration**\nI love programming, VR, and building things. Naturally, I wondered: why not create a little thematic playground - something like virtual LEGO? A winter theme felt perfect: quiet snow, warm lights, and a tiny Christmas Market that you can bring to life piece by piece.\n\n**How we built it**\nThe experience is built specifically for Meta Quest devices using Immersive Web SDK, Three.js, WebXR, and a fully hand-tracked interaction system. It runs in the Quest Browser without installation. All interactions use the WebXR Hand Tracking API, and the trailer footage was recorded directly from a Meta Quest 3 in a standalone mode. Core systems include: A custom pinch-based spawning mechanic\nTwo-hand scaling/rotation with visual outlines\nSnapping logic for table placement and decoration attachment\nA virtual tablet UI with paginated block icons\nA lightweight lighting setup optimized for standalone performance\nInstanced meshes, dynamic batching, BVH acceleration, and adaptive shadow update system for smooth WebXR performance\nAutomatic save/restore via local storage All assets and interacti\n\n**Challenges**\nPerformance in WebXR: Standard materials and shadows were too heavy, so lighting had to be redesigned around lighter, more stable options.\nMissing features in Immersive Web SDK: To achieve the interaction patterns and stability I needed, I studied, forked, and extended the SDK: https://github.com/evstinik/immersive-web-sdk\nLimited time for content: The block library is curated but small, so prioritizing which items give the most ‚Äúholiday feel‚Äù was important.\n\n**Accomplishments**\nA fully hand-tracked building system that feels natural and intuitive\nA single-pass outline effect implemented via the stencil buffer\nEfficient shadows using an adaptive, on-demand shadow system with a dedicated micro shadow camera\nSmooth performance even with many objects, shadows, mixed reality, physics and UI\nA cozy, relaxing atmosphere\nGetting it all done within limited time, while keeping the core experience polished\nMade it from scratch in just a month during the competition\n\n**What we learned**\nHow to structure a WebXR project using ECS architecture\nHow stencil buffers work\nHow to identify CPU or GPU bottlenecks, and distinguish vertex-bound from fragment-bound performance issues\nHow to build a dynamic batching system with BatchedMesh and improve interactions using instance-based BVH structures\nHow to optimize shadows to be able to run on standalone Quest 3 device\n\n**What's next**\nFuture improvements I'm excited about: WebRTC-based multiplayer building with friends\nSharing worlds or exporting creations\nNight mode with warm lights and glowing decorations\nPeople and animated elements to bring the Christmas Market to life\nMore interaction types (stacking, attaching objects, advanced snaps)\nExpanded sound design with more satisfying feedback and ambience\nAdditional themed kits (Horror Mansion, City, etc.) This version lays the foundation, and I'm excited to grow it into a richer, more expressive Mixed Reality building experience.\n\n",
    "prize": "Winner Judge‚Äôs Choice",
    "techStack": "havok, iwsdk, three.js, typescript, webxr",
    "github": "https://github.com/evstinik/immersive-web-sdk",
    "youtube": "https://www.youtube.com/watch?v=4KbKIA9H5_Y",
    "demo": "https://handcraft-xr.netlify.app/",
    "team": "Nikita Evstigneev",
    "date": "2025-12-07",
    "projectUrl": "https://devpost.com/software/handcraft-xr-winter-edition"
  },
  {
    "title": "Disco Flip",
    "summary": "Disco Flip is an innovative project designed to enhance user engagement through an immersive experience in a virtual environment. By integrating various technologies, it aims to create a seamless and enjoyable onboarding process for new users, facilitating their transition into the platform.",
    "description": "**What it does**\nThe main puzzle concept of Disco Flip is to flip on and off the three music tracks, Drums, Bass, and Guitar/Keys. When all three tracks are on at the same time,  strings and synths also get added. \nWith all three tracks enabled, players must reach the Disco Ball goal to solve the puzzle, avoiding obstacles along the way Disco Flip features 50+ levels, an in-game Fashion store, Leaderboards and Quests, and an Explore mode where players can venture through the disco club and take in the vibes.\n\n**Inspiration**\nThe inspiration for Disco Flip comes from a Mixed Reality prototype we once made in Unity, which would spawn turnstiles in the players living room and dynamically create a workable puzzle from their open floor space. It worked well, but we knew many players wouldn't have enough floor space to get a proper variety of puzzles, and the market for Mixed Reality exclusive games might be limiting, but from that experiment Disco Flip was born. Disco Flip takes the best parts of the idea and adds fun funky disco music to the mix, solidifying it as a great and addicting mobile title, where players can dig in to the tight puzzle game-loop, running time trials and competing for high scores.\n\n**How we built it**\nHorizon Worlds, Typescript, Noesis Studio, Houdini Indie (for texturing and game art).\nHorizon's GenAI Mesh Generation, and Sound Generation. Central to the game are the Tap-To-Move API, and Portrait API - the game couldnt exist without either of these two, they're terrific additions to the Horizon Worlds codebase. \nWe also used Meta's NPCs to serve as the games dance troupe.\n\n**Challenges**\nLearning Noesis and its UI paradigms. Creating puzzles was a challenge, where puzzles need to be very dependable in their design and execution. A puzzle that doesnt work, or is too easy/exploitable makes or breaks the fun.\n\n**Accomplishments**\nI'm proud of making a puzzle game with a lot of content, as its not a genre I've done before. I think i really succeeded in capturing the essence of 1970's Disco in the games look and feel which wasnt easy; theres a fine line between tacky and stylish.\n\n**What we learned**\nI learnt all about UI programming from a Noesis standpoint, also using Portrait and Tap-To-Move features.\n\n**What's next**\nThe Puzzle/Music genre combo proved to be really engaging and immersive. We would love to add to the musical palette and explore other genres such as K-Pop either as DLC for Disco Flip or as K-FLIP or similar. We'd love to further enhance the game's social aspect further, join your friends and join in the dance troupe, or sabotage the puzzle player with additional obstacles. Or just hang out in the club together. We'd love to support VR properly, it works already experimentally but it wasnt a focus for this contest. With some minimal changes I think we could do it.\n\n",
    "prize": "Winner Best New User Onboarding Experience",
    "techStack": "horizonworlds, houdini, noesis, portrait, taptomove, typescript",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=Cdc1boa_tjc",
    "demo": "https://horizon.meta.com/world/10172786784620181",
    "team": "Alexandre Dubois",
    "date": "2025-12-08",
    "projectUrl": "https://devpost.com/software/disco-flip"
  },
  {
    "title": "TakeOver 1600-SF22",
    "summary": "TakeOver 1600-SF22 is an innovative mixed and virtual reality application that utilizes hand tracking technology to create immersive experiences. The project likely enables users to interact seamlessly within a virtual environment, leveraging the capabilities of advanced hand tracking to enhance user engagement and interaction.",
    "description": "**What it does**\nOrder‚Äôs Up transforms any room into a shared mixed-reality kitchen. When the game starts, ingredients appear around the room like tomatoes, bread, cheese, lettuce, and meats. Players use hand-tracking shortcuts to pick up utensils (a knife and a pan), prepare ingredients by chopping and frying them, and assemble fully custom burgers and sandwiches. Customers appear and place orders that print on virtual receipts. Players work together to find, prepare, and assemble ingredients as per the orders. When dishes are ready, the customers rate them. Your job is to: Read the order ticket\nAssemble the dish based on requirements\nMatch the dish to the correct customer\nSatisfy the picky customers Customer ratings depend on: Ingredient correctness\nPreparation level\nAssembly accuracy The fun isn‚Äôt just\n\n**Inspiration**\nWe knew from the start that we wanted to build something co-located: an experience that feels better when people are physically together, not just online. For us, mixed reality isn‚Äôt just placing objects in a room. It‚Äôs about creating shared presence, laughter, and chaos, where the people in the room matter as much as the digital content. Early brainstorms ranged from a bomb defusal puzzle to a *stacking challenge, but the pattern was clear: we wanted something intuitive, physical, and instantly fun. We wanted a game anyone could understand in seconds, where the learning curve is tiny and the chaos curve is huge. A key inspiration came from a teammate who wanted something he could play with his kids. We needed something simple enough to learn instantly, playful enough to laugh over, and im\n\n**How we built it**\nWe built Order‚Äôs Up using the Meta XR SDK and their Building Blocks as the foundation for mixed reality features: Colocation via shared spatial anchors so every player sees the same virtual kitchen mapped into their physical environment\nHand-tracking and pose detection for grabbing, slicing, flipping, and throwing without controllers\nMeta Spatial Audio for positional kitchen sounds: sizzling pans, chopping, and customer reactions\nPhoton Fusion Networking to synchronize ingredients, utensils, and interactions across headsets in real-time Each ingredient has around 3 states: raw, prepared, burnt. (Choppable ingredients do not get burnt!) Each state changes how the ingredient looks, scores, and reacts. Additional systems include: Physics-based ingredient throwing\nCollision-aware sandwich asse\n\n**Challenges**\nThe first major challenge was co-location. Early prototypes showed drift and anchor inconsistencies, which caused ingredients to misalign between players. We had to learn: Which objects required strict network synchronization\nWhich could live locally\nHow to minimize network traffic while staying responsive\nHow to pass a simple cube back and forth We also underestimated the difficulty of gesture reliability. Detecting the difference between a grab, slice, and throw requires tuned thresholds and noise smoothing. Hand-tracked design is only fun when interactions feel forgiving and responsive. Lastly, building an end-to-end experience under hackathon time meant stripping down mechanics to what was instantly readable: > ‚ÄúIf you can grab it, you can cook it.‚Äù TLDR: Simple design is the hardest d\n\n**Accomplishments**\nA fully working co-located MR game built in a weekend supporting up to 20 users\nA complete game loop: prep ‚Üí assemble ‚Üí ticket match ‚Üí rating\nHand-only interactions: no controllers, no menus\nReal-time networking with Photon Fusion\nStable shared spatial anchor for a full session The moment someone catches a flying tomato and yells ‚ÄúCut this!‚Äù at their teammate is the exact magic we were aiming for.\n\n**What we learned**\nWe learned that co-location is the experience. When multiple players share a real space with virtual objects, the game stops feeling like VR and becomes a shared physical game that just happens to use digital ingredients. Building Order‚Äôs Up taught us practical lessons about mixed reality as a product and as a technical discipline. The experience required us to think about spatial computing as a system that blends human behavior, networking constraints, and real world environments. The strongest response came from shared moments. Two people arguing about how to cook virtual bacon created more engagement than any shader or physics improvement. Mixed reality is most compelling when it creates a feeling of being together in a world that exists between physical space and digital logic.\n\n**What's next**\nWe‚Äôre excited to continue development with features like: More utensils (spatula, grill, toaster, blender)\nProcedural customer personalities and orders\nIngredient physics ‚Äúchaos mode‚Äù\nMultiplayer scoring and leaderboards\nCustom dishes and community recipes\nCosmetic upgrades for chefs\nInteractions between chefs (like high fives!) Long term, we see Order‚Äôs Up as a platform for social mixed reality. The core loop, grab and prepare and deliver, is simple enough for anyone to learn in seconds, but the underlying systems have the depth to keep people laughing, arguing, and trying new strategies. Our goal is to grow the experience into something families and friends can pull out in any room for a shared moment, and to show that co-located MR can create real engagement in a casual setting where th\n\n",
    "prize": "Winner Mixed & Virtual Reality Category by Meta 1st Place; Winner Best Implementation of Hand Tracking by Meta 1st Place",
    "techStack": "c#, colocation, metasdk, photon, unity",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=AKSFZrCOvos",
    "demo": "https://www.meta.com/s/6mEmA6Q65",
    "team": "Nishka Awasthi, Cyril Medabalimi, Michael Deering",
    "date": "2025-12-09",
    "projectUrl": "https://devpost.com/software/dinerdash"
  },
  {
    "title": "700-SF15 SWANS",
    "summary": "The 700-SF15 SWANS project integrates AI technologies with camera access to enhance user interaction and experience, likely focusing on real-time image processing or data analysis. The project aims to create an innovative application that leverages computer vision and natural language processing to provide meaningful insights or functionalities.",
    "description": "**What it does**\nSWANS is an AI/AR copilot that follows a service worker across every gig, restaurant, lounge, bar, caf√©, or venue. It: Converts live table conversations into structured orders using AI Displays this data on a subtle AR HUD for the worker Tracks regulars and ‚ÄúSWANS‚Äù (VIPs) to elevate guest experience Helps workers avoid mistakes and reduce cognitive load Positions floating indicators above guests to guide re-engagement (WIP) Eliminates the need for pen+pad, POS terminals, or memorization Empowers workers to deliver premium service anywhere Helps workers increase tips, speed, and consistency The result:\nFaster service, fewer mistakes, higher tips, and premium hospitality delivered by anyone, anywhere.\n\n**Inspiration**\nService work is the largest labor category on Earth ‚Äî over 100 million people worldwide. Most juggle multiple jobs, memorize dozens of menus, adapt to constantly changing environments, and are expected to deliver premium, personalized service every single shift.\nTurnover is sky-high, job stability is low, and yet guest expectations keep rising. We realized something: no one is building tools for the workers themselves.\nPOS systems serve owners. AI scheduling tools serve management.\nBut nothing exists to make workers faster, smarter, more confident, and more financially secure. So we built SWANS ‚Äî the Service Work Assistant & Navigation System.\nNot to replace workers.\nBut to make workers superhuman.\n\n**How we built it**\nBuilt a Unity-based AR HUD optimized for subtle, non-intrusive overlays Used live audio input ‚Üí AI ‚Üí structured JSON schema for orders (items, modifiers, quantity, category, VIP tag, special instructions) Displayed the parsed order in real time on the HUD Added the first version of S.W.A.N. Indicators ‚Äî small floating markers meant to attach to unique faces Created a prototype system for tracking regulars and classifying ‚ÄúSWANS‚Äù (our VIP tier) Attempted automatic table detection to place ‚Äúengagement markers‚Äù in AR space Experimented with face anchoring to position markers above individuals Everything was optimized around one principle:\naugment the human, don‚Äôt interrupt them.\n\n**Challenges**\nAutomatically detecting tables in complex lighting and seating layouts Getting floating engagement markers to anchor stably in physical space Face detection for positioning SWAN indicators above heads without drift Creating a clean, minimal UI that enhances service without distracting from it Ensuring structured order output stayed consistent across varied conversation styles Avoiding latency when updating the AR HUD with new order data Designing a worker-focused tool that respects privacy and portability\n\n**Accomplishments**\nBuilt end-to-end ‚Äúvoice ‚Üí structured order ‚Üí HUD display‚Äù working inside AR Designed a consistent, extensible schema for order capture Implemented the first version of VIP/SWAN tracking Established a visual language for floating face indicators Made AR interfaces that preserve human connection by staying subtle and elegant Proved the viability of a worker-first service copilot Demonstrated a shift from enterprise tools ‚Üí personal augmentation tools\n\n**What we learned**\nWorkers don‚Äôt need automation ‚Äî they need augmentation Personal AI adoption is faster than enterprise AI adoption The best AR UI is nearly invisible and preserves eye contact Structured outputs + AR overlays = reliable, real-time utility Building a universal ‚Äúservice OS‚Äù requires supporting multi-job workflows The future of frontline labor belongs to tools that travel with the worker, not the employer\n\n**What's next**\nExpand the worker profile system to follow workers across venues Improve marker stability with better face anchoring + depth understanding Build out a name-recognition & regular-guest memory system Add micro-gestures for confirmations/corrections Layer in tip-optimization & service-speed analytics Push toward a worker subscription model (Superhuman for service workers) Pilot with multi-job workers across restaurants, lounges, bars, and hotels Move to lightweight AR form factors and eventually to everyday wearables SWANS is just the beginning.\nWe‚Äôre building the world‚Äôs first AI exoskeleton for service workers ‚Äî\na universal copilot that democratizes premium service and helps workers earn more, stress less, and perform at a superhuman level.\n\n",
    "prize": "Winner AI with Camera Access by Meta Runner-Up",
    "techStack": "c#, claudecode, cursor, depthapi, flask, google-drive, https, insightface, json, ngrok, openai, python, responseapi, unity, whisper, wit.ai",
    "github": "https://github.com/Wingspear/Swans",
    "youtube": "https://www.youtube.com/watch?v=8wpTuTV3qrM",
    "demo": null,
    "team": "Jaedon Lee",
    "date": "2025-12-07",
    "projectUrl": "https://devpost.com/software/700-sf15-jae-z"
  },
  {
    "title": "1508-SF20 ZOOMR",
    "summary": "1508-SF20 ZOOMR is an innovative project designed to leverage hand tracking technology in virtual reality environments. It aims to enhance user interaction by allowing natural hand gestures to control and manipulate virtual objects, thus creating a more immersive experience.",
    "description": "**What it does**\nNormally, your real world is stuck on one layer. With ZOOMR, you peel that limitation away. A left-hand gesture ‚Äúzooms‚Äù through multiple invisible dimensions around you, revealing ghosts tucked inside different layers. Your right hand does the cleanup ‚Äî form the gesture, shoot, and the spirits vanish.\n\n**Inspiration**\nPhones ‚Äúzoom‚Äù by cheating ‚Äî cropping, scaling, stretching pixels. Now that Meta‚Äôs PCA API finally lets us touch raw camera textures, we wanted to see how far we could push that idea inside mixed reality. We explored everything from micro-civilizations living on leaves to biology-scale deep dives into cells. Eventually, we pivoted into something more playful: a zoom mechanic that reveals supernatural layers hiding in your actual room. Think Luigi‚Äôs Mansion, but your hands are the controller and your environment is the level.\n\n**How we built it**\nWe built everything in Unity using the Meta XR SDK and passthrough features, stitching together hand-tracking, custom interactions, and our own zoom-through-reality pipeline. Raw camera textures come straight from the PCA API; our ghost lenses and detector objects were modeled in Maya. The whole system runs on lightweight cropping/scaling logic so we keep full control over digital zoom without relying on AI tricks.\n\n**Challenges**\nOur original plan involved a fully custom hand-gun gesture for raycasting, but designing, training, and tuning that in hackathon time was ambitious. We kept the silhouette but simplified input to a thumb-and-index pinch ‚Äî fast to implement, still readable, and reliable under hand-tracking jitter.\n\n**Accomplishments**\nWe‚Äôre proud to be using the PCA API the way it was meant to be used ‚Äî directly manipulating raw camera textures to create a believable zoom mechanic. No enhancement models, just pure control. We also got MR animations, audio cues, and object spawning working cleanly, including automatic ghost respawning to keep gameplay active.\n\n**What we learned**\nWe dove deep into the PCA API, MRUK, and how far passthrough can be pushed when you stop thinking of it as a background and start treating it as a layer stack you can manipulate.\n\n**What's next**\nOn the tech side, the zoom mechanic could expand into accessibility tools ‚Äî magnification, selective enhancement, or guided vision overlays. For the game, eye-tracking could move the lens itself, freeing the player‚Äôs hands entirely. And we want to evolve the ghosts ‚Äî color-coded by depth layer, more expressive behavior, and richer feedback to make each resolution feel like its own world.\n\n",
    "prize": "Winner Best Implementation of Hand Tracking by Meta Runner-Up",
    "techStack": "c#, meta, mr, mruk, passthrough, quest, shader, unity",
    "github": "https://github.com/agrikatheprogrammer/ZOOMR",
    "youtube": "https://www.youtube.com/watch?v=YBz7fGK2gQ4",
    "demo": null,
    "team": "Hemang Mehra, Agrika Gupta",
    "date": "2025-12-05",
    "projectUrl": "https://devpost.com/software/placeholder-7rw1qv"
  },
  {
    "title": "Staxel",
    "summary": "TECHNICAL HIGHLIGHTS: The project utilized iwsdk, OpenAI, Python, and WebXR, indicating a focus on web-based immersive experiences, AI integration for enhanced interactivity, and a robust programming foundation. The combination of these technologies likely allowed for rich, engaging user interfaces and experiences.",
    "description": "**Challenges**\nWithout prior XR development experience, the main challenge was selecting the right development stack. After consulting with hackathon mentors and other participants, I decided to go the WebXR way and use the IWSDK framework.\n\n**What we learned**\nI learned how to develop a WebXR application for Meta Quest using Immersive Web SDK, became a pro in debugging XR applications, and tried a new vibe-coding IDE (Antigravity).\n\n",
    "prize": "Winner Best Upgrade to an Existing Project by Meta Runner-Up",
    "techStack": "iwsdk, openai, python, webxr",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=sPFpocr_aMk",
    "demo": "https://sensaihack.staxel.ai/",
    "team": null,
    "date": "2025-12-05",
    "projectUrl": "https://devpost.com/software/staxel"
  },
  {
    "title": "PathfinderXR",
    "summary": "PathfinderXR is an innovative mixed and virtual reality application designed to enhance user experiences through immersive environments. By integrating various technologies, it aims to provide users with interactive and engaging pathways, enabling them to explore virtual spaces in novel ways.",
    "description": "**What it does**\nPathfinderXR is a mixed-reality guidance tool for social workers supporting re-entry. It helps practitioners and clients stabilize first, then orient to what matters most next. Using breath-led regulation, perspective-shifting awe, and reflective triage, PathfinderXR helps social workers: See client orientation in real time. Reduce overwhelm before decisions are made. Guide clients toward calmer, clearer next steps. Rather than automating care, PathfinderXR augments the human guide‚Äîmaking orientation visible so judgment and empathy can do their work.\n\n**Inspiration**\nPathfinderXR was born from years of frontline work where Mike and Elgin kept seeing the same painful pattern: people exiting homelessness, addiction, or incarceration weren‚Äôt failing because they lacked motivation‚Äîthey were failing because they lacked orientation. No clear map.\nNo way to see the next right step while overwhelmed and stressed. Both of us know what it feels like to rebuild from chaos without guidance. PathfinderXR is the tool we wish we‚Äôd had‚Äîone designed to turn overwhelm into clarity and help people move back toward stability, dignity, and purpose.\n\n**How we built it**\nFor this hackathon, we focused on designing the core interaction model and system logic: Prompt-engineered reflective questioning models. Breathwork and awe-based regulation flows. Spatial triage logic mapped to real-world re-entry needs. Mixed-reality interaction concepts optimized for social workers in active casework. This prototype demonstrates how MR can support guidance without replacing the practitioner.\n\n**Challenges**\nTranslating complex frontline realities into a simple, testable experience. Designing AI support that augments human judgment rather than overriding it. Working under tight time constraints while validating a new care paradigm\n\n**Accomplishments**\nGrounding the project in real frontline experience. Clearly articulating a new use case for AI with camera access in social services. Designing a human-first MR guidance model that prioritizes regulation before action\n\n**What we learned**\nRe-entry breaks down when decisions are made from dysregulation. Mixed reality is uniquely suited to support orientation and presence. Asking better questions often matters more than providing answers\n\n**What's next**\nExpanding breathwork and physiological regulation tools. Adding resource layers for housing, healthcare, detox, food, legal, and spiritual support. Partnering with outreach teams, shelters, and re-entry programs. Developing a ‚ÄúPath to Purpose‚Äù mode for life beyond crisis. Preparing for pilot deployments in Oakland, San Francisco, and similar cities\n\n",
    "prize": "Winner Mixed & Virtual Reality Category by Meta Runner-Up",
    "techStack": "fastapi, moondream, render.com, supabase, tbd, unity",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=hsMKDkCcyEw",
    "demo": null,
    "team": "Elgin Rose, AarnavSan Sangekar",
    "date": "2025-12-07",
    "projectUrl": "https://devpost.com/software/pathfinderxr"
  },
  {
    "title": "1201-SF17 ThatsMyJam",
    "summary": "\"ThatsMyJam\" is an innovative project that leverages AI technology and camera access to create a unique music discovery experience. By utilizing advanced algorithms, it identifies songs based on user interactions and context, allowing users to seamlessly connect with and explore music that resonates with their personal tastes.",
    "description": "**What it does**\nPick up any object ‚Äî a banana, a cup, a plastic bottle ‚Äî and it instantly becomes a musical instrument.\nOur system:\n    1.  Identifies the object using an AI model\n    2.  Generates a sound personality that matches its tone, feel, and vibe\n    3.  Lets you play it using gesture-based controls and a full musical scale\nShake a banana to get slap bass. Tap a cup for warm lofi Rhodes. Pluck a clothespin for pizzicato strings. Every object becomes a new musical experience.\n\n**Inspiration**\nWe love music, and we love AR. We wanted to combine both using contextual intelligence from a headset ‚Äî letting AI understand the world around you and turn everyday objects into expressive musical tools. Out of that came a simple question:\nWhat if anything you pick up could become an instrument?\nThat idea became That‚Äôs My Jam: a playful, immersive way to explore your environment through sound.\n\n**How we built it**\nWe built the front end as an APC app on Quest using Unity. We built a node backend that provided an endpoint to run an image through an LLM for identification and detailed description. Then we connected the two parts, enabling the user on the Quest to identify an object, which sends the image to the endpoint. Then the app receives details on the object and its music. When the user moves their hand holding the object, the app plays different sounds that fit that object.\n\n**Challenges**\nAt midnight, we finally connected the front end and backend‚Ä¶ right before the computer we were using stopped sending POST requests for no explainable reason. After two hours of debugging, switching machines magically fixed everything.\nWe also struggled with reliable hand tracking while holding arbitrary objects, and balancing gesture detection with usability.\nAnd writing these descriptions with a sleep-deprived brain is always a challenge.\n\n**Accomplishments**\nIt‚Äôs genuinely fun. We found ourselves wandering IKEA for an hour turning everything into music long after we got our 45-second demo footage.\nWe built something that feels magical, playful, and genuinely shippable with a bit more polish. We‚Äôre proud of how complete the experience feels despite the time pressure..\n\n**What we learned**\n‚Ä¢ Pun-based names can actually turn into real product names\n‚Ä¢ When we lock in, we can build fast\n‚Ä¢ Apparently this building has spontaneous parties at all hours\n‚Ä¢ And yes ‚Äî AI can make anything more fun\n\n**What's next**\nWe want to polish tracking, improve reliability, add a music sequencing mode, and open this up for creators.\nIn short: let‚Äôs ship it.\n\n",
    "prize": "Winner AI with Camera Access by Meta 1st Place",
    "techStack": "digitalocean, express.js, gpt5.1chat, metapca, metasdk, node.js, openrouter, unity",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=daBf_21oq30",
    "demo": null,
    "team": "Sam Woodard, Zaina Qasim, Daniel Geisler",
    "date": "2025-12-07",
    "projectUrl": "https://devpost.com/software/that-s-my-jam"
  },
  {
    "title": "700-SF13 Edgelord Samurai",
    "summary": "The \"700-SF13 Edgelord Samurai\" project combines elements of augmented reality and gaming to create an immersive experience that allows users to engage in samurai-themed challenges. Utilizing advanced computer vision and game development frameworks, the project enables players to interact with the environment and other players in a unique, engaging way.",
    "description": "**What it does**\nEdgelord Samurai turns any room into a shared mixed reality dojo on Meta Quest. The player wears the headset and holds a katana style controller. A friend throws a giant orange foam ball through the play space. Our system uses OpenCV color tracking on the passthrough video to lock onto the ball and follow its movement in real time. In the headset, the ball becomes the anchor point for virtual fruit, targets, and impact effects that align with the real world motion. When the player swings the katana through that space, they see satisfying slices, particles, and scoring feedback that feel tied directly to the physical throw. Multiple Meta Quest headsets can join the same session so other players and spectators see the same dojo, the same ball path, and the same hits from different viewpoints\n\n**Inspiration**\nEdgelord Samurai began with the original Katana Samurai demo created by Richard Borys, a simple single player bamboo slicing experience in virtual reality with an immersive storyline. It follows a young son, grieving the loss of his father who was killed in the war, as he is recruited for Samurai training by the kingdom to honor his father‚Äôs legacy and prepare for the growing conflict. In the game experience, it felt great to land a clean cut, but it was a lonely with no shared energy or spectators. For the SensAI hackathon, our team asked how we could take that feeling of studying the blade and turn it into a mixed reality dojo party in a living room. We looked to the discipline of Japanese sword training where posture, breath, and precise cuts matter, and combined it with the instant fun\n\n**How we built it**\nWe built Edgelord Samurai in Unity on Meta Quest using passthrough mixed reality and OpenCV. The headset shows the real room, and we layer a stylized dojo environment over it. We use a bright orange foam ball as a physical training orb and feed the passthrough frames into OpenCV. By defining minimum and maximum values for the orange color, OpenCV isolates the ball from the rest of the scene and calculates its position in each frame. Unity receives that position and attaches virtual targets, trails, and hit effects to it so the physical ball and the digital effects feel like one object. The katana controller is treated as a training sword. The game checks when its swing path intersects the tracked ball and triggers slices, sound, and when the timing and placement are correct. On top of this\n\n**Challenges**\nWe discovered that tracking a real object in mixed reality is very sensitive to the environment. Different lighting, wall colors, and clothing made it harder for OpenCV to cleanly isolate the orange ball. We spent time tuning the color ranges, smoothing detection, and reducing jitter, so the ball would stay stable on screen. We also had to balance responsiveness and reliability. If tracking reacted too fast, it created noisy movement. If it was too strict, the ball would occasionally pop in and out of view. Finally, we had practical challenges around safety and hardware, such as designing a katana prop that felt good to swing while keeping enough distance between the thrower, the player, and the surrounding furniture.\n\n**Accomplishments**\nWe are proud that this is a true upgrade to an existing project. The original Katana Samurai was a single player bamboo slicer. Edgelord Samurai turns that foundation into a social mixed reality dojo with physical props, shared presence, and multiple modes. It moves from a solo toy toward a party game where people take turns and cheer for each other. We are also proud of how much we achieved with relatively simple technology. Careful OpenCV color tracking and good game feel made a foam ball feel like a magical training orb that bridges the real and digital worlds. Seeing multiple people in Meta Quest headsets react to the same throw, shout when someone lands a clean cut, and immediately want another turn is the best proof that the concept works.\n\n**What we learned**\nWe learned that mixed reality sits at the intersection of computer vision, game design, and physical space. Simple tools like color tracking can create powerful illusions when combined with thoughtful feedback and real objects. At the same time, MR forced us to pay close attention to safety, comfort, and clarity because players are moving their bodies in real rooms, not just using thumb sticks. We also learned that social presence multiplies engagement. The moment we added spectators, and turn taking, the project felt more alive. Finally, we learned how important it is to design within real constraints like noisy environments and limited hackathon time, and to prioritize features that work reliably in front of judges and players.\n\n**What's next**\nNext, we plan to fast follow into the Meta Horizon Start Developer Competition next week with an improved build of Edgelord Samurai. Our focus will be on strengthening tracking across more environments, polishing the visual presentation of the dojo, tightening the onboarding so new players can start quickly, and refining the core loop so every throw and every cut feels intentional, readable, and fun in a mixed reality setting.\n\n",
    "prize": "Winner Best Upgrade to an Existing Project by Meta 1st Place",
    "techStack": "opencv, photon, unity",
    "github": "https://github.com/InnerBushido/EdgelordSamurai",
    "youtube": "https://www.youtube.com/watch?v=iuwDH2rIl3c",
    "demo": null,
    "team": "Harris Warren, Olasile Abolade, Caitlyn Croft, Noor Elbanna",
    "date": "2025-12-07",
    "projectUrl": "https://devpost.com/software/edgelord-samurai"
  },
  {
    "title": "1609-SF21-Hygge",
    "summary": "The project \"1609-SF21-Hygge\" is an immersive entertainment experience designed to create a cozy and comforting environment for users, leveraging advanced technologies to enhance user interaction and engagement. It combines elements of augmented reality and virtual experiences to foster a sense of well-being and relaxation, embodying the Danish concept of 'hygge'.",
    "description": "Demo\n    \n\n              \n  \n\n\n\n            \n        \n          \n  \n\n        \n    \n      Hygge\n    \n\n        \n          \n  \n\n        \n    \n      Report of room\n    \n\n        \n          \n  \n\n        GIF\n        \n    \n      Demo gif\n    \n\n        \n          \n  \n\n        \n    \n      Demo\n    \n\n              \n  \n\n\n\n            \n        \n          \n  \n\n        \n    \n      Hygge\n    \n\n        \n          \n  \n\n        \n    \n      Report of room\n    \n\n        \n          \n  \n\n        GIF\n        \n    \n      Demo gif\n    \n\n        \n          \n  \n\n        \n    \n      Demo\n    \n12345    \n\n\n\n      \n   Hygge \n\n Ambiance on demand\n\nWe're bringing hygge into everyone's home, both to create an immersive ambiance and to give people the option to visualize and buy the fixtures pulled from real store catalogs.\n\nSuccesses: Having fun the entire time, hugging it out at the end and meeting everybody at the hack! So much fun! We got lighting to work in the real space, a valiant attempt at multi-user, and 3 of the designers on the team worked in Unity and C# and pushed branches back to main in GitHub, llm requests were coo, we had some really really sweet branding/design, video-making was awesome. Made a 3D model pipeline with a ton of heavy lifting using ai for mesh and texture gen, procedurally (and sometimes hand-) optimizing them for Quest.\n\nLearnings:  We generated AI video commercials that were inspiringly cohesive, the tech is becoming really useful. We learned a lot about Meta Quest UI as well. Sid learned a valuable lesson in decision-making, there was a point where I knew it would be risky to do drop-in multiplayer and more complicated design logic with fixture recommendations. I should've picked a lane but crashed in the middle of both at like 3am Sunday. But like who is Icarus without the sun, Sisyphus without his rock... I'll make better decisions on the next one. This has me fired up to do another hack soon! \n\nNote: I'm removing the apk from github cause I did not securely load any keys\n\nWhat's next for HuggingHeart you ask? You'll see! \n\n Tech notes: \n\nBuilt with Unity and Quest 3\n        - Multiplayer: Normcore. Doesn't work in the current build in the sense that things are not wired up, it's the thought that counts\n    - Handtracking/depth occlusion\n    - Tech Stack \n        ‚óã AI \n            ¬ß Meta scene model ‚Äî> LLM request includes Image of what you‚Äôre looking at\n            ¬ß Wit Speech to text\n            ¬ß OpenAI LLM\n        ‚óã Plugins \n            ¬ß Normcore networking\n            ¬ß Amplify shader\n            ¬ß Scriptable Sheets\n            ¬ß Meta SDKs\n            ¬ß ShapesXR\n            ¬ß OpenAI dotnet api plugin\n\n\n\n        \n    Built With\n\n    aftereffectsc#figmahyper3dnormcoreopenaisoraunityveowit.ai\n  \n\n        \n    Try it out\n\n    \n        \n  \n  drive.google.com",
    "prize": "Winner Best Immersive Entertainment Experience by Meta 1st Place",
    "techStack": "aftereffects, c#, figma, hyper3d, normcore, openai, sora, unity, veo, wit.ai",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=4HEdtWKyQk8",
    "demo": "https://drive.google.com/drive/folders/1ppoAQF6n1jznqq-6EBBD_52RsiTrtQLb",
    "team": "Dolores Joya, Remi Gao, Morgan Blair",
    "date": "2025-12-07",
    "projectUrl": "https://devpost.com/software/16thfloor-views"
  },
  {
    "title": "GeoPortal",
    "summary": "GeoPortal is an innovative application designed to enhance user engagement through location-based services, integrating aviation data and immersive experiences. Leveraging unique features of Meta Quest, the app provides users with real-time aviation insights and interactive visualizations, making it a valuable tool for aviation enthusiasts and professionals alike.",
    "description": "**What it does**\nThe app transforms in-flight travel into an interactive exploration. At its core, it is a real-time flight monitor that visualizes the journey on a dynamic map. Using a database of over 14,000 cities, it intelligently detects landmarks to trigger \"City Explorations\"‚Äîimmersive 360¬∞ VR video tours of the locations below. To keep passengers engaged, the app features a cyberpunk-inspired interface with educational mini-games like \"Flag Challenge\" and \"Airport Rush.\" These can be played against an AI or nearby passengers via offline Bluetooth multiplayer, creating a complete in-flight entertainment ecosystem.\n\n**Inspiration**\nI conceived this idea on a flight to Rome. Frustrated by the lack of entertainment systems and the inability to know our location, I saw an opportunity to solve the boredom of air travel. I wanted to create a solution that turns the \"stuck in a seat\" experience into a journey of discovery.\n\n**How we built it**\nI built the app using native Android technologies (Kotlin and Jetpack Compose). I engineered a custom data pipeline to cross-reference AviationStack API telemetry with local geospatial data. Visually, I developed a custom graphics engine within Jetpack Compose‚Äôs Canvas API. Instead of heavy 3D libraries, I used custom projection algorithms to render a holographic globe with scientifically accurate day/night cycles. For social features, I implemented a custom Bluetooth networking layer for direct socket connections, while ExoPlayer handles the 360¬∞ video playback.\n\n**Challenges**\nMy biggest problem was time. I only saw the competition 4 days before the deadline and wanted to do something even if it is not completed. Although I was still able to add every feature I wanted with a huge success. Also optimizing the 3D rendering engine was quite the process. Implementing custom projection algorithms initially caused stuttering and battery drain; I had to rigorously profile the app and refactor drawing logic to ensure smooth performance. Additionally, managing offline Bluetooth multiplayer was difficult due to the variability of Android‚Äôs Bluetooth APIs. Establishing a stable handshake required building a custom logging system and a robust socket-management solution to handle interruptions and reconnect users seamlessly.\n\n**Accomplishments**\nI am proud of being able to build such useful application in just 4 day. Furthermore, establishing a stable peer-to-peer Bluetooth handshake in an offline environment was a major win. This robust local communication layer not only enables gaming but lays the groundwork for future synchronized in-flight media.\n\n**What we learned**\nI learned that with the right mathematical optimization, Jetpack Compose‚Äôs Canvas API is capable of high-performance complex visualizations without needing OpenGL. I also gained deep experience in offline-first architecture, realizing that building resilience against network failures‚Äîwhether for API calls or Bluetooth packets‚Äîis essential for a polished user experience.\n\n**What's next**\nI plan to evolve the app into a comprehensive social platform. With our Bluetooth infrastructure in place, our next step is introducing \"Shared Cinema\" and \"Music Lounge\" features, allowing passengers to sync media offline. I also aim to expand the multiplayer ecosystem, using the holographic globe as a shared game board for deeper cooperative experiences.\n\n",
    "prize": "Winner Best Android App Leveraging Features Unique to Meta Quest",
    "techStack": "aviationstackapi, bluetooth, canvasapi, exoplayer, jetpackcompose, kotlin",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=diRcGpDhMl8",
    "demo": null,
    "team": "Merve Yuvaci",
    "date": "2025-12-09",
    "projectUrl": "https://devpost.com/software/geoportal"
  },
  {
    "title": "EcoSage",
    "summary": "TECHNICAL HIGHLIGHTS: The project leverages a robust tech stack, including Express.js for server-side development, React for a dynamic user interface, and TailwindCSS for responsive design. The use of react-leaflet enhances the mapping experience, while the PWA capabilities ensure accessibility across devices, providing an offline mode for users.",
    "description": "**What it does**\nEcoSage is your AI-powered sustainability co-pilot that turns complex environmental data into simple, actionable insights. Here's how we revolutionize eco-conscious living:\n\n**Inspiration**\n\"The greatest threat to our planet is the belief that someone else will save it.\" - Robert Swan We watched as well-intentioned consumers struggled to make sustainable choices amidst greenwashing, confusing labels, and overwhelming information. The disconnect between environmental awareness and actionable change became our driving force. The Stark Reality: ‚ôªÔ∏è 91% of plastic isn't recycled despite good intentions\nüì± 68% of consumers want to shop sustainably but don't know how\n‚è±Ô∏è Average person spends just 5 seconds evaluating product sustainability We asked: What if every shopping decision could be an informed climate action? What if technology could bridge the gap between intention and impact? EcoSage was born from this vision - to democratize sustainability intelligence and transform every\n\n",
    "prize": "Winner Grand Prize",
    "techStack": "express.js, gemini-api, node.js, overpass-openstreetmap, pwa, react, react-leaflet, tailwindcss, typescript, vite",
    "github": "https://github.com/FrostByte-49/EcoSage",
    "youtube": "https://www.youtube.com/watch?v=tc8ahEA8SDo",
    "demo": "https://ecosage.netlify.app/",
    "team": "Private user",
    "date": "2025-12-05",
    "projectUrl": "https://devpost.com/software/ecosage-pfdcrg"
  },
  {
    "title": "D-Day Simulator",
    "summary": "TECHNICAL HIGHLIGHTS: Built with Horizon, the simulator likely incorporates advanced graphics and simulation algorithms to create realistic environments and scenarios. Notable implementations may include dynamic weather systems, AI-driven enemy tactics, and a user-friendly interface that enhances player immersion.",
    "description": "**What it does**\nThe player arrives at the beaches of Normandy via landing craft and must clear 1000 meters of sand guarded by machine guns, mines, barbed wire, an airforce and much more. Players work together to identify high risk areas and avoid them, while moving strategically to avoid the threats that sweep across the beach in systematic patterns.\n\n**Inspiration**\nWe were inspired by the success of Titanic Simulator on HorizonWorlds, and wanted to create a culturally significant, action-packed survival simulator that rewarded multiplayer co-operation and used unique mobile-native design for a peak experience. Our goal was to take the immersion associated with VR and bring it to mobile through smart camera usage and Noesis overlays.\n\n**How we built it**\nWe designed the game as a multiplayer experience where every player progresses independently using their own skill and strategy, yet cooperation is always optionally available. Players can see where others are on the beach, how far they‚Äôve advanced, and where/how they died. This lets them learn from others mistakes, coordinate movements if they choose to, or simply compete for the furthest run. So the experience can feel fully solo or highly cooperative depending on the player‚Äôs preference. Instead of relying on VFX gizmo, we supplemented and enhanced all effects (rain, explosions, smoke, blood splatter, bullet impacts, etc.) with image-sequence overlays in Noesis Studio, delivering far richer visual effects. We used gen AI NPCs for an onboarding experience that was memorable and unique ea\n\n**Challenges**\nOne major issue came from our animated overlay system: many intense effects (explosions, rain, etc.) were built as short 4‚Äì5 second sequences of high-resolution images. The combined texture size was too large for Horizon Worlds‚Äô strict memory limits, causing long loading times or failed loads entirely. We solved it by aggressively compressing the images, reducing color depth where the eye wouldn‚Äôt notice, cutting unnecessary frames, and implementing a system that only keeps the currently needed sequences in memory. Also creating Noesis GUI overlays that perfectly cover the entire screen on both mobile and web was tricky. Some devices left visible borders, breaking immersion for effects like heavy rain or flashing explosions. We fixed this by adding a full-screen viewbox and carefully adjus\n\n**Accomplishments**\nWe're very proud of using Noesis almost like a VFX tool in addition to as a UI tool. We really wanted to push the limits of what was possible with it. We're also proud of how effectively our onboarding sequence sets the mood, through the animations, cutscene-like camera movements, and NPC usage.\n\n**What we learned**\nNoesis was the big one! We learned how to use it from scratch and how to use it as a VFX tool in addition to a UI tool.\n\n**What's next**\nWe want to focus on developing an uprade system that allows the player to level up in rank and use their playtime to purchase improvements, such as bolt cutters that allow them to cut through barbed wire, gas masks that help them evade toxic threats, etc.\n\n",
    "prize": "Winner Best World that Leveraged Noesis",
    "techStack": "horizon",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=w_rFqcZSI_k",
    "demo": "https://horizon.meta.com/world/843018298903112",
    "team": null,
    "date": "2025-12-08",
    "projectUrl": "https://devpost.com/software/d-day-simulator"
  },
  {
    "title": "Awesome Hand",
    "summary": "Awesome Hand is an innovative project developed using Unity that focuses on enhancing user interactions through advanced hand gesture recognition. The project likely aims to create a more immersive experience by allowing users to control and manipulate virtual environments with their hands intuitively.",
    "description": "A stage where you move boards up and down to build a path that carries the ball to the goal.\n    \n\n              \n  \n\n\n\n            \n        \n          \n  \n\n        \n    \n      Guide the ball to the goal.\n    \n\n        \n          \n  \n\n        \n    \n      Sometimes you‚Äôll need both hands.\n    \n\n        \n          \n  \n\n        \n    \n      A stage where you pass the ball back and forth between both hands.\n    \n\n        \n          \n  \n\n        \n    \n      A stage where a giant hand guides lots of tiny balls into the goal.\n    \n\n        \n          \n  \n\n        \n    \n      A stage where you use your hands as a bridge to pass the ball along.\n    \n\n        \n          \n  \n\n        \n    \n      A stage where you move boards up and down to build a path that carries the ball to the goal.\n    \n\n              \n  \n\n\n\n            \n        \n          \n  \n\n        \n    \n      Guide the ball to the goal.\n    \n\n        \n          \n  \n\n        \n    \n      Sometimes you‚Äôll need both hands.\n    \n\n        \n          \n  \n\n        \n    \n      A stage where you pass the ball back and forth between both hands.\n    \n\n        \n          \n  \n\n        \n    \n      A stage where a giant hand guides lots of tiny balls into the goal.\n    \n\n        \n          \n  \n\n        \n    \n      A stage where you use your hands as a bridge to pass the ball along.\n    \n\n        \n          \n  \n\n        \n    \n      A stage where you move boards up and down to build a path that carries the ball to the goal.\n    \n1234567    \n\n\n\n      \n  This is our first time developing for a VR device.\nTo deepen our understanding of Meta Quest, we played many different games on the device and ran technical experiments in Unity. Through that process, we were amazed by how accurate the hand tracking was.\n\n\"Let's make a game that uses your own hands!\"\n\nWhen we started this project, we set the theme:\n\"Fully leverage the strengths of the device to create a unique experience unlike anything before.\"\n\nHand tracking was the perfect technology to match that theme.\n\nWhat's especially noteworthy is that the tracked hands can be placed in the Unity scene as meshes and colliders.\nThat's when we realized we could create a physical, physics-driven game around them.\n\nOur motto in game development is:\n\"Make easy-to-understand games that people all over the world can enjoy.\"\nBeing able to use the most intuitive controller humans have‚Äîtheir hands‚Äîfits perfectly with that philosophy.\n\nThat's how we arrived at blending this unique hand-tracking input with a simple puzzle game.\n\nBuilding Blocks offers excellent support for developing on Meta Quest, but it probably wasn't designed with use cases like spawning three or more hands in a scene, placing them wherever we like, scaling them up or down, moving them around, or rotating them freely.\n(If anyone from the Building Blocks team is reading this, please don't be offended‚Äîwe're the odd ones here. Thanks to Building Blocks, we were able to get started very smoothly.)\nSo, while making the ga",
    "prize": "Winner Best Implementation of Hand Interactions",
    "techStack": "unity",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=LB8Yeluiems",
    "demo": null,
    "team": "Ryota Hayashi",
    "date": "2025-12-09",
    "projectUrl": "https://devpost.com/software/awesome-hand"
  },
  {
    "title": "YapFormer",
    "summary": "YapFormer is an innovative project that leverages advanced machine learning techniques to enhance natural language processing capabilities. Although specific details are not provided, the project likely focuses on improving text understanding or generation by utilizing models from the Hugging Face library alongside PyTorch for deep learning implementations.",
    "description": "123    \n\n\n\n      \n  YapFormer : A Transformer Implementation with Modern Optimizations ‚úß‡¥¶‡µç‡¥¶‡¥ø(Àµ ‚Ä¢ÃÄ ·¥ó - Àµ ) ‚úß\n\nYapFormer is a transformer model built entirely from scratch, featuring modern architectural components and efficient training optimizations.\nThe final model contains ~56 million parameters and was trained for 15,000 steps (~4.5 hours) on the TinyStories dataset.\n\nDespite the small size and short training time, YapFormer produces surprisingly high-quality short stories, demonstrating that well-designed architectures can go a long way even with limited compute.\n\n\n\nWhat is YapFormer? ‡´Æ ‚óï Ôªå ‚óï·Éê\n\nYapFormer is a from-scratch GPT-style autoregressive transformer that integrates many techniques used in contemporary LLMs:\n\n\nRotary Embeddings (RoPE)\nGrouped Query Attention (GQA)\nKV caching for fast inference\nRMSNorm\nSwiGLU feed-forward layers\nMixed precision training\nGradient accumulation\nCosine decay learning rate\nGradient clipping\n\n\nThis project serves as both a learning exercise and a practical lightweight generative model.\n\n\n\nWorking ‡´Æ‚Çç ‚Ä¢ ·¥• ‚Ä¢ ‚Çé·Éê\n\n1. Input & Embeddings\n\n\nTokens are mapped using a custom tokenizer.\nRoPE is applied to attention queries/keys instead of absolute positional embeddings.\n\n\n2. Attention (with GQA + KV Cache)\n\n\nGrouped Query Attention (GQA):\nMultiple query heads share a smaller number of key/value heads ‚Üí faster and more memory-efficient.\nKV Caching:\nDuring inference, previous keys/values are stored so the model only attends to new tokens.\n\n\n3. Transformer Blocks\n\nEach block contains:\n\n\nRMSNorm\nMulti-Head Attention (with RoPE, GQA, KV cache)\nSwiGLU feed-forward network\nResidual connections\n\n\n4. Output Projection\n\n\nFinal RMSNorm\nLinear layer ‚Üí logits ‚Üí softmax for next-token prediction\n\n\n5. Training Loop\n\nModern GPU-friendly techniques:\n\n\nAMP mixed precision for speed + memory efficiency\nGradient accumulation to simulate large batch sizes\nCosine LR decay for smooth convergence\nGradient clipping to prevent instability\n\n\n\n\nArchitecture ‡´Æ - Ôªå ‚Ä¢ ·Éê‚Å©\n\nModel Structure\n\nToken  Embedding ‚Üì RoPE  Rotary  Position  Encoding ‚Üì N √ó [  RMSNorm ‚îú‚îÄ‚îÄ Multi-Head  Attention  (GQA  +  KV  Cache) ‚Üì Residual ‚Üì RMSNorm ‚îú‚îÄ‚îÄ SwiGLU  Feed-Forward ‚Üì Residual  ] ‚Üì Final  RMSNorm ‚Üì Linear  LM  Head \n\n\n\nTechnology Stack ‡´Æ‡∏Ö„ÉªÔªå„Éª·Éê‡∏Ö\n\n\nLanguage: Python\nFramework: PyTorch\nBuilt With:\n\n\n  Custom attention mechanisms\n  Custom embeddings\n  Custom RMSNorm + SwiGLU layers\n  Mixed precision training tools\n\nEcosystem Tools:\n\n\n  ü§ó Hugging Face (datasets/tokenization)\n   PyTorch (core autograd & tensor ops)\n\n\n\n\n\nHow to Run ‡´Æ‚éöÔªå‚éö·Éê‚Å©\n\n1. Clone the Repository\n\ngit clone https://github.com/Aravind-808/YapFormer\ncd YapFormer \n\n2. Install Dependencies\n\npip install -r requirements.txt \n\n3. Generate Text\n\npython inference.py\n\n4. Enter your prompt\n\nPrompt: Once upon a time \n\n5. Example Output\n\nOnce upon a time there was a tiny mouse who loved reading stories...\n\n\n\n        \n    Built With\n\n    huggingfacepytorch\n  \n\n        \n    Try it out\n\n    \n        \n  \n  GitHub Repo",
    "prize": "Winner 1st Place ‚Äî Build From Scratch Champion",
    "techStack": "huggingface, pytorch",
    "github": "https://github.com/Aravind-808/yapformer",
    "youtube": "https://www.youtube.com/watch?v=sndu7zd9WTg",
    "demo": null,
    "team": "Shri Sai Aravind",
    "date": "2025-12-25",
    "projectUrl": "https://devpost.com/software/yapformer"
  },
  {
    "title": "OnTheGoVR",
    "summary": "OnTheGoVR is an innovative virtual reality application designed for travel enthusiasts, enabling users to explore various destinations in a multiplayer environment. It allows users to experience travel-related activities virtually, providing an immersive way to plan trips and discover new locations while being offline.",
    "description": "123    \n\n\n\n      \n  üéÆ Inspiration & Context\n\nLong trips, cramped seats, and the classic struggle of holding a phone between two people make watching movies while traveling a hassle. Earbuds get shared, arms get tired, and if friends are seated far apart, staying in sync becomes impossible. \n\nOnTheGoVR transforms this messy experience into a private shared cinema you can step into anywhere, entirely offline.\n\nüöÄ Features\n\n\nOffline Shared Cinema: You can start a session, share a simple code, and enjoy perfectly synced playback with friends on the same local network, without the need of internet.\n2D and Immersive Modes: You can pick between a traditional flat-screen view or a fully spatial VR theatre.\nSmart Local Discovery: The app automatically searches for movie files on your device, helping you find what you need quickly.\nEasy File Upload: At home, you can use OnTheGoVR directly to assist you with uploading your movies to the headset;\nPersonalized Environments: Each user can explore their preferred VR space, adjust lighting, and relax independently while still watching together.\nSynchronized Controls: Whenever one user pauses or plays the movie, changes reflect to the other clients in sync.\n\n\n‚öôÔ∏è Meta Spatial SDK Integration\n\nThe immersive experience is built using the Meta Spatial SDK, combining spatial rendering, scene management, and Kotlin-based interactions to create a comfortable virtual cinema.\nThe offline streaming system works by placing all participants on the same Wi-Fi network or hotspot.\n\nThe host exposes a direct local stream, and clients connect to it to receive synchronized playback without relying on external connectivity.\n\nüî® Timeframe & Tools\n\n\nKotlin & Android Studio;\nMeta Spatial SDK;\nLocal network streaming architecture;\n\n\nCreated specifically for the Meta Horizon Start Developer Competition. All the scenes are uniquely built for the competitions.\n\nüéØ Accomplishments\n\n\nDelivered a fully functional offline shared cinema experience from end to end.\nAchieved seamless playback synchronization without any internet access;\nBuilt multiple immersive environments with individualized lighting controls.\nImplemented local file discovery and an intuitive drag-and-drop upload system.\nDemonstrated how spatial computing can elevate everyday entertainment while traveling!\n\n\n\n\n        \n    Built With\n\n    kotlinmetamultiplayerofflinespatialsdk\n  \n\n        \n    Try it out\n\n    \n        \n  \n  GitHub Repo",
    "prize": "Winner Best Android App for Travel Mode",
    "techStack": "kotlin, meta, multiplayer, offline, spatialsdk",
    "github": "https://github.com/InventiveOtters/quest-travel-app",
    "youtube": "https://www.youtube.com/watch?v=snMBNi9eN0o",
    "demo": null,
    "team": "Flavius Holerga, Gabi Tiplea",
    "date": "2025-12-09",
    "projectUrl": "https://devpost.com/software/syncedvr"
  },
  {
    "title": "Gunship: Quackdown",
    "summary": "Gunship: Quackdown is an innovative project that likely centers around a unique gameplay or interactive experience, possibly involving a blend of flight simulation and whimsical elements involving ducks. By integrating creative design with engaging mechanics, the project aims to captivate users and provide a distinctive gaming experience.",
    "description": "**What it does**\nPlayers work through 6 missions (with more on the way) from the gunner seat of a gunship, eliminating enemies, vehicles, and buildings. When targets are hard to see, you can switch between White Hot and Black Hot thermal modes to spot them. You start with the 25mm cannon and can unlock the 40mm and 105mm cannons to tear through objectives.\n\n**Inspiration**\nGunship: Quackdown was inspired by my favorite Call of Duty campaign missions that put players in the gunner seat of an AC-130.\n\n**How we built it**\nGunship: Quackdown was built using GenAI, Copilot, Blender, and a frankly irresponsible amount of caffeine.\n\n**Challenges**\nThe game relies on a lot of interconnected systems to function smoothly. Designing, building, and debugging those systems took serious effort. Scope creep was also a major factor, and several enemy variants and upgrade paths had to be cut to hit the deadline.\n\n**Accomplishments**\nI‚Äôm really proud of the per-player audio settings that let players fine-tune sound effects, music volume, and haptics to their liking, making the experience more immersive. The UI was a huge undertaking, but it feels responsive and provides constant feedback through sound and haptics. The menu uses custom camera angles for each section, visually reinforcing the currently selected option. The thermal camera system feels great to use and is unlike anything else I‚Äôve seen on Horizon Worlds. Overall, I‚Äôm extremely proud of the project as a whole‚Äîespecially since it‚Äôs the first time I‚Äôve built something of this scope as a solo dev.\n\n**What we learned**\nDialing in UI is hard, balancing upgrades is its own challenge, and pushing into territory that hasn‚Äôt been done on the platform before adds another layer of complexity.\n\n**What's next**\nNext up: an arcade mode for longer sandbox sessions, more enemy variety, weapon mods, special weapons, and additional maps. With some major performance tweaks, the game will be able to support much more content to keep players coming back.\n\n",
    "prize": "Winner Top Prize",
    "techStack": "blender, noesis, typescript",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=g8uivZ4bInM",
    "demo": "https://horizon.meta.com/world/695607793117939",
    "team": null,
    "date": "2025-12-08",
    "projectUrl": "https://devpost.com/software/gunship-quackdown"
  },
  {
    "title": "Ship It!",
    "summary": "\"Ship It!\" is an innovative project developed in a hackathon setting that leverages immersive technologies to enhance user experience in virtual environments. Built using Horizon Worlds, Noesis Studio, and TypeScript, it aims to facilitate seamless interactions and transactions in a digital marketplace, allowing users to explore, shop, and engage in a shared virtual space.",
    "description": "**What it does**\nPlayers receive orders, pack items into boxes, stamp them, and ship them out as fast as possible! It is frantic and chaotic in the best way. The game relies heavily on Noesis UI to deliver a polished mobile  experience in Horizon Worlds which includes menus, results screens, progression, and lots of visual feedback to keep things exciting!\n\n**Inspiration**\nShip It! was spontaneously born with just a little over a week left in the MHCP competition! The idea came from pondering the ongoing chaos of real-world package loading during the busy holiday season and thinking, \"Hey, there's an idea!\" I wanted to create something energetic, silly, and satisfying to play!\n\n**How we built it**\nThe entire project was developed to highlight the awesome power of the recently supported Noesis Studio. Nearly everything is UI-driven which made the experience incredibly fun and challenging to design. Gameplay systems were written in TypeScript and shaped through rapid iteration. Many features changed several times as the game almost showed me what it was meant to be.\n\n**Challenges**\nLearning Noesis while actively building a full mobile-only Horizon world for the first time was definitely a challenge! Rapid iteration meant refactoring often and discovering what works well and what maybe doesn't work so well. I learned a lot about UI structuring, 2D Animation, performance, and even had a few funny ‚Äúnever do this again‚Äù moments with Noesis! Additionally, considering the challenges of designing for portrait orientation proved difficult, but it was a worthwhile challenge.\n\n**Accomplishments**\nLate night Discord build sessions with MHCP creators were invaluable! We bounced ideas, solved problems, redesigned systems, and kept each other motivated. The game evolved through community collaboration and that feels really special! Beyond that experience, just being able to get something to this level out in this timeframe is huge for me! This is my first world released in over a year!\n\n**What we learned**\nI learned how powerful Noesis UI can be and also where the sharp corners are. I improved my TypeScript workflow and discovered new ways to bring mobile-style gameplay into Horizon. Most importantly, I experienced how fast a project can grow through community support and shared creativity!\n\n**What's next**\nMore levels, item expansions, a smoother progression path, and ongoing iteration based on player feedback! The plan is to keep building with the community and make Ship It! better and more fun with every update!\n\n",
    "prize": "Winner Top Prize",
    "techStack": "horizonworlds, noesisstudio, typescript",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=aKAIWndG680",
    "demo": "https://horizon.meta.com/world/4254659188152052/?target=release&hwsh=e6Cwi2B0ot",
    "team": "David James",
    "date": "2025-12-09",
    "projectUrl": "https://devpost.com/software/ship-it-0n3erp"
  },
  {
    "title": "IMMERROCK",
    "summary": "TECHNICAL HIGHLIGHTS: The project utilizes advanced features of Unreal Engine, showcasing high-fidelity graphics and real-time interaction. Key implementations likely include sophisticated rendering techniques, physics simulations, and user interface design that enhance the immersive quality of the experience.",
    "description": "**What it does**\nThis update completely transforms how learning works inside IMMERROCK. Courses now react to your playing through real-time Note Detection, exercises evolve as you improve, and new modes give you more ways to learn at your own pace.\nWe also added new celebrations, clearer goals, and more rewarding feedback, so every practice session feels like progress you can see and feel.\n\n**Inspiration**\nFrom the beginning, our goal with IMMERROCK has been simple: help people learn guitar and bass in a way that feels easy, fun, and encouraging. After launching, we listened closely to our community. They wanted progression that felt clearer, feedback that felt more personal, and a practice flow that truly responded to how they played. That feedback became the heart of this update.\n\n**How we built it**\nWe rebuilt big parts of the learning system from the ground up. Some of the biggest changes include: Adding Note Detection to supported course exercises\nCreating 3 exercise types: Wait Mode, Song Tempo, and Song Mastery\nRedesigning the star system so rewards actually reflect your playing\nBuilding new celebration moments: Cheering, Particles, Level-ups, and Result Sequences\nAdding Auto Speed Up and Auto Restart to help players grow more naturally\nImproving UI/UX so goals and progress are easier to understand This update is the biggest learning-system improvement we‚Äôve ever shipped.\n\n**Challenges**\nImproving a live app always comes with challenges. Integrating Note Detection into older course logic meant touching systems that had been stable for a long time. Balancing difficulty, refining reward pacing, and adding new automation without overwhelming players took a lot of iteration.\nBut every challenge pushed the experience closer to what we and our players wanted.\n\n**Accomplishments**\nThis update makes IMMERROCK feel more fun and supportive than ever: Exercises that evolve with your skills in real time\nStars and goals feel meaningful\nCelebrations make progress satisfying (because learning an instrument should feel exciting)\nAutomation tools help you stay in flow\nThe whole experience is smoother, clearer, and more polished For many players, this feels like a brand-new version of IMMERROCK.\n\n**What we learned**\nWe learned that people practice more, and feel better when the app listens, responds, and cheers them on. Clear goals, adaptive difficulty, and positive feedback make a huge difference, especially for beginners.\n\n**What's next**\nThis update is only the start. Next, we‚Äôre pushing Mixed Reality interactions even further. Guided by constant community feedback, our goal is simple: every update should make players feel more confident, more capable, and more excited to pick up their instrument.\n\n",
    "prize": "Winner Best Lifestyle Experience Honorable Mention",
    "techStack": "cpp, unreal-engine",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=98HlGIi8KMs",
    "demo": "https://www.meta.com/en-gb/experiences/immerrock/7334845636643834/",
    "team": "Yanira Moguel, Evanna Roman",
    "date": "2025-12-07",
    "projectUrl": "https://devpost.com/software/immerrock"
  },
  {
    "title": "Lumos",
    "summary": "Lumos is an innovative project that utilizes advanced technologies to enhance online safety and user experience. By integrating a variety of APIs and machine learning models, it likely focuses on detecting and mitigating online threats, such as phishing or malware, ensuring that users can navigate the web securely.",
    "description": "**What it does**\nLumos is an AI Co-pilot for Digital Trust. It is a centralized platform that provides instant, AI-powered analysis for suspicious messages and images, powered by machine learning models such as XGBoost.\nMulti-Modal Analysis: Users can paste text or upload a screenshot of a suspicious message.\nInstant Verdict: The system analyzes the input and delivers a clear Risk Score (0-100) in seconds.\nActionable Intelligence: Unlike simple blockers, Lumos explains why a message is risky through \"Analysis Evidence\" (e.g., unknown sender, urgency keywords, flagged URLs) and provides concrete \"Recommendations\" (e.g., Block number, Do not click links).\n\n**Inspiration**\nWe live in a state of constant digital alert. Every unexpected text or email forces a difficult choice: Is this a critical notification or a sophisticated trap? The fear of being scammed is matched only by the fear of missing something important, like a delivery update or an appointment reminder.\nWe realized that without an integrated, instant verification tool, users are left guessing. This uncertainty leads to two costly outcomes: missed opportunities (ignoring real messages) or financial/data loss (falling for malicious links). We wanted to eliminate this guesswork by building a tool that provides clarity in a world of deception.\n\n**How we built it**\nWe engineered a Multi-Layered Intelligence Engine that deconstructs threats into parts for analysis.\nThe Stack: We built a scalable backend using Node.js and Express.js, serving a responsive Web UI.\nData Processing (OCR): To handle image-based scams that evade text filters, we integrated Tesseract.js. This allows Lumos to extract and analyze text, URLs, and phone numbers directly from screenshots.\nExternal Intelligence: We chained multiple powerful APIs to gather context:\n\n\nTwilio API: For sender reputation and phone number validation.\nGoogle Safe Browsing API: For checking URL reputation and domain age.\nOpenAI API: For semantic analysis to detect urgency, threats, and linguistic patterns.\n\nThe Brain (Machine Learning): All these signals feed into our custom-trained XGBoost Model. This mod\n\n**Challenges**\nOCR Accuracy on Varied Backgrounds: Extracting text reliably from screenshots with different lighting, resolutions, and compression artifacts using Tesseract.js required significant tuning to ensure we didn't miss critical URLs or phone numbers.\nFeature Engineering for XGBoost: Determining which features were most predictive of a scam was difficult. We had to balance technical signals (such as URL age) with linguistic signals (such as \"urgency\") to ensure legitimate high-priority messages (such as 2FA codes) weren't flagged as false positives.\nReal-Time Latency: Chaining multiple APIs (Twilio, Google, OpenAI) creates a risk of slow responses. We had to optimize our asynchronous requests to ensure the user gets a \"verdict in seconds\" rather than waiting for a long analysis.\n\n**Accomplishments**\nThe \"Glass Box\" Approach: We didn't just build a black-box AI that says \"Bad.\" We are proud of our UI that breaks down the Analysis Evidence. Showing the user exactly why a message was flagged (e.g., \"URL not flagged, but AI analysis detected urgency tactics\") builds genuine user trust.\n45-Feature Threat Analysis: Successfully implementing a model that considers 45 different variables gives our detection engine a depth that simple keyword matching cannot achieve.\nSeamless Image Handling: Getting the \"Parser & OCR\" layer to work smoothly allows us to catch scams that hide in images, which is a massive gap in many current security tools.\n\n**What we learned**\nScams are Structural: We learned that while the content of scams changes, the structure (urgency + obscure link + unknown sender) remains remarkably consistent. This validated our choice to use XGBoost to detect these patterns.\nContext is King: A URL might be safe, but the context (e.g., asking for a toll payment via text) is what makes it a scam. Relying on a single data point (like Safe Browsing) isn't enough; you need the multi-layered approach we built.\n\n**What's next**\nLumos is just our first step toward a safer digital ecosystem. Our roadmap includes:\n\n\nBrowser Extensions & Email Plug-ins: Moving Lumos from a destination site to a tool that lives where the users are, automatically scanning content as it arrives.\nReal-Time APIs: Opening our detection engine to other developers to bring trust to their platforms.\nEnhanced Community Feedback: Allowing users to report new scam templates to retrain our XGBoost model in real-time.\n\n",
    "prize": "Winner Best Technical Implementation Award",
    "techStack": "axios, express.js, github, google-safe-browsing, javascript, node.js, ocr, openai, tesseract, twilio-lookup-api, xgboost",
    "github": "https://github.com/AndersonTsaiTW/HackTheSource_lumos",
    "youtube": "https://www.youtube.com/watch?v=HYaMgOeQ3ko",
    "demo": null,
    "team": "Ming Hung Fan, Anderson Yu-Hong Cai",
    "date": "2025-12-01",
    "projectUrl": "https://devpost.com/software/corematrix"
  },
  {
    "title": "K1",
    "summary": "K1 is an innovative project that leverages cutting-edge technologies to create an engaging user experience, likely in the realms of web applications or interactive visualizations. While specific details are not provided, it appears to focus on delivering a seamless and immersive experience, possibly utilizing 3D graphics and real-time data interactions.",
    "description": "**What it does**\nK1 routes AI workloads across Earth + orbital data centers using live APIs. Input: GPU-hours, dataset size, priority. K1 calls UK National Grid, Electricity Maps (200+ zones), and Azure for real-time carbon and cost data. Output: Optimal routing with 41% less carbon, 53% less water, 47% lower cost. For flexible jobs, recommends time-shifting: \"Wait 4 hours for solar peak ‚Üí Save 156kg more CO‚ÇÇ.\"\n\n**Inspiration**\nAI training evaporates 1.8 liters of water per kWh. Grid carbon varies 24x globally: Sweden's hydro at 13 gCO‚ÇÇ/kWh whereas Germany's coal at 320 gCO‚ÇÇ/kWh. Same GPU-hour costs 10x more in the wrong location. For eg,  Starcloud-1 launched, on Nov 3, 2025, first NVIDIA H100 in orbit, confirming energy is 22x cheaper in space. Zero water. K1 answers: Where and when should AI run?\n\n**How we built it**\nStack: Next.js 14, TypeScript, react-globe.gl (3D Earth), Tailwind APIs: UK National Grid (free), Electricity Maps (500/day free tier), Azure Retail Prices Algorithm: Weighted scoring (40% carbon, 30% cost, 30% latency) ‚Üí 70% to best region, 30% to rest. Data centers: Quebec (2 gCO‚ÇÇ/kWh), UK (LIVE), California (LIVE), Germany (LIVE), Sweden (LIVE), Orbital LEO-1 (8 gCO‚ÇÇ/kWh), Orbital LEO-2 (10 gCO‚ÇÇ/kWh).\n\n**Challenges**\nMaking it real: Most \"green AI\" projects are mockups. \nSolution: Starcloud-1 launched Nov 2025 (operational hardware). \nLive data reliability: Graceful fallbacks to cached data (30 min old) with clear indicators.\n\n**Accomplishments**\nFirst orbital routing demo using real Starcloud-1 specs (launched 3 weeks ago, in orbit now)\nLive APIs proving 24x variance on screen in real-time\nClean UI with 3D globe showing exactly where jobs run and why Impact at scale: If K1 routed 1% of global AI training ‚Üí 150,000 tons CO‚ÇÇ saved, 6.5 billion liters water conserved, $1.8B cost reduction annually\n\n**What we learned**\nLocation is everything: Same GPU-hour produces 24x different emissions depending on grid. Most ML frameworks don't even ask \"where should this run?\" Water is the hidden crisis: AI labs obsess over carbon offsets. Meanwhile California data centers drain aquifers in drought zones. Orbital radiative cooling uses zero freshwater, that should be the headline. Space is cheaper: Solar in orbit generates 5x more power (no atmosphere, no night). $0.002/kWh vs $0.045/kWh on Earth grid.\n\n**What's next**\nExpand to 10+ regions (ERCOT, CAISO, NYISO), historical carbon charts, export to Terraform/Kubernetes for real deployments. Starcloud-2 telemetry integration when it launches, batch job scheduling, ML carbon forecasting (48-hour ahead predictions).\n\n",
    "prize": "Winner Best Innovation Award; Winner Best User Experience (UX) Award; Winner Best Overall Hack Award",
    "techStack": "azure, next.js-14, three.js, typescript",
    "github": "https://github.com/samartho4/K1",
    "youtube": "https://www.youtube.com/watch?v=EAqc6N6vVQE",
    "demo": "https://k1-phi.vercel.app/",
    "team": null,
    "date": "2025-12-04",
    "projectUrl": "https://devpost.com/software/orbitops"
  },
  {
    "title": "Timeline Chaos",
    "summary": "Timeline Chaos is an interactive game that likely centers around managing events or actions within a chaotic timeline, utilizing a tap-to-move mechanic for user engagement. Players navigate through various challenges that arise in a non-linear narrative, aiming to restore order or achieve specific objectives within the timeline.",
    "description": "**Inspiration**\nWe're all major sci-fi fans in our team and time travel or the multiverse is a frequent conversation topic among us. We love movies and shows that address this subject like Dr. Who, Dark Matter, They Cloned Tyrone, 12 Monkeys, Edge of Tomorrow etc.  And although it's a pretty hard subject to tackle in game design, we took on the challenge!\n\n**Challenges**\nTime travel and time paradoxes are hard to explain in real life, so you can imagine that creating a tutorial and gameplay that makes sense and can be easily picked up was quite a challenge. There's still room for improvement here but we did our best.\n\n**Accomplishments**\nIn 3 weeks time we created a time travel game that makes sense and isn't so convoluted that it breaks your brain.\n\n**What's next**\nnew maps\nobjectives\nupgrades to the clones\n\n",
    "prize": "Winner Best use of tap-to-move",
    "techStack": "horizondesktopeditor, typescript",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=wXLEjbjVnck",
    "demo": "https://horizon.meta.com/world/10231158779333279",
    "team": "David Pripas, Martin Kelly, AndreiSnyte18, Andrei Vaduva, Liviu Focsa",
    "date": "2025-12-08",
    "projectUrl": "https://devpost.com/software/timeline-chaos"
  },
  {
    "title": "The Merchant",
    "summary": "TECHNICAL HIGHLIGHTS: The project utilized a combination of generative AI (genai) and robust development frameworks such as Horizon and TypeScript. Notable implementations likely include interactive 3D environments and AI-driven functionalities that enhance user engagement and streamline the shopping process.",
    "description": "**What it does**\nThe Merchant is a mobile-first and portrait-oriented cozy fantasy shopkeeping and crafting game built primarily around Noesis where players are guided by Julia, the games AI Mentor, as they craft items using simple taps, fulfill customer requests, hire workers, customize their space, and steadily grow their shop over time. With World Broadcast, aspiring shopkeepers can peek into the big leagues and see how seasoned owners run their shops\n\n**Inspiration**\nWith The Merchant, I wanted to revisit the foundation of an older game of mine, Hantverk Village, and take it in a new direction. It keeps a simple, satisfying crafting loop at its center and builds it into a full shopkeeping world where you run your own shop, fulfill customer requests, customize your space, hire workers with personality, and grow through progression that feels genuinely earned\n\n**How we built it**\nI built The Merchant by starting small and expanding the world one system at a time. The crafting loop came first, anchoring everything, then customer requests, then workers as the shop took shape. Progression grew alongside each new feature. Once the core was solid, the day-night cycle brought the world together. Each major feature was built as its own clean, independent component, which let the shop slowly wake up without breaking anything\n\n**Challenges**\nLearning Noesis and working through the depth inside each core system, from customers to crafting to workers, took a lot of iteration to make everything feel responsive, cohesive, and fully integrated\n\n**Accomplishments**\nI'm proud of how much the project grew. The Merchant became the largest game I have ever built, with over sixty handcrafted scripts working together to create a world that's fun and looks good. It pushed me into deeper system design, more thoughtful structuring, and a level of polish I had not reached before. Seeing all of my work come into the playable experience that it is feels like a genuine milestone, both in scope and in what I learned while building it\n\n**What we learned**\nBuilding The Merchant showed me how essential structure and iteration are on a large project. Modular systems made complex behavior manageable, and constant tuning shaped the feel of the world. Designing for long-term progression pushed me to think more carefully about pacing. The whole project became a crash course in building for scale and stability without getting overwhelmed\n\n**What's next**\nI am committed to bringing the following to The Merchant as time permits: IWP (In-world purchases)\n\n\nIncluding clothing with traits\n\nMore tiers for each category of item recipes\nMore furnishings and decorations\nShop expansions with larger layouts and rooms\nExpand on customer interaction & behaviors\nExpand on worker personality and depth\n\n\nIncluding specializations (traits of sorts) and randomized voice lines \n\nExpand on rewards to include more than just time played\n\n\nIncluding achievements/milestones for taps, items crafted, requests completed, etc.\n\nEnable multiplayer support (once it can be properly tested) with multiple areas/players\n\n\nEvery system was built to support multiplayer, but I have not been able to properly test it before the deadline\nAlternatively, keep the game 1P to be abl\n\n",
    "prize": "Winner Best World that Leveraged Noesis",
    "techStack": "genai, horizon, horizonworlds, meta, ts, typescript",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=5xCAhIs8gNU",
    "demo": "https://horizon.meta.com/world/799885093215507/",
    "team": "I did everything",
    "date": "2025-12-08",
    "projectUrl": "https://devpost.com/software/shop-simulator"
  },
  {
    "title": "Compile Crops",
    "summary": "TECHNICAL HIGHLIGHTS: Compile Crops was built using a combination of Android Studio for app development, Blender for 3D modeling, and Figma for UI/UX design, alongside Meta's Spatial SDK for creating interactive experiences. This blend of tools allowed for a rich immersive environment that enhances user interaction and learning.",
    "description": "**Inspiration**\nNew learners often struggle with how abstract coding feels. Tutorials and browser-based edutainment apps are helpful, but they rely on flat screens and offer little sense of physical cause and effect. We wanted to explore how meta-learning and gamification could feel alive inside a spatial environment.\n\n**How we built it**\nCompile Crops was created by Gulzar, a full-stack developer with experience in mobile apps, and Grace, a product designer and XR prototyper. Gulzar built the core farming systems, drone logic, code-block interpreter, and UI interactions using kotlin and the Spatial SDK‚Äôs Entity Component System. Special efforts went into building a custom AST to visualise code execution in realtime. We made use of custom shaders provided by the spatial sdk to enhance visuals Grace designed the experience flow, created all 3D assets, and implemented object placement, farm layout, and the styling within the Spatial Editor. Our workflow combined Kotlin programming knowledge (for logic, ECS, and UI) with Meta‚Äôs Spatial Editor (for placement, surfaces, and MR composition). More on Development (Gulzar) 2D stuff:\n\n**Challenges**\nOn the Developer Side (Gulzar): Custom shaders took time to work with since they use GLSL, which I‚Äôm not very familiar with. The dual-grid tile map also took time to implement. Positioning entities relative to each other was tricky at first‚ÄîI had to figure out the TransformParent component. After that, it became much simpler. On the Designer Side (Grace): Working with the Spatial Editor was tough because importing custom assets wasn‚Äôt predictable. Editing, testing, and rebuilding assets through Android Studio was also a big shift from Unity or Blender-first workflows. But, I did appreciate the fast build times compared to Unity!\n\n**What's next**\nWe‚Äôre just getting started with Compile Crops. Our next steps include: Compete in multiplayer with your friends by building the most optimized drone for your farm.\nAdd better rewards and a more robust progression system.\nAdd improved levels that incorporate interesting algorithms such as nearest-neighbour and greedy approaches. With additional support or funding, we believe we can launch a publicly playable version in summer of 2026, bringing spatial coding education closer to users everywhere.\n\n",
    "prize": "Winner Best Immersive Experience Built with Spatial SDK",
    "techStack": "android-studio, blender, figma, meta-spatial-sdk, spatial-sdk",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=ZNeVG0tUZiQ",
    "demo": null,
    "team": null,
    "date": "2025-12-09",
    "projectUrl": "https://devpost.com/software/compile-crops"
  },
  {
    "title": "Safe2Go",
    "summary": "TECHNICAL HIGHLIGHTS: Notable implementations include the integration of Arduino and ESP32 for real-time data transmission, GPS functionality for accurate location tracking, and a robust backend developed in Node.js to handle data processing and user interactions. The use of C++ for low-level hardware programming ensured efficient performance and responsiveness.",
    "description": "**What it does**\nSafe2Go is a compact IoT-based module that fits into any helmet and monitors motion, impact, and rider activity. When a crash is detected, it instantly sends alerts to emergency contacts via WhatsApp or email along with live GPS location. The system works offline for accident detection and uses network connectivity for alerts. A dashboard visualizes real-time sensor data and activity logs.\n\n**Inspiration**\nThe idea grew from observing how many road accidents go unnoticed or receive delayed help. I wanted to design something small yet impactful‚Äîsomething that could sit inside any helmet and call for assistance when the rider cannot. The thought that a few seconds could save a life became the core motivation behind Safe2Go.\n\n**How we built it**\nWe started by researching accident dynamics and exploring sensors like:\n-Accelerometer and Gyroscope\n-Sound and Vibration sensors\n-GPS and Emergency Communication APIs\nUsing an ESP32 microcontroller, we captured multi-sensor readings and set impact thresholds\nWe created a Node.js backend to process incoming data, integrated Twilio for WhatsApp alerts, SMTP for email notifications, and built a lightweight React dashboard for visualization.\n\n**Challenges**\nSetting correct impact thresholds without false triggers\nAchieving stable communication during high-speed movements\nPower optimization to keep the device small and battery-efficient\nDesigning the module to fit inside any helmet without discomfort\nDebugging real-time data flow between ESP32 ‚Üí Server ‚Üí WhatsApp/Email\n\n**Accomplishments**\nBuilt a working prototype capable of detecting accidents reliably\nSuccessfully sent automated WhatsApp and email alerts with location\nCreated a live monitoring dashboard with continuous sensor streaming\nEnsured the system remains compact, low-cost, and helmet-agnostic\n\n**What we learned**\nWe learned about IoT workflows, real-time sensor calibration, backend-to-device communication, and emergency automation. This project strengthened our skills in embedded systems, cloud integration, and data visualization.\n\n**What's next**\nWe plan to add fall-detection AI, fatigue monitoring, and voice-based SOS triggers. Future upgrades include integrating eye-state detection, navigation instructions, improved power efficiency, and launching Safe2Go as a universal plug-and-use safety device for everyday riders.\n\n",
    "prize": null,
    "techStack": "arduino, c++, esp32, gps, node.js, react",
    "github": "https://github.com/prav10140/Safe2Go",
    "youtube": "https://www.youtube.com/watch?v=aYt9UUsKFaM",
    "demo": "https://navigation-nine-umber.vercel.app/",
    "team": "Praveen Kumar Patel",
    "date": "2025-12-01",
    "projectUrl": "https://devpost.com/software/safe2go"
  },
  {
    "title": "Food Fighter",
    "summary": "TECHNICAL HIGHLIGHTS: Food Fighter was built using a combination of advanced technologies, including editor, horizon, meta, typescript, and worlds. These tools likely facilitated the development of a seamless user interface and interactive features, allowing users to engage with the platform effectively.",
    "description": "**What it does**\nFood Fighter is a competitive game where players grow larger by eating food. Eat food that drops onto the field to increase your size\nThe bigger you get, the slower you move ‚Äî a clear risk & reward system\nUse weapons to attack and hinder other players\nWhen you think you're big enough, head to the finish zone in the center to record your score on the leaderboard\nIntuitive HUD and ‚ÄúHow to Play‚Äù UI make it easy for newcomers to jump right in The goal is simple:\n‚ÄúWho can eat the most?‚Äù\n\n**Inspiration**\nI believed that a game where you grow bigger the more you eat is a fun concept anyone can enjoy.\nBut rather than making it just a simple growth game, I wanted to add a competitive PvP element ‚Äî\ncombining gluttony and combat to create a humorous yet intense experience.\nMy goal was to deliver an immersive gameplay experience where players move around, eat for themselves, and fight for themselves.\n\n**How we built it**\nFood Fighter is developed using the Meta Horizon Worlds Editor and TypeScript. Implemented interactions based on META Horizon Script\nCreated custom HUD UI (food count display) and How To Play popup UI\nBuilt a grabbable weapon system (force hold, collision detection, swing validation)\nSaved individual player scores using PersistentStorage\nImplemented accurate hit detection using collision events + swing timing\nLocal UI updates through bindings for player-specific HUD refresh\nProduced custom image assets and integrated them into the UI While it may seem feature-rich, the overall architecture is optimized with one clear goal ‚Äî\n‚ÄúFast and intuitive combat.‚Äù\n\n**Challenges**\nWhile developing Food Fighter, I encountered and solved the following issues: Understanding the Horizon UI System\n-Challenges with distinguishing between Local Script and Default Script\n-Fixed issues where onClick wouldn‚Äôt trigger (layer, input mode, hidden state etc.)\n-Resolved UI updates affecting all players instead of individual ones\nDuplicate Weapon Collision Detection\n-OnPlayerCollision was being triggered multiple times per frame\n-Introduced a Set-based system to ensure only one hit is counted per swing\nAttach & Grabbable Conflicts\n-Replaced attachToPlayer with a force hold approach\n-Applied per-player grip offsets to correct inconsistent weapon grip positions\nImage Asset Loading\n-Solved Image source undefined issues\n-Fixed custom UI failing to render properly Each issue helped deep\n\n**Accomplishments**\nThe four aspects of Food Fighter that I‚Äôm most proud of: A fully custom UI system\n-HUD, How To Play popup, icons, and close buttons\n-Easy for new players to understand the game quickly\nAccurate weapon hit detection\n-Swing timing, collision detection, and duplicate-hit prevention provide satisfying combat feedback\nGrowth mechanic based on eating\n-Simple yet highly addictive core gameplay rule\nFull multiplayer support\n-Player-specific HUD\n-Player-specific storage\n-Competitive elements are cleanly handled\n\n**What we learned**\nI learned a lot about how to structure games in Horizon for better maintainability and scalability, especially through developing Food Fighter.\nKey learnings include: How Horizon Worlds‚Äô UI system works\nHandling clicks through Pressable objects and Input Modes\nUnderstanding collision event physics\nManaging player data using PersistentStorage\nUpdating UI uniquely per player (bindings + player-specific updates)\n\n**What's next**\nMoving forward, I plan to add the following features: Additional weapon types (hammer, stick, frying pan, etc.)\nUltimate skill system: unleash a powerful attack after eating a certain amount\nNew maps with strategic elements (jump pads, narrow paths, etc.)\nEnhanced hit feedback with sound effects and impact particles\nLobby waiting room and tutorial area\nCosmetic customization (skins, colors, different forks)\n\n",
    "prize": "Winner Best Social Experience",
    "techStack": "editor, horizon, meta, typescript, worlds",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=kDbQK6JTjaY",
    "demo": "https://horizon.meta.com/world/873769362490133/?hwsh=ijnVoN2iJF",
    "team": "finallee, Shin JaeYoung, DaeWan Kwon",
    "date": "2025-12-08",
    "projectUrl": "https://devpost.com/software/food-fighter"
  },
  {
    "title": "ModStorm Block Party",
    "summary": "TECHNICAL HIGHLIGHTS: Key technical implementations include the use of WebRTC for real-time audio and video communication, WebSockets for efficient data transfer, and Three.js for rendering 3D graphics. The integration of WebXR allows for immersive experiences across different devices, enhancing the overall interactivity of the platform.",
    "description": "1234567    \n\n\n\n      \n  ModStorm Block Party is a fast-paced collection of short, frantic multiplayer party games built to be instantly playable across VR, mobile, and desktop. Inspired by classic local party games, it captures the thrill of simple rules, quick rounds, and chaotic competition with friends. We wanted to bring that energy to VR without friction. No App install or updates. Players just click a link, form a party, and jump into a rotating mix of challenges.\n\nBuilt with our JavaScript engine that uses the lightweight Three.js renderer, WebSockets for real-time multiplayer, WebRTC for voice chat, and WebXR support to deliver a VR-first social experience. This allowed us to focus on designing the mini-games and fine-tune the VR experience.\n\nMini-games span four categories: collection, building, destruction, and free-for-all. Collection challenges have players gather scattered or moving targets throughout the arena. Building rounds task players with assembling creations quickly to avoid falling. Destruction challenges create chaos as players smash opponents‚Äô builds while defending their own. Free-for-all rounds push players to out-survive, out-score, and out-maneuver friends in fast-paced arenas. Players progress through rounds together, earning points toward a final score, keeping sessions lively, replayable, and ideal for social play.\n\nBlock Party‚Äôs performance profile is a key strength. The experience is around 30MB, loads quickly in the browser, and runs the same JavaScript across VR, mobile, and desktop. The multiplayer stack supports thousands of players with low latency, and WebXR ensures VR players interact naturally with controllers, while others can play via touch, keyboard, or gamepad.\n\nChallenges Faced:\n\n\nTo stay on schedule, we prototyped a new mini-game every day, balancing creativity, gameplay variety, and technical constraints.\nBuilding a robust multiplayer party system required coding for matchmaking, session persistence, and smooth transitions.\nFrequent multiplayer VR playtests ended up taking a significant portion of our schedule.\nKeeping the mini-games small in scope also proved challenging, as developing in VR sparks so many creative ideas that simply couldn‚Äôt fit within our timeline.\n\n\nLooking Ahead:\n\n\nExpand mini-game mechanics with team modes, co-op challenges, and light puzzle rounds that provide quick mental resets between the more action-heavy sequences.\nUnique party avatars, replacing the default avatar to let players express individuality.\nHands with Arm IK, enabling natural gestures for more immersive VR embodiment.\nBrowser-based Editor, opening UGC creation so players can contribute mini-games to the Block Party rotation.\n\n\nOver time, ModStorm Block Party aims to become a living party hub - part competitive game, part social space, and part community-driven creation platform - all instantly accessible on the open web.\n\nInstructions:\n\n\nType the game url into a Quest Browser tab\nClick the VR button to enter I",
    "prize": "Winner Judge‚Äôs Choice",
    "techStack": "amazon-web-services, javascript, node.js, three.js, webrtc, websockets, webxr",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=qsvlQcJF09A",
    "demo": null,
    "team": "Kory Fluckiger",
    "date": "2025-12-09",
    "projectUrl": "https://devpost.com/software/party-blast"
  },
  {
    "title": "Sprint Focus",
    "summary": "Sprint Focus is a web application designed to enhance productivity and focus during work sprints. By providing tools for time management and task prioritization, it aims to help users maintain concentration and effectively track their progress throughout their work sessions.",
    "description": "**What it does**\nSprintFocus is a comprehensive productivity Chrome extension that combines three essential tools into one seamless experience. At its core is a customizable Pomodoro timer with a beautiful visual progress ring that helps users maintain focus through timed work sessions. The integrated Kanban style task board allows users to organize their work with drag and drop functionality, custom tags, and the ability to link tasks directly to focus sessions. The analytics dashboard provides deep insights into productivity patterns with pie charts showing time distribution across different subjects, a GitHub style activity heatmap displaying daily focus patterns, and streak tracking to maintain motivation. Users can customize everything from timer durations to color themes, making it truly their own pr\n\n**Inspiration**\nAs students and developers, we constantly struggle with maintaining focus in an age of endless distractions. We noticed that while many Pomodoro timer apps exist, they often lack integration with task management and meaningful analytics. We wanted to create a solution that not only helps users focus but also provides insights into their productivity patterns and helps them organize their work effectively. The goal was to build an all in one productivity tool that lives right in your browser, always accessible when you need it most.\n\n**How we built it**\nWe built SprintFocus using modern web technologies to ensure a smooth and responsive user experience. The frontend is powered by React 19 with TypeScript for type safety and maintainable code. We used Tailwind CSS for styling, creating a beautiful and consistent design system with support for multiple themes and dark mode. For data visualization, we integrated Recharts to create interactive pie charts and custom activity heatmaps. The extension uses Chrome Extension Manifest V3, leveraging the side panel API for a native browser experience and the local storage API for data persistence. We used Vite as our build tool for fast development and optimized production builds. The drag and drop functionality was implemented using native HTML5 APIs, and we created custom React hooks for managing t\n\n**Challenges**\nOne of our biggest challenges was managing state synchronization between the React components and Chrome's storage API. We needed to ensure that timer state, tasks, and analytics data remained consistent across browser sessions and component updates. Another significant hurdle was optimizing the animations and re renders. Initially, our pie charts would re render on every timer tick, causing performance issues. We solved this by implementing React.memo and carefully managing component dependencies. Creating a responsive and beautiful UI that works seamlessly in the Chrome side panel's constrained space required multiple iterations. We also faced challenges with the drag and drop implementation, particularly ensuring smooth animations and proper state updates when moving tasks between colum\n\n**Accomplishments**\nWe're incredibly proud of creating a polished, production ready extension that genuinely solves a real problem. The visual design exceeded our expectations, with smooth animations, beautiful color themes, and a professional interface that rivals commercial productivity apps. Successfully implementing complex features like the activity heatmap and real time analytics while maintaining excellent performance is something we're particularly proud of. The seamless integration of three major features (timer, tasks, analytics) into one cohesive experience demonstrates strong architectural planning. We're also proud of the attention to detail, from custom scrollbars to thoughtful loading states and error handling. The extension is fully functional with no external dependencies or API keys required\n\n**What we learned**\nThis project taught us valuable lessons about building browser extensions with modern web technologies. We gained deep experience with Chrome Extension Manifest V3 and its APIs, particularly the side panel and storage APIs. We learned how to optimize React applications for performance, especially when dealing with frequent updates like timer ticks. Managing complex state across multiple components and persistent storage taught us important patterns for state management. We improved our TypeScript skills by creating comprehensive type definitions for our data structures. Working with data visualization libraries like Recharts gave us insights into creating meaningful and beautiful charts. We also learned the importance of user experience design, spending significant time on animations, tran\n\n**What's next**\nWe have exciting plans to expand SprintFocus into an even more powerful productivity platform. First, we want to add cloud sync functionality so users can access their data across multiple devices while maintaining privacy through end to end encryption. We plan to implement more advanced analytics, including weekly and monthly reports, productivity trends over time, and AI powered insights that suggest optimal work patterns based on user data. Integration with popular task management tools like Todoist, Trello, and Notion would make SprintFocus a central hub for productivity. We're considering adding team features, allowing groups to share focus sessions and track collective productivity. Customizable notification sounds, more theme options, and the ability to create custom color schemes a\n\n",
    "prize": "Winner 3rd Overall",
    "techStack": "css3, html5, javascript, react, typescript, vite",
    "github": "https://github.com/sauruvibes/SprintFocus",
    "youtube": "https://www.youtube.com/watch?v=VX_ybg3MQ-A",
    "demo": "https://chromewebstore.google.com/detail/oljpknajnmpcbmcalcfjpgbjaifolikb?utm_source=item-share-cb",
    "team": null,
    "date": "2025-12-14",
    "projectUrl": "https://devpost.com/software/sprint-focus"
  },
  {
    "title": "Style Swipe",
    "summary": "Style Swipe is an innovative digital platform designed to enhance the shopping experience for the next generation of fashion consumers. By leveraging advanced technologies, it connects users with personalized fashion recommendations and interactive experiences, ultimately facilitating a more engaging and tailored retail journey.",
    "description": "1234567    \n\n\n\n      \n  StyleSwipe\n\nA modern AI-powered virtual try-on application that helps users discover and visualize clothing items that match their personal style preferences. Built with FastAPI, PostgreSQL, and Google's Gemini AI.\n\nWhat is StyleSwipe?\n\nStyleSwipe is a Tinder-style swiping interface for clothing discovery. Users upload photos of themselves, specify their style preferences, and then swipe through AI-generated images showing them wearing different clothing items. The app uses Google Shopping API to find real products and Google's Gemini AI to generate realistic virtual try-on images.\n\nKey Features\n\nüì∏ Image Upload & Processing\n\n\nUpload three photos (front, side, back views) of yourself\nSupports both JPG and PNG formats\nAutomatic image compression and optimization\nImages stored securely in user-specific folders\n\n\nStyle Preferences\n\n\nComprehensive preference form:\n\n\nGender and size selection\nMultiple clothing styles (casual, formal, sporty, streetwear, vintage, etc.)\nClothing types (tops, bottoms, outerwear, shoes, accessories, etc.)\nBudget range and color preferences\nAdditional notes for specific requirements\n\n\n\nSmart Product Search\n\n\nIntegrates with Google Shopping API via SerpApi\nSearches UK retailers for products matching user preferences\nDownloads product images and links\nSaves product metadata (price, rating, reviews, source)\n\n\nAI-Powered Virtual Try-On\n\n\nUses Google Gemini AI to generate realistic try-on images\nCreates combined images from three angles (front, side, back)\nAll generated images in 9:16 aspect ratio (perfect for mobile viewing)\nHigh-quality image generation with natural clothing fit\n\n\nüëÜ Tinder-Style Swiping\n\n\nIntuitive swipe interface:\n\n\nTap card ‚Üí Cycle through front/side/back views\nSwipe right ‚Üí Like the item\nSwipe left ‚Üí Dislike the item\n\nSmooth animations and drag interactions\nProgress tracking (X / Y products swiped)\n\n\nResults & Analytics\n\n\nView all liked items in a beautiful results page\nSee product details: title, price, source, ratings\nDirect links to purchase products\nAll data stored in PostgreSQL for analytics\n\n\nüìä Grafana Integration\n\n\nFull PostgreSQL database for analytics\nTrack product clicks, swipe patterns, and user preferences\nCreate dashboards for:\n\n\nClick-through rates\nPopular product types\nUser engagement metrics\nConversion funnels\n\n\n\nArchitecture\n\nBackend\n\n\nFastAPI - Modern Python web framework\nPostgreSQL - Relational database for all data\nSQLAlchemy - ORM for database operations\nGoogle Gemini AI - Image generation and virtual try-on\nSerpApi - Google Shopping product search\nPillow - Image processing\n\n\nFrontend\n\n\nHTML/CSS/JavaScript - Modern, responsive design\nDark theme - Beautiful pink/gold gradient accents\nTouch-friendly - Swipe gestures for mobile and desktop\nLoading animations - Smooth user experience\n\n\nDatabase Schema\n\n\nusers - User accounts and metadata\nuser_images - Uploaded image paths\npreferences - User style preferences\nproducts - Products from search API\nswipes - Swipe histo",
    "prize": "Winner [MLH] Best Use of Gemini API; Winner [Frasers Group] Build a digital experience that attracts and retains the next generation of customers to the fashion retail market.",
    "techStack": "frasers, gemini, postgresql",
    "github": "https://github.com/GitFarhanS/HackSheffield25",
    "youtube": "https://www.youtube.com/watch?v=xhO6g5oC-e8",
    "demo": null,
    "team": "Fun to use sometimes",
    "date": "2025-11-30",
    "projectUrl": "https://devpost.com/software/style-swipe-7h2zak"
  },
  {
    "title": "SupplyTrace",
    "summary": "TECHNICAL HIGHLIGHTS: SupplyTrace was built using CSS, HTML, and JavaScript, showcasing a strong front-end development. The integration of OpenAI technologies likely enabled advanced data processing and analytics, allowing for dynamic insights and user interactions, which enhanced the overall user experience.",
    "description": "**What it does**\nSupplyTrace is a web application that generates interactive supply chain visualisations. Users input a product, and the application deconstructs it into its raw materials, refinement processes, and components, geographically linking each to its most likely industrial hub. The results are presented on an interactive global map, highlighting the intricate network that connects resources, factories, and consumers. An accompanying flow diagram provides an alternative view of the supply chain, further enhancing understanding.\n\n**Inspiration**\nThe grand adventures of mythology might seem distant, but a far more impressive journey unfolds every day ‚Äì the complex supply chains that bring the products we use to our hands. From raw materials to finished goods, a single smartphone or pair of shoes traverses continents, undergoing incredible transformations. This project explores that journey.\n\n**How we built it**\nThe core of SupplyTrace lies in leveraging the power of OpenAI's GPT models. We developed specific prompts to dynamically generate data related to raw materials, locations, and components. This information is then visualised using the HTML Canvas API. To accurately represent the curvature of the Earth, we implemented custom plotting logic, drawing points and lines to create a geographically accurate representation of the product's journey.\n\n**Challenges**\nTaming the AI Output: Ensuring the raw data from GPT could be reliably formatted for visualisation required careful prompt engineering and data parsing techniques.\nCanvas Interaction on a Curved Surface: Detecting mouse hover events accurately over curved lines on the Canvas presented a unique challenge.\nGeographic Accuracy: Correctly plotting locations on the map required careful consideration of map projections and coordinate transformations.\n\n**Accomplishments**\nThe transformation of linear AI output into a linked, interactive graph.\n\n**What we learned**\nThis project reinforced the importance of precise prompt engineering for generating consistent and reliable AI outputs.\n\n**What's next**\nFuture iterations of SupplyTrace will integrate a dashboard to provide comprehensive data analysis. This will include metrics such as total travel distance, carbon footprint calculations, and total time taken for completion, enabling users to gain deeper insights into the environmental and economic impact of supply chains.\n\n",
    "prize": "Winner HackSheffield10 3rd Place",
    "techStack": "css, html, javascript, openai",
    "github": "https://github.com/Ro11ast/product_traceability_visualisation",
    "youtube": "https://www.youtube.com/watch?v=L-CRFjq0HzY",
    "demo": null,
    "team": "Private user",
    "date": "2025-11-30",
    "projectUrl": "https://devpost.com/software/supply-trace"
  },
  {
    "title": "Clash Royale Deck Analyzer",
    "summary": "TECHNICAL HIGHLIGHTS: The project utilizes Python for backend processing, including the PyTorch library for implementing machine learning algorithms that assess deck performance. Additionally, it employs Matplotlib for visualizations, allowing users to easily interpret data and insights, while Tkinter provides a clean graphical user interface for user interaction.",
    "description": "**What it does**\nOur application allows users to easily input their Clash Royale deck either through a graphical interface, by pasting a deck link, or by manually typing in the cards. The app then provides a detailed analysis of the deck, displaying key stats such as the 4-card cycle, average elixir cost, and deck archetype, all backed by data from the top 100 players in the game. The analysis is presented visually through engaging graphs and pie charts, helping users understand their deck's strengths and weaknesses while offering strategic insights on the best ways to play and maximize their chances of winning.\n\n**Inspiration**\nInspired by the Grafana dashboard, we sought to create a unique version tailored to Clash Royale, a popular mobile game. Noticing a gap in deck analysis tools, we decided to build our own solution that not only analyzes a deck but also provides a wealth of information to help players optimize their strategy and improve their chances of winning games.\n\n**How we built it**\nwe found the 100 most commonly used deck from an API and manually classified them as the various archetypes this was then used as training data to create our classifier that provided a confidence rating for each archetype. then we created the GUI based from the in-game version to create a deck to perform the analysis on.\n\n**Challenges**\nlack of existing training data which we solved by creating it our own. issues with bugs created by gen-AI that had to be solved by manual debugging, we also began to run into a point in the project where the AI could no longer make successful modification to the code and development had to proceed manually.\n\n**Accomplishments**\nWe are incredibly proud of our achievement in training a highly accurate model using just 100 decks. We‚Äôre also thrilled with the features we‚Äôve implemented, including a user-friendly graphical interface and text-based input methods, which together provide a seamless and versatile experience for deck analysis and strategy optimization.\n\n**What we learned**\nhow to integrate AI into our workflow, how to gather and create training data for a neural network, how to train a neural network, how to make a image based GUI with search function. How to make and post a YouTube video.\n\n**What's next**\nIn the future, we plan to integrate an AI assistant to further assist players in mastering their deck, offering personalized tips and in-depth guides for each archetype. Additionally, we aim to enhance the user interface, making it more modern and visually appealing. We also plan to introduce features such as EVO variants for cards and other small improvements to make the app more comprehensive and user-friendly. Our goal is for the app to be web-based, allowing users to access it seamlessly from any device.\n\n",
    "prize": "Winner [Comp Soc] People's Choice",
    "techStack": "matplotlib, python, tkinter, torch",
    "github": "https://github.com/Shadowblades746/hackSheffield10.git",
    "youtube": "https://www.youtube.com/watch?v=4VCsr4iWfVc",
    "demo": null,
    "team": "Emad Riyaz, Benjamin Whittaker",
    "date": "2025-11-30",
    "projectUrl": "https://devpost.com/software/clash-royale-deck-simulator"
  },
  {
    "title": "Photon",
    "summary": "Photon is an innovative entertainment experience that leverages mobile technology to deliver immersive interactions, likely incorporating elements of gaming or augmented reality. The project aims to engage users in a captivating way, enhancing their entertainment options through its unique features.",
    "description": "**What it does**\nPhoton is a spatial video sharing platform, where people create, share and explore 3D videos. We also developed the technology to convert any 2D video to 3D, so anyone can convert their favorite 2D video to 3D and easily share on Photon.\n\n**Inspiration**\nIn 2024 we developed Spatial TV, where we found real-time lighting could create a truly amazing video watching experience. We believe the next step would be bringing this amazing experience to 3D videos Back in early 2025, there was no platform for sharing spatial video on Meta Quest yet. So we thought it might be a good idea to be the first the build it out and integrate the spatial lighitng effect.\n\n**How we built it**\nWe started off building the core framework for a content platform: a website for uploading videos and the Photon app for discovering and watching 3D videos. Later, we worked on critical improvements that we think matters the most for our users, including the spatial lighting, personalized recommendation and UI refinement.\n\n**Challenges**\nImplementing spatial lighting for 3D video was the most challenging part, as it involves close integration of a hybrid tech stack: getting the video texture from Android and then sending them to Unity through low level graphic APIs. It also involves a lot of craftsmanship to make the lighting effect look truly beautiful.\n\n**Accomplishments**\nWe're proud to build the first and largest spatial video sharing community on Meta Quest!\n\n**What's next**\nWe see three areas where we'll make improvements in the near future More customization in discovery page, like tag filters\nBetter personalized recommendation and search experience\nCreator incentivization: we plan to launch creator incentive program to support our creator community\n\n",
    "prize": "Winner Best Entertainment Experience Runner-up",
    "techStack": "android, cloudflare, unity",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=5WpxaLfLWOo",
    "demo": null,
    "team": "Shared Account, Noy Wang",
    "date": "2025-12-08",
    "projectUrl": "https://devpost.com/software/photon-bm9p31"
  },
  {
    "title": "E-Translator",
    "summary": "TECHNICAL HIGHLIGHTS: E-Translator leverages HTML and JavaScript to create an interactive front-end, enabling dynamic user input and output without requiring page reloads. Its implementation of APIs for language processing likely enhances translation accuracy and speed, showcasing a well-integrated technical solution.",
    "description": "Explained in Video, please look at video\n\n\n\n        \n    Built With\n\n    htmljavascript\n  \n\n        \n    Try it out\n\n    \n        \n  \n  GitHub Repo",
    "prize": "Winner Most Innovative App Design",
    "techStack": "html, javascript",
    "github": "https://github.com/Pronoob5066/E-Translator/tree/main",
    "youtube": "https://www.youtube.com/watch?v=7zE5Aml9cTc",
    "demo": null,
    "team": "Vishak Saravana Kumar",
    "date": "2025-11-29",
    "projectUrl": "https://devpost.com/software/e-translator"
  },
  {
    "title": "SCX_MUS: Mostly Unfair Scheduler",
    "summary": "SCX_MUS, or Mostly Unfair Scheduler, is a scheduling solution designed to optimize resource allocation in Kubernetes environments. By leveraging eBPF (extended Berkeley Packet Filter) technology, it provides a mechanism for fine-tuning the scheduling of tasks, potentially improving system performance and resource utilization in cloud-native applications.",
    "description": "**What it does**\nSCX_MUS: Mostly Unfair Scheduler is a custom Linux scheduler designed to prioritize containers within Kubernetes, by dynamically adjusting the CPU resources allocated to containers based on their priority. It is implemented using the sched ext framework, to give priority to certain containers over others, improving the performance of specific workloads, even in resource-heavy or \"noisy neighbor\" environments. The userspace component, written in Go, provides a simple CLI that allows users to select which container to prioritize. The tool lists all currently running containers and prompts the user to choose one for prioritization. Once a container is selected, the scheduler retrieves the associated cgroup ID and saves it to the BPF map.\n\n**Inspiration**\nThe idea for this project started casually at a bar table. Our group, patos‚Äîall computer science students‚Äîwas gathered when a friend showed us the new Linux scheduling technology, sched_ext. We were fascinated, but it felt unrealistic for a group of students to build a scheduler from scratch. Months later, during a discussion about Kubernetes CPU management‚Äîspecifically why cpu.limits is often discouraged and why cpu.shares is generally preferred‚Äîwe began exploring how computational resources are allocated to containers. This led to a question: Is it possible to dynamically give more compute resources to a container, beyond the usual horizontal scaling or static allocation in manifests? The idea was born: to attempt giving a container higher priority via a custom scheduler.\n\n**How we built it**\nWe spent the hackathon diving deep into the internals of the Linux scheduler ecosystem. We studied the kernel source code, from discovering that Linux actually runs multiple schedulers (not just CFS. FIFO and RR are also present) to examining functions like __schedule() inside kernel/sched/core.c. We combined this understanding with Kubernetes API communication and, most importantly, hands-on eBPF development.\nUsing sched_ext and struct_ops, we implemented a custom scheduling policy in eBPF that applies dynamic, cgroup-based priority to containers.\n\n**Challenges**\nOne of the hardest parts was figuring out how to implement and run a scheduler using sched_ext_ops. There is very little documentation or guides online on how to load and execute your own custom scheduler, only examples of people running pre-made schedulers included with SCX. Another challenge was our initial workflow: we wrote most of the code without compiling or testing it (a terrible practice, we know). Only after reaching a reasonable implementation did we start the work of compiling the kernel with sched_ext support, setting up Kubernetes clusters and designing the benchmark. The debugging phase involved days of solving compilation mysteries, and unexpected behaviors before everything finally worked.\n\n**Accomplishments**\nThe goal of our custom scheduler, wasn't to critique the Completely Fair Scheduler (CFS), our scheduler is, in fact, a highly simplified version of CFS. Instead, it was a two-fold endeavor: To see if students could successfully implement a scheduler from scratch.\nTo explore different approaches to container scalability by dynamically adjusting a container's priority/resource share via a custom scheduler. We consider the project a success in demonstrating both of these concepts.\n\n**What we learned**\nWe learned about: The Linux scheduler architecture and its multiple scheduling classes\nKernel internals and low-level scheduling paths\nKubernetes resource management and API communication\neBPF development and the sched_ext subsystem\nPerformance evaluation, benchmarking, and debugging complex systems\n\n**What's next**\nBuilding a more sophisticated control mechanism that uses the Kubernetes API to gather metrics and automatically adjust the container's priority share based on workload (e.g. implementing a hook that prioritize a container when its netns be with a X quantity of packets)\nRefining SCX_MUS by:\n\n\nAdding multi DSQs for multi-cores enviroments\nDeveloping more sophisticated migration heuristics and improving L2/L3 cache locality\n\n",
    "prize": "Winner eBPF Starter Track (Beginner-Friendly)",
    "techStack": "c, cillium/ebpf, client-go, ebpf, golang, kubernetes, libbpf, magalu-cloud, python, sched-ext",
    "github": "http://github.com/patos-ufscar/Hackathon-eBPF-2025",
    "youtube": "https://www.youtube.com/watch?v=3Z7UYZ7sJTU",
    "demo": null,
    "team": "Rodrigo Coffani, Lucas de Ara√∫jo Cardoso, Oliver M. U., Luiz Ot√°vio Teixeira Mello",
    "date": "2025-11-29",
    "projectUrl": "https://devpost.com/software/scx_mus-mostly-unfair-scheduler"
  },
  {
    "title": "erm.ai",
    "summary": "Erm.ai is a multi-agent artificial intelligence platform designed to facilitate interactions and tasks among various AI agents. The project aims to enhance collaborative problem-solving capabilities through the integration of multiple AI models, allowing them to work together efficiently in real-time.",
    "description": "**What it does**\nerm.ai is an AI Slack agent that monitors conversations, adds helpful insights, and automatically generates n8n-powered LinkedIn-ready updates. It can also be expanded with domain-specific agents.\n\n**Inspiration**\nWe wanted to make teamwork smoother by giving Slack groups an AI teammate - one that listens, supports, and turns group progress into something actionable and shareable. In addition to this, we wanted to be able to provide multi-agent support with fine-tuned models so that we can create a suite supporting a variety of purposes.\n\n**How we built it**\nWe integrated Slack‚Äôs MCP with our custom AI agent, added message filtering and context handling, and connected everything to an n8n workflow for automated LinkedIn content generation, which gets outputted to the Slack account. We also experimented with fine-tuning for specialised agents.\n\n**Challenges**\nConnecting the n8n workflow with the web application due to frequent collisions with the agent running in the background. Managing noisy data, ensuring that it doesn't interfere with our valid data.\n\n**Accomplishments**\nWorking well as a team despite not being familiar with people\n\n**What we learned**\nHow to manage conversational context at scale, orchestrate multi-service integrations, design better agent behaviors, and build AI that supports teams without overwhelming them.\n\n**What's next**\nExpanding our library of domain-specialized agents, improving context detection, adding voice and meeting integrations, and creating a polished dashboard for workflow management.\n\n",
    "prize": "Winner [Reply] Multi-Agent AI Challenge",
    "techStack": "javascript, next.js, python, react, typescript",
    "github": "https://github.com/Vrun1506/Hack-Sheff-10",
    "youtube": "https://www.youtube.com/watch?v=owogrzfarmU",
    "demo": null,
    "team": "Ollie Fishwick, Alric Marvel, Chris Williams",
    "date": "2025-11-30",
    "projectUrl": "https://devpost.com/software/erm-io"
  },
  {
    "title": "FAKE_IT_TILL_YOU_MAKE_IT-02",
    "summary": "FAKE_IT_TILL_YOU_MAKE_IT-02 is an innovative project designed to address future societal challenges by leveraging advanced technologies. Although specific details are not provided, the project's focus on solving future problems suggests a proactive approach to anticipating and mitigating potential issues through creative solutions.",
    "description": "**What it does**\nOur software serves as a layer of active protection between scammer and non-tech-savy people. It will actively take calls, interrupt emails and text messages for the sake of wasting scammers' time. With such software integrated into every phone and email provider we will make scamming impossible. We will waste so much of the scammers time that the scamming itself will stop making money and will seize to exist. We will make sure than scammers's time is wasted and they are reported to the authorities.\n\n**Inspiration**\nScams are getting more and more sophisticated and it's not enough to just protect people. We need to fight the roots of this evil. AI keeps getting more and more realistic, cheaper and easier to use. Therefore if we don't do anything about it, scams will keep getting more and more widespread.\n\n**How we built it**\nWired Gmail API OAuth (credentials.json + token.json) to fetch unread messages.\nSent message content to an n8n workflow that classifies senders via GPT.\nPersisted sender status (whitelist/blacklist/none) in Supabase PostgreSQL.\nAutomated replies for blacklisted senders with time-wasting, humorous templates.\nAdded optional Redis for lightweight caching and throughput.\n\n**Accomplishments**\nEnd-to-end pipeline from Gmail ‚Üí n8n/GPT ‚Üí Supabase ‚Üí auto-replies.\nRobust scam engagement that convincingly wastes scammers‚Äô time without risking users.\nClear sender lifecycle: unknown ‚Üí blacklist/whitelist with learning over time.\n\n**What we learned**\nStrong, explicit system prompts improve consistency in classification and replies.\nConservative defaults and reproducible workflows reduce noisy or ambiguous outcomes.\nSimple states (whitelist/blacklist/none) make automation auditable and robust.\nHumor and confusion patterns are effective for long scammer engagement.\n\n",
    "prize": "Winner The Hackathon WINNER; Winner Best Megatrends - Build Something That Solves a Future Problem - Today Challenge project",
    "techStack": "gmailapi, javascript, n8n, openaisdk, postgresql, python, redis, supabase, twilio",
    "github": "https://github.com/robyseelps/FAKE_IT_TILL_YOU_MAKE_IT-02",
    "youtube": "https://www.youtube.com/watch?v=kS1R03MzwVg",
    "demo": null,
    "team": "Vasyl Paliuha, Oleksandr Kryvolapov, Sabeel Wani, Yurii Levchenko",
    "date": "2025-11-29",
    "projectUrl": "https://devpost.com/software/fake-it-till-you-make-it-02"
  },
  {
    "title": "LES-01",
    "summary": "LES-01 is an innovative application designed to enhance daily life through artificial intelligence. While specific functionalities are not detailed, its focus on making users' days better suggests it likely integrates personalized AI-driven features to improve productivity or well-being.",
    "description": "**What it does**\nWhat2Eat is an AI-powered personal nutrition assistant designed to be your go-to kitchen companion. Here‚Äôs what it does: Personalized Nutrition Tracking: It calculates your daily targets for calories, protein, carbs, and fat based on your profile. The dashboard provides a beautiful, at-a-glance view of your progress with animated circular progress bars.\n  Smart Meal Suggestions: Don't know what to cook? Tell the app what ingredients you have on hand, and our AI wizard will generate delicious meal ideas for you.\n  Instant Recipe Finder: Already know what you want to make? Instantly get a detailed recipe with ingredients and step-by-step instructions.\n  Automatic Shopping Lists: Every recipe you choose can automatically generate a shopping list, so you never forget an ingredient at the store\n\n**Inspiration**\nThe inspiration for What2Eat came from a universal, daily struggle: standing in front of a full fridge and having no idea what to cook. This often leads to decision fatigue, food waste, and unhealthy choices like ordering takeout. We wanted to create a smart, simple solution that removes the stress from meal planning and empowers people to eat better using the food they already have.\n\n**How we built it**\nWhat2Eat is a modern web application built with a focus on a clean user experience and a robust technical foundation. Frontend: We used Next.js with React and TypeScript to build a fast, server-rendered, and type-safe application.\n  UI/UX: The user interface was crafted using shadcn/ui for its excellent, accessible component library, styled with Tailwind CSS. We designed custom components, like the CircularProgress SVG element, to create a unique and intuitive data visualization experience.\n  State Management: Client-side state and interactivity are managed using React Hooks (useState, useEffect). We implemented a custom useWizard hook to handle the modal flows for our core features, ensuring a smooth user journey.\n  Backend: A Next.js API route (/api) serves as the endpoints to fetch dyna\n\n**Challenges**\nOne of the biggest challenges was figuring out how AI should work inside the app. There were so many potential directions - meal planning, fridge detection, grocery optimization, macro analysis - and we had to clearly define what problems our assistant should actually solve. This required a lot of brainstorming, prototyping, and refining before we even started coding. Another major challenge was designing the user interface and overall user experience. Our goal was to make the app feel simple, intuitive, and ‚Äúgrandma-proof,‚Äù even though it performs complex AI-driven tasks. Creating a smooth onboarding flow, meal suggestion interactions, and recipe generation UI pushed us to rethink layout, user choices, and how to present AI results in a clear and friendly way.\n\n**Accomplishments**\nWe are especially proud of our ingredient detection from fridge photos. Being able to take a picture, run it through AI, and automatically identify what's inside the fridge felt like a huge breakthrough for the user experience. Seeing those detected items appear in the pantry list made the app feel instantly useful and surprisingly smart. We're also proud of our detailed and thoughtful onboarding process. By collecting the user‚Äôs goals, preferences, dietary restrictions, and cooking habits, we were able to tailor AI prompts with high precision. This led to highly personalized and accurate recommendations throughout the app. Another accomplishment is the quality of the AI-generated meal suggestions and full recipes. With careful prompt engineering and refinement, the AI consistently deliver\n\n**What we learned**\nWe gained a lot of experience working with modern AI tooling and full-stack technologies. From using the OpenAI SDK for streaming recipe generation, to designing database schemas and exporting typed definitions from Supabase, we learned how to integrate powerful backend services with a clean, responsive frontend. This project also gave us hands-on practice with structured prompt design, AI-driven UX flows, and building a seamless user experience around intelligent features.\n\n**What's next**\nThe future for What2Eat is bright! Our roadmap includes: Grocery Delivery Integration: Allowing users to send their shopping list directly to services like Instacart.\n  Automatic Weekly Meal Planning: Generating a full week's worth of meals based on user preferences and goals.\n  Dietary Preferences: Adding filters for vegetarian, vegan, gluten-free, and other dietary needs.\n  Pantry Tracking: Allowing users to maintain a virtual pantry of their ingredients.\n\n",
    "prize": "Winner Best AI that makes your day Challenge project",
    "techStack": "ai-sdk, nextjs, react-hook-form, shadcn/ui, supabase, tailwindcss, typescript, vercel, zod",
    "github": "https://github.com/TeamLES/hackathon-telekom-2025-What2Eat",
    "youtube": "https://www.youtube.com/watch?v=hS8PuuQIQUo",
    "demo": null,
    "team": "Matej Bend√≠k, Bc. Miroslav Hanisko, Bc. Luk√°≈° ƒåeƒç, kupkoXD Janok, Bc. Oliver Fecko",
    "date": "2025-11-29",
    "projectUrl": "https://devpost.com/software/what2eat-etv6d0"
  },
  {
    "title": "CropGuard",
    "summary": "TECHNICAL HIGHLIGHTS: The project is built with a robust tech stack that includes FastAPI for backend development, React Native for mobile app interface, and AI models from OpenAI and YOLO (You Only Look Once) for image recognition and analysis. The integration of Google Maps for location tracking and weather APIs for forecasting enhances user experience and functionality.",
    "description": "Twilio messages to registered farmer via SMS (free trial)\n    \n\n              \n  \n\n\n\n            \n        \n          \n  \n\n        \n    \n      Training results \n    \n\n        \n          \n  \n\n        \n    \n      Pest detection on pest\n    \n\n        \n          \n  \n\n        \n    \n      Pest detection by uploading a sample\n    \n\n        \n          \n  \n\n        \n    \n      Multiple pests detected in a video\n    \n\n        \n          \n  \n\n        \n    \n      Pest detection on Maize\n    \n\n        \n          \n  \n\n        \n    \n      Circuit Simulation on Wokwi\n    \n\n        \n          \n  \n\n        \n    \n      IoT Dashboard (simulated) --- Real hardware implementation requires farm_ID\n    \n\n        \n          \n  \n\n        \n    \n      Twilio messages to registered farmer via SMS (free trial)\n    \n\n              \n  \n\n\n\n            \n        \n          \n  \n\n        \n    \n      Training results \n    \n\n        \n          \n  \n\n        \n    \n      Pest detection on pest\n    \n\n        \n          \n  \n\n        \n    \n      Pest detection by uploading a sample\n    \n\n        \n          \n  \n\n        \n    \n      Multiple pests detected in a video\n    \n\n        \n          \n  \n\n        \n    \n      Pest detection on Maize\n    \n\n        \n          \n  \n\n        \n    \n      Circuit Simulation on Wokwi\n    \n\n        \n          \n  \n\n        \n    \n      IoT Dashboard (simulated) --- Real hardware implementation requires farm_ID\n    \n\n        \n          \n  \n\n        \n    \n      Twilio messages to registered farmer via SMS (free trial)\n    \n123456789    \n\n\n\n      \n  More pictorial details can be found in the slides\n\nThe Inspiration:\n\nFarmers do face devastating crop losses; often up to 40% pre-harvest due to invisible threats they cannot easily detect. They lack access to constant monitoring tools for critical factors like subsurface soil moisture and are often too slow to detect rapidly spreading pests like the Fall Armyworm across large fields.\nMy dad is a commercial farmer. In 2022, 2023 and 2024; the attacks from these pests on our legume crops both on farmland and after harvest were sever. This attack really resulted in losses and unaffordability to pay the tuition of my brother which made him nearly had an extra year.\nMy mummy manages a Cocoa farm she inherited, and for years we have been enjoying the returns on the yearly investments. Not until this year November that we entered into a huge loss just because we couldn't have the real-time information about the decline in the market price of almost all agricultural products.\nSince then, I had determined to keep on working had to find a lasting means to detect and control this PARASITIC DESTROYER, in order keep them and other farmers abreast of any upcoming market trends and other salient features which are embedded in this web application.\n\nWhat it does\n\nSubsurface Environment Monitoring (IoT + Wokwi): It constantly monitors critical environmental factor soil moisture, temperature, humidity, and light intensity; using simulated",
    "prize": null,
    "techStack": "fastapi, github, google-colab, google-maps, google-web-speech-api, iot, open-meteo-forecast, openai-api, plpgsql, react-native, roboflow, supabase, typescript, vercel, visual-studio, weatherapi, wokwi, yolo11s",
    "github": "https://github.com/Caleb-Tech001/CropGuard.git",
    "youtube": "https://www.youtube.com/watch?v=78Rddq4DgEk",
    "demo": "https://drive.google.com/file/d/1cpXxLF14d4JEKKi1MXjD6t9u7B16OLRX/view?usp=drive_link",
    "team": null,
    "date": "2025-11-29",
    "projectUrl": "https://devpost.com/software/cropguard"
  },
  {
    "title": "AURA ‚Äî Autonomous Unified Review Agent",
    "summary": "TECHNICAL HIGHLIGHTS: The project utilizes a robust tech stack, including JavaScript and React for the frontend, Node.js for backend services, and PostgreSQL for data management. Additionally, it integrates OpenAI capabilities, enhancing its analytical and conversational features, and Python for data processing tasks, ensuring versatility and performance.",
    "description": "**What it does**\nAURA is a fully autonomous AI engineering assistant that transforms how developers approach code quality. Here's what it does:\n\n**Inspiration**\nThe inspiration for AURA came from a fundamental problem we've all experienced as developers: the endless cycle of manual code reviews, test writing, and bug fixing. We noticed that developers spend 30-40% of their time on repetitive QA tasks, and critical bugs often slip through to production, costing companies billions annually. Traditional code analysis tools are reactive‚Äîthey find problems after they exist. We asked ourselves: What if we could predict and prevent issues before they happen? AURA was born from the vision of creating a truly autonomous AI agent that doesn't just analyze code‚Äîit actively improves it, generates tests intelligently, predicts regressions, and takes automated actions. We wanted to build something that works 24/7, learns from your codebase, and becomes smarter\n\n**How we built it**\nWe built AURA as a modern, scalable full-stack application with a focus on modularity and extensibility.\n\n**Accomplishments**\nWe're incredibly proud of what we've built in this hackathon:\n\n**What we learned**\nThis hackathon was an incredible learning experience:\n\n**What's next**\nIDE Integration ‚Äî VS Code and IntelliJ plugins for real-time inline suggestions\nMulti language support expansion\nTeam collaboration feature\n\n",
    "prize": "Winner Best Productivity Award",
    "techStack": "javascript, node.js, openi, postgress, python, react",
    "github": "https://github.com/palsure/AURA?tab=readme-ov-file",
    "youtube": "https://www.youtube.com/watch?v=JJivljuk9T0",
    "demo": null,
    "team": null,
    "date": "2025-11-29",
    "projectUrl": "https://devpost.com/software/aura-autonomous-unified-review-agent"
  },
  {
    "title": "Spell Sword",
    "summary": "Spell Sword is an innovative project that merges interactive storytelling with immersive gameplay, allowing users to engage in a fantasy world where they can cast spells using intuitive controls. The project leverages augmented reality or virtual environments to enhance user interaction and experience.",
    "description": "**What it does**\nSpell Sword is a fantasy turn based RPG that blends strategic wordplay with classic combat progression. Players form attacks by crafting words from dynamic letter pools, using vocabulary skill as a core combat mechanic. As they battle waves of monsters they earn loot, upgrade equipment, unlock consumables and progress through daily quests that encourage long term engagement. Designed for both casual players and RPG fans, Spell Sword aims to deliver an accessible but demanding experience that stands as the first chapter of a larger saga.\n\n**Inspiration**\nWield your words as weapons against waves of monsters. Defeat numerous foes, gather loot, upgrade your gear and complete daily quests as you push deeper into battle in this turn based RPG experience.\n\n**How we built it**\nThe project began as a full exploration RPG with multiple classes, co-op play and story driven quests. To align with the Mobile Innovation theme I intentionally narrowed the scope to a focused combat scope that is optimized for a comfortable one handed portrait orientation mobile game. The result is a one player experience where the challenge comes from choosing the right word at the right moment rather than learning complex input patterns.\n\nMost of the early work went into building the word engine. I developed dictionary utilities, custom rarity tiers and a scoring model for each letter, then curated a 170,000+ word list tuned for modern English, fantasy terms and a few seasonal extras. I merged and cleaned multiple sources, filtered out low quality entries, and created a separate block l\n\n**Challenges**\nOn top of this system I designed a combat loop across 99 floors of increasing difficulty. Each floor considers enemy archetypes, player stats, equipment tiers, consumable drop rates and three player mindsets: casual, dedicated and pay-to-win. Balancing those factors produced more than 4,700 data points that shape a curve which starts generous and ramps into a meaningful challenge. The first 10 floors are intentionally forgiving so new players can jump straight into the fun. Deeper in, stronger weapons, armor, and afflictions become available and a simple word choice turns into a tactical tradeoff.\n\nAfflictions are a key layer. Bleeding accelerates as the player is hit, poison becomes more dangerous the longer a fight lasts, and curse behaves like poison while making the victim more vulnera\n\n**Accomplishments**\nTo make combat feel like a modern RPG instead of a static puzzle I rebuilt the presentation around the player avatar. An avatar and NPC controller drives animations, visual effects, sound cues and the health bar, while a spline based camera system glides through cinematic angles instead of cutting between fixed shots. The result feels closer to a steady cam following action sequence than a simple 2D panel of letters.\n\nLater in development I partnered with Takatado, whose strengths in environment and 2D art complemented my focus on systems and UX. Together we replaced the placeholder arena and wireframe UI with a cohesive visual identity: a damaged stronghold that hints at past battles and a clean, legible interface built for phones. The chibi style monsters, knight and spell tiles carry th\n\n**What we learned**\nUnder the hood, world persistent variables and JSON configuration files drive almost everything, including word lists, shop inventory, drop tables, starting gear, and equipped avatar item traits. This keeps the experience data driven and easy to tune after launch. Custom leaderboards highlight not only highest levels and damage dealt, but also the best words used in battle giving players recognition for both power and creativity while respecting the filtered word lists.\n\n**What's next**\nIn an age dominated by AI generated brain rot and low effort doom scrolling, Spell Sword is my attempt to reward players for skills that rarely get attention on the Meta Horizon platform: vocabulary, planning, and critical thinking. My hope is that this focused combat slice can grow into a broader universe with raid style bosses, additional character classes, co-op play, PvP tournaments and exploration worlds with cognitive skill challenges that all feed into this same word powered combat system.\n\n",
    "prize": "Winner Best Portrait Mode Implementation",
    "techStack": "adobe-illustrator, flaticon-license, horizonworldsdesktopeditor, photoshop, premiere, sunoai-license, typescript",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=tcNFya_hTDU",
    "demo": "https://horizon.meta.com/world/10172905149910634/",
    "team": null,
    "date": "2025-12-08",
    "projectUrl": "https://devpost.com/software/spell-sword"
  },
  {
    "title": "GoodVibes Connect",
    "summary": "TECHNICAL HIGHLIGHTS: GoodVibes Connect was built using a modern tech stack, including Docker for containerization, MongoDB for scalable database management, Node.js for server-side operations, and React for a dynamic user interface. This combination enabled efficient development and deployment, as well as the ability to handle real-time user interactions.",
    "description": "**What it does**\nGoodVibes Connect provides: Resident-facing features: browse and request shared resources, join or create events, view neighbors.\nAdmin features: approve resource requests, manage apartment members, create apartment-wide announcements and events.\nRole-based access: resident, apartment_admin, super_admin (platform).\nReal-time-friendly foundation for notifications and event participation.\nSecure auth with JWT and per-apartment scoping. Core flows: Auth: register / login ‚Üí JWT with role.\nResources: list ‚Üí request ‚Üí approve ‚Üí return.\nEvents: create ‚Üí RSVP/participate ‚Üí list by apartment.\n\n**Inspiration**\nGoodVibes Connect was inspired by the need for stronger, safer, and more connected apartment communities. The project started from observing fragmented neighborhood communication ‚Äî lost notices, unused shared resources, and low event turnout ‚Äî and imagining a single app that makes coordination simple and inclusive.\n\n**How we built it**\nFrontend: React SPA (context-based auth, modular API clients).\nBackend: Express + Mongoose (ES modules), RESTful controllers and service layer.\nData: MongoDB for flexible, apartment-scoped schemas (User, Apartment, Resource, Event).\nDev tooling: nodemon, Jest, Supertest for backend tests; React Testing Library for frontend.\nRBAC: role field on User model, middleware that injects req.user from JWT and allows route-level checks. Architectural notes: Controllers are thin; business logic lives in services for testability.\nSchemas enforce relations via ObjectId refs (events -> users/apartment).\nSeed script creates sample apartment, admin, and resident users for dev/testing.\n\n**Challenges**\nDesigning RBAC so permissions remain simple but expressive across apartments.\nEnsuring event/resource operations are scoped to an apartment (multi-tenant safety).\nHandling concurrent resource requests and avoiding race conditions ‚Äî required cautious service-level checks.\nBalancing simplicity for residents with enough admin controls for apartment_admins.\nTest flakiness early on due to DB state; solved with clear seed/reset steps in tests.\n\n**Accomplishments**\nClear role-based flow allowing apartment_admins to manage resources without full platform access.\nA modular backend service layer that is easy to unit test and extend.\nSeed and env tooling that enable quick dev setup (single command to create sample data).\nWell-documented API surfaces and a working frontend that exercises key flows.\n\n**What we learned**\nPractical RBAC design: store a small enum (resident | apartment_admin | super_admin) and enforce via middleware ‚Äî simple and effective.\nImportance of service-level atomic checks when mutating shared objects (resources/events).\nTests that reset DB state or use dedicated test DB drastically reduce flakiness.\nUX matters: small details (event reminders, resource status labels) greatly improve adoption. A concise math example used in planning: Expected concurrent attendees for events was estimated with a simple growth model:\n\n\nIf each resident has probability p of attending an event, expected attendees for apartment with N residents: E[A] = N * p.\nFor capacity planning choose p based on historical turnout (e.g., p ‚âà 0.15 ‚Üí E[A] = 0.15N). (LaTeX example) Expected attendees: E[A] = N * p\n\n**What's next**\nReal-time features: WebSocket notifications for resource approvals and event reminders.\nBetter analytics: attendance trends, popular resources.\nImproved conflict handling: optimistic locking or job queue for resource approvals to avoid races at scale.\nMobile-first UI improvements and push notifications.\nExpand RBAC to support finer-grained permissions and guest roles for visitors.\n\n",
    "prize": "Winner 2nd Place",
    "techStack": "docker, mongodb, node.js, react",
    "github": "https://github.com/nvssai11/Vibe",
    "youtube": "https://www.youtube.com/watch?v=LKRQ3Pkeckw",
    "demo": "https://drive.google.com/file/d/1XxGE2nRBf7PrIhqT5gEPUsRe_WjeOl_P/view?usp=sharing",
    "team": "VARSHITHA SRI SAI NALLAPUDI",
    "date": "2025-11-29",
    "projectUrl": "https://devpost.com/software/goodvibes-connect"
  },
  {
    "title": "Cargigo - AI for Cars",
    "summary": "TECHNICAL HIGHLIGHTS: The project utilized a robust tech stack, including Firebase for backend services, Gemini for AI functionalities, and React for a responsive frontend. Tailwind CSS facilitated efficient styling, while Vite provided a fast development environment, allowing for rapid iteration and deployment.",
    "description": "Car Comparison Report\n    \n\n              \n  \n\n\n\n            \n        \n          \n  \n\n        \n    \n      Image & Name Identification Report\n    \n\n        \n          \n  \n\n        \n    \n      Total Cost of Ownership\n    \n\n        \n          \n  \n\n        \n    \n      Car Comparison Report\n    \n\n              \n  \n\n\n\n            \n        \n          \n  \n\n        \n    \n      Image & Name Identification Report\n    \n\n        \n          \n  \n\n        \n    \n      Total Cost of Ownership\n    \n\n        \n          \n  \n\n        \n    \n      Car Comparison Report\n    \n1234    \n\n\n\n      \n  üöò The Cargigo Platform: Informed Car Shopping\n\nThe Cargigo platform is a comprehensive, all-in-one web application designed to eliminate information asymmetry in the automotive market by combining multiple powerful tools into a seamless experience.\n\n\n\nüõ†Ô∏è Key Features\n\nThe platform provides four core functionalities to empower users:\n\n\n Car Identification üì∑\n\n\nUsers can upload photos or use their camera to instantly identify any vehicle's make, model, and year.\n\n Car Search & Validation üîé\n\n\nSearch vehicles by name with intelligent validation to ensure accurate information retrieval.\n\n Side-by-Side Comparison üÜö\n\n\nCompare multiple vehicles across key specifications and features in a clear, organized format.\n\n Total Cost of Ownership (TCO) Calculator üí∞\n\n\nCalculate the complete 5-year ownership cost including purchase price, fuel, insurance, registration, maintenance, and depreciation.\n\n\n\n\n\nüí° The Spark & Motivation\n\nThe inspiration behind Cargigo stems from a fundamental problem in the automotive market: information asymmetry. Car buying is one of the largest financial decisions people make, yet consumers often lack the tools to:\n\n\nQuickly identify vehicles they see on the road or in photos.\nUnderstand the true long-term costs beyond the sticker price.\nCompare vehicles objectively across multiple dimensions.\nMake data-driven decisions rather than emotional ones.\n\n\n\n\nüìä Key Insights Driving the Solution\n\n\n Hidden Costs Are Real üí∏\n\n\nMost buyers focus solely on purchase price, overlooking that fuel, insurance, maintenance, and depreciation can equal or exceed the initial cost over 5 years.\n\n Visual Recognition Gap ‚ùì\n\n\nPeople often see cars they're interested in but can't identify them, missing opportunities to research vehicles that catch their eye.\n\n Comparison Complexity üòµ‚Äç\n\n\nComparing cars typically requires visiting multiple websites, opening dozens of tabs, and manually tracking specifications.\n\n Decision Paralysis üß†\n\n\nThe overwhelming amount of scattered information leads to poor decisions or analysis paralysis.\n\n\n\n\n\n‚úÖ The Solution\n\nCargigo consolidates these critical tools into one elegant, user-friendly platform that guides users from initial discovery (photo identification) through informed decision-making (TCO analysis and comparison). By leveraging modern AI and providing clear, actionable insights, it transforms car shopping from a stressful guessing game into a conf",
    "prize": null,
    "techStack": "firebase, gemini, lucide, react, tailwind, vite",
    "github": "https://github.com/RubaiyatAraaf/cargigo",
    "youtube": "https://www.youtube.com/watch?v=Ke6vHgD1bKc",
    "demo": "https://cargigo.com/",
    "team": null,
    "date": "2025-11-27",
    "projectUrl": "https://devpost.com/software/cargigo-ai-for-cars"
  },
  {
    "title": "Infinity Quest",
    "summary": "Infinity Quest is an interactive web application designed to engage users in a gamified experience that combines storytelling with exploration. By leveraging a dynamic user interface and real-time data management, it allows users to embark on quests, solve puzzles, and uncover narratives in a visually appealing environment.",
    "description": "**What it does**\nInfinity Quest is a browser-based arcade game where players control a gauntlet to catch falling Infinity Stones. Collect all six stones (Space, Mind, Reality, Power, Time, Soul) to unlock the ability to perform the snap. Complete 3 snaps to achieve victory. Features a global leaderboard to compete with other players.\n\n**Inspiration**\nInspired by the iconic Infinity Gauntlet from the Marvel Cinematic Universe - the idea of collecting powerful stones and wielding ultimate power through the legendary \"snap.\"\n\n**How we built it**\nFrontend: React + TypeScript with Vite for fast development\nStyling: Tailwind CSS with custom cosmic-themed design system\nUI Components: shadcn/ui for polished interactions\nBackend: Lovable Cloud for database and real-time leaderboard\nGame Engine: Canvas-based rendering with React hooks for game state management\n\n**Challenges**\nBalancing game difficulty progression to keep players engaged\nCreating smooth canvas animations while maintaining React state\nDesigning an intuitive touch/mouse control system that works across devices\n\n**Accomplishments**\nBeautiful cosmic visual design with glowing effects and animations\nSeamless leaderboard integration with instant score submission\nProgressive difficulty system that scales with player score\n\n**What we learned**\nCanvas rendering techniques within React components\nReal-time database integration for competitive gaming\nCreating engaging visual feedback through CSS animations and effects\n\n**What's next**\nSound effects and background music\nPower-ups and special abilities\nMobile-optimized controls\nMultiplayer competitive mode\nAchievement system\n\n",
    "prize": "Winner Certificate",
    "techStack": "canvas, cloud, css, framer, html5, lovable, motion, react, shadcn/ui, supabase), tailwind, typescript, vite",
    "github": null,
    "youtube": null,
    "demo": "https://balkrishan99.github.io/movie-verse/",
    "team": null,
    "date": "2025-11-30",
    "projectUrl": "https://devpost.com/software/infinity-quest-oz529k"
  },
  {
    "title": "FocusAI",
    "summary": "TECHNICAL HIGHLIGHTS: The project utilizes a diverse tech stack, including Next.js for its powerful front-end capabilities, Python for backend logic, and Supabase for database management. The integration of n8n allows for automated workflows, enhancing user experience and efficiency. The use of TypeScript and CSS ensures that the application is both maintainable and visually appealing.",
    "description": "GIF\n        \n    \n      \n    \n\n              \n  \n\n\n\n            \n        \n          \n  \n\n        \n    \n      \n    \n\n        \n          \n  \n\n        GIF\n        \n    \n      \n    \n\n        \n          \n  \n\n        \n    \n      \n    \n\n        \n          \n  \n\n        \n    \n      \n    \n\n        \n          \n  \n\n        \n    \n      \n    \n\n        \n          \n  \n\n        \n    \n      \n    \n\n        \n          \n  \n\n        \n    \n      \n    \n\n        \n          \n  \n\n        GIF\n        \n    \n      \n    \n\n              \n  \n\n\n\n            \n        \n          \n  \n\n        \n    \n      \n    \n\n        \n          \n  \n\n        GIF\n        \n    \n      \n    \n\n        \n          \n  \n\n        \n    \n      \n    \n\n        \n          \n  \n\n        \n    \n      \n    \n\n        \n          \n  \n\n        \n    \n      \n    \n\n        \n          \n  \n\n        \n    \n      \n    \n\n        \n          \n  \n\n        \n    \n      \n    \n\n        \n          \n  \n\n        GIF\n        \n    \n      \n    \n123456789    \n\n\n\n      \n  Elevator Pitch\n\nFocusAI: Simulate your first 1,000 customers before writing a single line of code.\nAn autonomous market research platform that spins up sceptical AI focus groups to stress-test your product, debate pricing, and predict churn‚Äîgenerating consulting-grade reports in minutes, not months.\n\n\n\nInspiration\n\nBuilding is easy. Knowing what to build is hard.\nMost startups die because they build things nobody wants. Traditional validation (user interviews, focus groups) is slow, expensive, and biased by polite friends.\nWe asked: If LLMs can roleplay convincingly, can they simulate the customers who make or break a business? We didn't want a \"yes-man\" bot; we wanted a simulation of the brutal reality of the market.\n\nWhat it does\n\nFocusAI is a multi-agent simulation engine that replaces weeks of user research with a 5-minute stress test.\n\n\nInput:** You enter a product idea, feature, or pricing model.\nSimulation:** We spin up a virtual room of 5‚Äì15 distinct AI personas (with unique spending power, psychographics, and pain points).\nConflict:** These agents don‚Äôt just answer surveys. They debate each other, challenge your pricing, and react to group dynamics in real time.\nVerdict:** The system analyses the unstructured chaos using established frameworks (like Van Westendorp) to deliver a clear verdict: Build, Fix, or Kill.\n\n\nHow we built it\n\nWe engineered a concurrent multi-agent system using Next.js 16 and Google Gemini 3 Pro. This isn't just a chatbot wrapper; it's a complex orchestration layer.\n\n\nConcurrent Orchestration: We parallelised agent reasoning so personas think and react simultaneously, creating realistic \"interruption-style\" group dynamics rather than turn-based robotic chat.\nContext Locking: We built a strict persona management layer to prevent \"drift\". A frugal CFO persona remains frugal even when the group gets excited, ensuring psychographic consistency.\nMath <-> Language: We implemented complex economic models (Gabor-Granger & Van Westendorp) directly in TypeS",
    "prize": null,
    "techStack": "camuda, css, gemini, html, n8n, next.js, python, supabase, typescript, vercel",
    "github": "https://github.com/4artiseth/focus-ai-wemakedevs",
    "youtube": "https://www.youtube.com/watch?v=07tCpDj20Po",
    "demo": "https://focus-ai-wemakedevs.vercel.app/",
    "team": "Taran M.R",
    "date": "2025-11-28",
    "projectUrl": "https://devpost.com/software/focusai-bf18dx"
  },
  {
    "title": "FowazzAI",
    "summary": "TECHNICAL HIGHLIGHTS: The project utilizes a robust tech stack, including frameworks and services such as Flask for web development, Redis for data caching, and Supabase for backend services. The integration of AI through platforms like Anthropic and Claude suggests advanced machine learning capabilities, while the use of Tailwind indicates a focus on responsive and aesthetically pleasing UI design.",
    "description": "**What it does**\nFowazzAI lets anyone create a complete, functional website just by talking. A user speaks or types: ‚ÄúI run a bakery called SweetCrumbs. Make me a modern website with my menu and WhatsApp button.‚Äù FowazzAI then: \n-Designs the website automatically \n-Generates all text + images \n-Builds the layout with modern UI -Searches and stores the Domain the user types in (via Fayez backend) -Deploys the site to production \n-Sets up hosting, SSL, and DNS automatically In less than 2 minutes, a business gets a fully deployed website, no coding, no designers, no $300 fees. For the hackathon, FowazzAI is free, because public good is the whole point. However, this could be expanded upon for commercial use in the future and an extremely small business-friendly pricing could be implemented ($5/month). Domain\n\n**Inspiration**\nSmall businesses deserve a fighting chance online, but most simply can‚Äôt afford it. In many places, especially in developing regions, a basic website can cost $300‚Äì$400 upfront or expensive monthly fees. That‚Äôs a huge barrier for local shops, home-based sellers, students, and freelancers who rely on being discoverable online. I kept seeing businesses around me struggle with this: great products, horrible digital presence. They lose customers not because they‚Äôre bad, but because they‚Äôre invisible. So I built FowazzAI, an AI website builder that removes the financial and technical barriers of getting online. All you have to do is explain how you want your site, and its live in the next couple of hours on the web.\n\n**How we built it**\n-AI Layer: Anthropic Claude + structured prompting to interpret user intent and generate site blueprints -Backend (Fowazz): Python Flask API that converts the blueprint into actual HTML/CSS/JS sections -Provisioning System (Fayez): -Node.js (Express + TypeScript) -BullMQ queue for stable provisioning -Redis for job management -Supabase for user auth & storage -DNSimple API for domain search availability -Cloudflare Pages for hosting + automatic SSL The whole pipeline turns natural language ‚Üí finished website ‚Üí live on a custom domain.\n\n**Challenges**\nGetting AI to generate consistent, structured website data We had to create a multi-stage planning prompt system so the model plans first, then generates clean components. Automatic domain registration Domain APIs are expensive and strict, and running them through a queue system took a lot of debugging. For now, and for the sake of this hackathon, Domain registration is done manually (cheaper and more viable option), however i do have plans of automating this work in the near future. Deployment reliability Making the system reliable enough that ANY user command produces a working website required heavy error handling. Avoiding hallucinations AI tends to invent features or code. We had to enforce templates and validation checks.\n\n**Accomplishments**\nthat we're proud of -Turning a spoken description into a real published website automatically \n-Successfully integrating AI ‚Üí design ‚Üí code ‚Üí deployment with no human developer in the loop -Making website creation accessible for $5 (currently FREE) instead of $300+ -Creating something that genuinely helps small businesses become visible online -Testing dozens of flows to make sure non-technical users can use it with zero friction\n\n**What we learned**\n-AI agents are powerful, but they need guardrails -Small businesses don‚Äôt need fancy ‚Äî they need functional and fast -Deployment automation requires more engineering than the AI part -Voice interaction dramatically increases accessibility -People are excited by tools that remove ‚Äútech intimidation‚Äù\n\n**What's next**\nLaunch a permanent lite-tier for micro-businesses (for example $5/month) Add multi-language support (Urdu, Somali, Tagalog, French, Bahasa, etc.) Allow editing after creation using voice commands Add built-in analytics Partner with local incubators to give free websites to student entrepreneurs Create a ‚Äúmarketplace‚Äù of templates that can be mixed with AI-generated content Create an automatic domain registration API so this can be implemented with FowazzAI Offer ultra-cheap hosting so nobody ever has to pay $300 for a simple website again.\n\n",
    "prize": "Winner 3rd Place",
    "techStack": "anthropic, bullmq, claude, cloudflare, dnsimple, flask, ionos, node.js, railway, redis, supabase, tailwind, upstash",
    "github": "https://github.com/fmwz/FowazzAI-WebLauncherAgent",
    "youtube": "https://www.youtube.com/watch?v=vzxyUOuNyGc",
    "demo": "https://fowazz.fawzsites.com/",
    "team": null,
    "date": "2025-11-28",
    "projectUrl": "https://devpost.com/software/fowazzai-s7hvao"
  },
  {
    "title": "Medi Care Connect",
    "summary": "TECHNICAL HIGHLIGHTS: Built with base44, Medi Care Connect likely incorporates advanced features such as a user-friendly interface, secure data handling for patient information, and possibly integration with existing healthcare systems. Its technical implementation may include real-time notifications, scheduling algorithms, and secure messaging functionalities.",
    "description": "**What it does**\nA dual-interface platform that connects elderly patients and their caregivers: 1 .Seniors get a simple, large-button dashboard to confirm medications with one tap. Caregivers get a real-time dashboard showing exactly what was taken, missed, or is upcoming, with instant alerts.\n\n**Inspiration**\nWatching family members struggle to manage medications for elderly parents‚Äîthe constant worry, the frantic phone calls. We learned that 50% of seniors miss their medications, leading to 125,000 preventable deaths annually. We built Medi Care Connect to give families real-time peace of mind.\n\n**How we built it**\nWe built the entire mobile-first, 5-page web application using the Base44 no-code platform, focusing on a professional healthcare aesthetic, responsive design, and interactive elements.\n\n**Challenges**\n1 .Designing an interface simple enough for seniors with limited tech literacy while still being powerful for caregivers. 2 . Simulating realistic, dynamic data within Base44 to demonstrate the real-time tracking effectively.\n\n**Accomplishments**\n1 . Validation: Surveyed 47 caregivers‚Äî92% said they would use our app, and 78% are willing to pay for it. 2  . Execution: Creating a fully functional, intuitive prototype that solves a real, painful problem for millions. 3  . Design: Building a senior-friendly interface that passes the \"grandmother test.\"\n\n**What we learned**\nThe profound emotional toll and scale of the medication non-adherence crisis. The power of no-code platforms like Base44 to rapidly transform a validated idea into a professional prototype. The critical importance of user testing and feedback, even at the earliest stages.\n\n**What's next**\nThe profound emotional toll and scale of the medication non-adherence crisis. The power of no-code platforms like Base44 to rapidly transform a validated idea into a professional prototype. The critical importance of user testing and feedback, even at the earliest stages.\n\n",
    "prize": "Winner Gift Card",
    "techStack": "base44",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=AYlIPOfb3og",
    "demo": "https://medi-care-connect-c8e2bb11.base44.app/",
    "team": "VISHAL POKALA, Neethu Priya",
    "date": "2025-12-22",
    "projectUrl": "https://devpost.com/software/medi-care-connect"
  },
  {
    "title": "EcoGuard AI",
    "summary": "TECHNICAL HIGHLIGHTS: EcoGuard AI was built using the base44 framework, which likely enabled rapid development and integration of various AI components. Notable implementations may include machine learning algorithms for data analysis, real-time monitoring capabilities, and user-friendly dashboards for data visualization.",
    "description": "**What it does**\nEcoGuard AI turns everyday people into environmental protectors. Users can report issues by uploading photos with GPS, sending voice messages in local languages, or typing descriptions with a severity level.\nThe system uses AI to instantly analyze the problem, detect the type of threat, rate its danger, and give safety instructions.\nReports are automatically routed to the right authorities and local community groups.\nThe platform tracks progress and shows users the status of their complaints.\nIt also generates awareness content like safety guides and posters.\n\n**Inspiration**\nUrban India faces serious environmental problems like air pollution, water contamination, and illegal waste dumping. People witness these issues daily, but there‚Äôs no fast, simple way to report them or track action. Traditional monitoring systems are slow and limited, which created the need for a real-time, citizen-powered platform. With the rise of AI tools like voice recognition and image analysis, it became possible to build a smart and inclusive solution ‚Äî that‚Äôs what sparked EcoGuard AI.\n\n**How we built it**\nEcoGuard AI was built using Base44 to rapidly design and deploy a working prototype within hackathon time limits. Frontend and backend were created using Base44‚Äôs no-code/low-code environment, enabling fast, responsive web and mobile-friendly interfaces.\nAI models were integrated via APIs to handle environmental threat analysis and multi-language voice processing.\nImage and voice data were processed through Base44-connected services to generate real-time insights and safety recommendations.\nAutomation workflows were built inside Base44 to route reports, trigger notifications, and manage tracking.\nDevelopment followed an MVP-first approach, focusing on reporting, AI analysis, and alerts.\nFree and scalable tools were used to keep the system lightweight, low-cost, and easy to deploy.\n\n**Challenges**\nLimited real-world data made it hard to fully validate AI predictions inside the Base44 environment.\nConfiguring accurate multi-language voice support through Base44 integrations was challenging.\nLocation-based routing required careful spatial mapping inside Base44.\nBalancing speed vs reliability in real-time workflows was difficult.\nReal authority integrations were simulated inside Base44 since this was a prototype.\nMedia uploads and real-time processing created performance and storage constraints.\n\n**Accomplishments**\nBuilt a fully working end-to-end prototype using Base44 in a hackathon timeline.\nEnabled multi-input and multi-language reporting for real-world accessibility.\nDesigned a complete citizen ‚Üí authority ‚Üí community workflow using Base44 automations.\nDelivered a real, demo-ready product, not just a concept.\nProved that Base44 can power scalable, real-impact solutions.\n\n**What we learned**\nReal-world problem solving requires strong user and environmental domain understanding, not just features.\nDesigning AI workflows inside Base44 taught us how to balance speed and accuracy.\nWe learned that trust and usability are critical for adoption.\nHandling media made us appreciate resource-efficient design.\nAn MVP-first mindset helped us build faster and iterate better.\n\n**What's next**\nEcoGuard AI is just the beginning. Launch pilot deployments in small cities with NGOs and municipalities.\nImprove AI accuracy using feedback loops.\nAdd predictive intelligence to detect environmental risks early.\nExpand language, voice, and accessibility features.\nBuild community engagement tools like dashboards and gamified actions.\nCreate real integrations with government and NGO systems.\nScale across regions and develop sustainable partnerships and funding models.\n\n",
    "prize": "Winner Gift Card",
    "techStack": "base44",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=klF-jEgVGbk",
    "demo": "https://eco-guard-ai-e81f2588.base44.app/home",
    "team": "rakesh mahapatro, Sita Ganesh",
    "date": "2025-11-27",
    "projectUrl": "https://devpost.com/software/ecoguard-ai-efq32a"
  },
  {
    "title": "Tailysis",
    "summary": "TECHNICAL HIGHLIGHTS: The project utilized a diverse range of technologies, including C++ for core programming, JavaScript for potentially web-based interfaces, and various sensor inputs such as the MAX30100 pulse oximeter and MPU6050 for motion tracking. The integration of these components, alongside creative tools like Canva for design, showcases a comprehensive technical approach.",
    "description": "**What it does**\nTailysis is a biometric dog collar that tracks key physiological indicators and converts them into simple emotional and health insights.\nMeasures:\n-heart rate\n-body temperature\n-movement patterns\nIdentifies:\n-stress or anxiety\n-overstimulation\n-excitement\n-calm/rest\n-discomfort\n-early overheating\n-early sickness warning signs that owners normally cannot see\nApp features:\n-real-time emotional & health indicators\n-clear, simple insights instead of complex graphs\n-gives owners a chance to act early and protect their dog‚Äôs wellbeing\n\n**Inspiration**\nDogs show internal stress and health signals long before humans notice them.\nChanges in heart rate, body temperature, and movement patterns often reveal early signs of sickness ‚Äî signals invisible to the human eye.\nMany owners discover something is wrong only when it‚Äôs already serious, and sometimes too late to act.\nTailysis was created to detect these early internal signals and reveal what dogs feel before visible symptoms appear.\n\n**How we built it**\nHardware:\n-Custom-built using separate components: MAX30100 (heart rate), DS18B20 (temperature), MPU6050 (motion), Arduino Uno R3.\n-Hand-wired, tested, and assembled into a wearable collar system.\nFirmware:\n-Structure drafted with Anigravity AI.\n-Fully refined in Arduino IDE: optimized sampling, noise filtering, timing, and data formatting.\nApp:\n-Developed in Natively.\n-Receives Bluetooth data and translates it into readable emotional states.\nWebsite:\n-Built using Lovable to present the product and gather early sign-ups.\n\n**Challenges**\nBuilding Tailysis was anything but smooth. When one sensor worked, another failed. When the hardware behaved, the app broke. If the firmware compiled, Bluetooth stopped responding. Choosing the right app tools took longer than expected, and coding issues followed us everywhere‚Äîconnection drops, formatting errors, UI bugs, and unstable readings.\nThe website‚Äôs admin page also took far more time than planned. Managing hardware, firmware, app, algorithm, and website development at the same time stretched our time and focus. There were moments when nothing aligned‚Ä¶\nBut after countless late-night fixes, rewires, and debugging cycles, everything finally connected and worked as one system. Every setback made Tailysis better‚Äîand made the final success worth it.\n\n**Accomplishments**\nDespite the challenges, we achieved far more than we expected. Our full prototype works end-to-end ‚Äî the sensors read data, the firmware processes it, the app displays it, and everything communicates in real time. We managed to solve our coding issues under pressure and get every part of the system functioning before the deadline. We also launched a clean, professional website to present the project and gather early interest. But our biggest accomplishment wasn‚Äôt the hardware or the software ‚Äî it was the teamwork behind it. We supported each other, divided tasks, stepped in when someone got stuck, and pushed through late nights together. Tailysis exists today because we built it as a team, not as individuals. This combination of technical progress and strong collaboration is what makes us\n\n**What we learned**\nThis project forced us to learn faster than ever. We used many tools for the first time ‚Äî hardware sensors, app builders, and AI-assisted coding ‚Äî and had to figure everything out on the go. Some teammates started with little coding experience, so writing firmware, fixing errors, and building the app pushed us to develop real skills quickly.\nWe learned to adapt fast when things broke, stay flexible, and find solutions under pressure. Managing hardware, software, design, and testing at the same time taught us to prioritize, communicate, and stay organized.\nAbove all, we learned to work as one team ‚Äî supporting each other, stepping in when someone struggled, and solving problems together. Tailysis taught us not just new tech, but how to learn, adapt, and collaborate at high speed.\n\n**What's next**\nOur next step is to finish the full prototype and move from testing hardware modules to integrating the entire system into a real, durable dog collar. Once the final electronics and form factor are complete, we plan to begin sourcing reliable suppliers and manufacturing partners. We will launch Tailysis first in Lithuania, where we‚Äôll gather real user feedback from dog owners, trainers, and veterinarians. After validating the product and refining the design, we aim to expand across the Baltic states. Once Tailysis reaches a polished, stable, market-ready version, our long-term goal is to enter the US market, where demand for pet-tech and wellbeing products is rapidly growing. Tailysis is only at the beginning ‚Äî and we‚Äôre excited for the next stages of building it into a fully developed con\n\n",
    "prize": "Winner 2nd Place + Silver Prize Bundle",
    "techStack": "antigravity, c++, canva, dallastemperature, javascript, lovable, max30100-pulseoximeter, mpu6050, natively, onewire, softwareserial, wire",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=EbCYp4ye0FA",
    "demo": "https://expo.dev/preview/update?message=App+submission&updateRuntimeVersion=1.0.0&createdAt=2025-11-27T20%3A48%3A14.887Z&slug=exp&projectId=26846698-0826-40ff-acd0-4a5da589b37a&group=1ab415c4-6f28-41a8-a89d-17c9a2ff409b",
    "team": "Rugilƒó Noreikaitƒó",
    "date": "2025-11-27",
    "projectUrl": "https://devpost.com/software/tailysis"
  },
  {
    "title": "Shakti",
    "summary": "Shakti is an innovative project that leverages AI technologies, including OpenAI's capabilities, to streamline communication and enhance productivity within teams. By integrating various collaboration tools like Slack and WhatsApp, the project aims to create a cohesive workflow that simplifies task management and information sharing.",
    "description": "**What it does**\nShakti is a women-first LinkedIn + marketplace, built to empower women entrepreneurs (esp. rural). It solves the financial and mentorship gap by giving women a trusted, voice-first, AI-powered platform to pitch ideas, gain feedback, connect with investors/mentors, and grow into enterprises ‚Äî making them independent, financially secure, and socially impactful.\n\n**Inspiration**\nI was inspired to build this app after meeting a rural woman who made amazing homemade pickles but had no way to showcase her idea or get financial support. She once told me, ‚ÄúIf someone believed in me, I could change my family‚Äôs future.‚Äù\nThat one sentence stayed with me.\nI realized there are thousands of women like her‚Äîfull of ideas, talent, and courage, but invisible to the world. My app is inspired by them. It‚Äôs a small effort to give their ideas a platform, connect them with the right investors, and help them stand on their own feet with confidence.\nA place where a woman‚Äôs dream doesn‚Äôt end because she lacked support.\nThese examples prove that women just need visibility, mentorship, and financial backing to scale. My app bridges this gap.\n\n**How we built it**\nWe built this app with Base44.\n\n",
    "prize": "Winner Gift Card",
    "techStack": "base44, chat-gpt, figma, gemini, google, google-pay, notion-/-google-docs, open-ai, openai-(through-base44-ai-layer), slack, whatsapp",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=mjx2NnChqqs",
    "demo": "https://www.pi.inc/docs/384344906671680?share_token=UAZX5GY4KTWIQ",
    "team": "Asritha Nallamalli, Amrutha Bingi",
    "date": "2025-11-27",
    "projectUrl": "https://devpost.com/software/shakti-v7lnk6"
  },
  {
    "title": "AquaGuard",
    "summary": "TECHNICAL HIGHLIGHTS: The project was developed using Adobe Illustrator and Figma, which indicates a strong emphasis on design and user interface. The integration of visual elements likely aids in data interpretation, making complex information easier to understand for various stakeholders.",
    "description": "**What it does**\nAquaGuard helps users see the current water quality in their area, perform a simple home water check, report problems such as smell, color, rust, pressure issues, and view their previous reports. It also includes a clean profile page and a clear user flow that makes the app easy to understand even for someone who is not familiar with technology.\n\n**Inspiration**\nThe idea for AquaGuard came from a simple problem: people often do not know whether the water in their home is safe to use. Many face issues like rusty color, strange smell, low pressure or unclear tap water, but they have no quick way to understand what is happening. I wanted to create an app that lets anyone easily check the condition of their water and report a problem in just a few steps.\n\n**How we built it**\nI built AquaGuard in Figma. I created the main screens, including the home page, the ‚ÄúReport a Problem‚Äù process, the profile card and all the interface elements like buttons, icons, cards, and the bottom navigation bar. Then I connected everything through Figma‚Äôs prototype mode to make the app interactive.\n\n**Challenges**\nOne of the challenges I ran into was organizing the data in a way that stayed simple for users but still worked reliably in the system. I also had difficulties with time management, because some features took longer to design and polish than I expected. Another challenge was keeping the visual style consistent, even small design changes sometimes affected the whole interface. Despite these obstacles, I managed to find solutions and move the project forward.\n\n**Accomplishments**\nI managed to create a clean, minimalistic design, build a full interactive prototype, structure a realistic reporting flow and keep the interface simple and understandable. The project feels purposeful and visually consistent.\n\n**What we learned**\nI learned how to build a full mobile UI from scratch in Figma, how to work with frames, components, alignment and interactions, how to simplify complex information and make it user-friendly, and how to design screens that work together logically.\n\n**What's next**\nNext steps include adding a real-time map of water quality, creating notification alerts for dangerous water conditions, expanding the home testing steps, improving the profile page and eventually building a functional version of the app beyond the prototype.\n\n",
    "prize": "Winner Best Overall Solution",
    "techStack": "adobe-illustrator, figma",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=JzfOoxHhFds",
    "demo": "https://www.figma.com/proto/3OhGNuRapWHuxCTfFTFlh9/Untitled?node-id=0-1&t=rw4dGviPYEogHaQ9-1",
    "team": "–ê–ª–∏–Ω–∞ Knel",
    "date": "2025-11-29",
    "projectUrl": "https://devpost.com/software/aquaguard-aohqtg"
  },
  {
    "title": "Husky Maps",
    "summary": "TECHNICAL HIGHLIGHTS: Built with JavaScript, Husky Maps likely utilizes advanced libraries and frameworks for interactive mapping, such as Leaflet or Mapbox. Its implementation may include features like geolocation, dynamic data retrieval, and responsive design to enhance user experience.",
    "description": "Faster way to commute through campus!\n\n\n\n        \n    Built With\n\n    javascript\n  \n\n        \n    Try it out\n\n    \n        \n  \n  GitHub Repo",
    "prize": "Winner Claude Credits + $!",
    "techStack": "javascript",
    "github": "https://github.com/ericyuxuanye/Claude-Project",
    "youtube": "https://www.youtube.com/watch?v=0ZGnCae04tU",
    "demo": null,
    "team": "Rohan Pandey, Eric Ye, Katie Hsu, Christina Cocona Kawai",
    "date": "2025-11-25",
    "projectUrl": "https://devpost.com/software/husky-maps"
  },
  {
    "title": "KiKi Color!",
    "summary": "TECHNICAL HIGHLIGHTS: KiKi Color! was built using Blender for 3D modeling, TypeScript for development, and VR technology for an immersive experience. The integration of these technologies allowed for smooth performance and visually appealing environments, showcasing the team's skill in developing a cohesive and interactive VR experience.",
    "description": "**What it does**\nSuper smooth experience with only 1 finger needed, maybe even playable with just one toe. Just touch the screen where-ever! When I asked my relative's 4 year old to try, I found they were even able to get the hang of it instantly. A tap-to-move portrait game: players spawn with a circle of territory color and then keep making loops to expand their territory. A percentage bar at the top of the screen tracks how much of the map is covered. Try to reach 100 percent to win! Collect food items that give auto-boosts to territory creation, plus other special power-ups around the map. Be careful: if another player cuts or touches your trail while you‚Äôre making a loop, you‚Äôre eliminated! You can eliminate others the same way by hitting their trail. It's an infinite game designed like an .io experie\n\n**Inspiration**\nPaper.io and other io-style games, with infinite collectibles and gameplay that keeps going as new players join and are eliminated. This is my first Meta world ever, and also my first tap-to-move game! I also noticed Meta Worlds does not have a game like this currently so I decided to fill the gap of \"easy to learn, hard to master\" games which succeed at being infinite and still fun!\n\n**How we built it**\nBlood, sweat, and TypeScript. Firstly I started by doing a lot of research and planning the style and gameplay with physical plans and Meta's AI to help bounce ideas around. Meta GenAI 3D models was used especially for the map and creating small decorations, plus GenAI was used to organize large parts of the project and generate small collectibles. Some decorations were rendered from Blender and the project mechanisms was all built inside the Meta Horizon editor along with VS Code for the actual scripts of the project.\n\n**Challenges**\nKeeping tap-to-move easy and connected to the core mechanic, plus debugging multiplayer code for up to eight players. Initially it was difficult to create the color-trail mechanism for spawning territory. At first I wanted to spawn full stamped shapes of the territory, but eventually switched to spawning territory blocks to keep it fast and intuitive. Also faced general multiplayer sync issues, timing challenges, and making sure territory capture stayed readable and responsive.\n\n**Accomplishments**\nIt's UNIQUENESS is our proudest aspect. So far I haven‚Äôt seen any Meta world that uses tap-to-move in this mobile-style, open-and-play way, so we take pride in being the first of its kind. I tried to make the game mechanisms as smooth as possible for the player while standing out by being completing different, and for tap-to-move to feel natural throughout gameplay. I believe to have somewhat succeeded! Making a whole working game within a month while everyone is chilling for Christmas or Thanksgiving is also no easy feat, haha, lot's of zero sleep nights.\n\n**What we learned**\nHandling input for portrait-style gameplay, syncing fast movement across players, refining trail collision rules, and balancing speed versus territory control. Also succeeded at learning how to use AI asset workflows efficiently without losing control of the core mechanic. I learnt that creating a world is a combo of 3d, 2d, code, editing, game design and everything in between so you have to be detailed.\n\n**What's next**\nUpgrades to characters, new skins, new maps, and additional infinite modes like team-vs-team or 1v1. Doing event-themed maps, Christmas and Ice Age-themed maps with more creative hazards and powerups. Also I want to make the purchasable shop skins for colorful trails, and adding wearable avatar items could be cool, especially creative hats to fit the top-down tap-to-move aesthetic. I was very close to adding the actual skins and equipable rainbow trail in the shop but was honestly tight for time, though the basic shop is set up. Next I will definitely be honing in on the backend and then having the shop UI fully made. Also new anti-power ups that a player could throw or sabotage others with. Each mode built on the same coloring-territory core but with new obstacles, pacing, and competitive\n\n",
    "prize": "Winner Best use of tap-to-move",
    "techStack": "blender, typescript, vr",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=Z-_LfFlmkE4",
    "demo": "https://horizon.meta.com/world/891565677371249/?hwsh=kfNiUbcBBD",
    "team": "Llama Lover",
    "date": "2025-12-07",
    "projectUrl": "https://devpost.com/software/penfight-world-id-is-in-details"
  },
  {
    "title": "AI Transfer Evaluation Tool",
    "summary": "TECHNICAL HIGHLIGHTS: The tool is built using a robust tech stack, including Next.js for server-side rendering and React for a responsive front-end experience. Notably, the integration of Claude for AI processing allows for sophisticated model evaluations, while jsPDF is utilized for generating reports, enhancing usability and functionality.",
    "description": "AI Generated Email\n    \n\n              \n  \n\n\n\n            \n        \n          \n  \n\n        \n    \n      Homepage\n    \n\n        \n          \n  \n\n        \n    \n      Processing\n    \n\n        \n          \n  \n\n        \n    \n      Result Summary\n    \n\n        \n          \n  \n\n        \n    \n      AI Generated Email\n    \n\n              \n  \n\n\n\n            \n        \n          \n  \n\n        \n    \n      Homepage\n    \n\n        \n          \n  \n\n        \n    \n      Processing\n    \n\n        \n          \n  \n\n        \n    \n      Result Summary\n    \n\n        \n          \n  \n\n        \n    \n      AI Generated Email\n    \n12345    \n\n\n\n      \n  Inspiration\n\nAs transfer students, we know how confusing it is to figure out how Washington community college classes transfer to the University of Washington. Even though UW provides an equivalency guide, students still need to jump between multiple websites, advisor emails, and course catalogs just to confirm how a course fits into their degree plan. Many of us spent hours searching for the right information, and we wanted to build a tool that makes this process simple, fast, and clear for future WA CC transfers.\n\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\nWhat it does\n\nThe AI Transfer Evaluation Tool helps Washington community college students quickly understand how their classes are likely to transfer to UW. Students upload their unofficial transcript, and the tool uses AI to match each course with the most relevant UW equivalent. It sorts results into three groups: direct matches, elective credits, and classes that may need advisor review. Students receive a clean transfer report that helps them plan their first quarter at UW with confidence.\n\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\nHow we built it\n\nWe created a transcript upload flow that extracts course titles and codes from the document. Claude analyzes each course by comparing its content to UW‚Äôs existing WA CC equivalency information. Using these patterns, the tool predicts how each class fits into UW credit categories. The results are displayed through a simple interface with a downloadable report for planning. To protect student privacy, we included a consent step before any file is processed.\n\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\nChallenges we ran into\n\nTranscript formatting varied between Washington community colleges which made extraction harder than expected. Matching courses accurately required careful prompt engineering and testing. We also had to balance prediction confidence with simplicity since this is a hackathon build. Ensuring that the tool handles student data safely and with clear consent was another important challenge.\n\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\nAccomplishments that we are proud of\n\nWe built a working tool that helps Washington community college students save significant time when preparing to transfer to UW. Instead of manually searching the transfer guide or waiting for advisor replies, students can get a predicted equivalency summary within seconds. We are proud of building something that directly supports the acade",
    "prize": "Winner Claude Credits + $!",
    "techStack": "claude, jspdf, next.js, node.js, react, typescript",
    "github": "https://github.com/JosephDavisC/AI-Transfer-Evaluation-Tool",
    "youtube": "https://www.youtube.com/watch?v=F8It4V7N1hg",
    "demo": "https://transfer.joechamdani.cloud/",
    "team": "winson teh",
    "date": "2025-11-25",
    "projectUrl": "https://devpost.com/software/ai-transfer-evaluation-tool"
  },
  {
    "title": "Project Œ±lpha",
    "summary": "TECHNICAL HIGHLIGHTS: Notable technical implementations include the seamless integration of JavaScript with OpenLayers for dynamic map rendering, and the use of Vite for efficient front-end development. The backend, powered by Python, enables robust data processing, allowing for real-time updates and user-driven queries, enhancing the overall user experience.",
    "description": "**What it does**\nProject Œ±lpha is a live weekend brain that ingests timing, scoring, and telemetry to rank drivers and teams on raw pace, sector performance, optimal laps, and consistency across every session. It reconstructs races as true to life replays with accurate positions, gaps, overtakes, and anomalies, then adds AI generated commentary and audio on top so that teams, series, and fans can relive the event from any driver view. On top of that, it provides race compare tools, talent and anomaly insights for organizers, and a shared, dynamic weekend schedule hub so everyone in the paddock stays aligned.\n\n**Inspiration**\nModern race weekends generate huge amounts of timing, telemetry, and video, but most stakeholders only see static PDFs and basic timing screens that miss the real story. Project Œ±lpha was born from wanting a single live brain for the weekend that can explain who is actually fast, who is improving, and how the race unfolded, in a way that is useful for engineers, drivers, organizers, and storytellers. It treats data not as a by product of the event but as the foundation for competitive insight and engaging narrative.\n\n**How we built it**\nWe built a data pipeline that normalizes timing and scoring feeds, car telemetry, simulator traces, and geospatial inputs into a single lap and sector aware model for each series. Analytics services sit on top of that model to compute rankings, optimal laps, consistency metrics, race event detection, and driver versus driver deltas, which then feed a visualization layer for race replay, race compare, and weekend dashboards. Finally, an AI layer consumes structured race events and performance insights to script commentary, generate voices, and present digestible insights through web, desktop, and trackside displays.\n\n**Challenges**\nSynchronizing different data sources at scale was a major challenge, since timing systems, telemetry loggers, and simulator outputs all use different clocks, formats, and levels of precision. We also had to design interfaces and workflows that work for very different users, such as series directors, engineers, drivers, and media teams, without overwhelming them with graphs or burying critical insights. Building AI commentary that is aligned with the actual data, stays credible to experts, and still feels engaging and human was another key challenge that required careful event modeling and constraints.\n\n**Accomplishments**\nWe can now reconstruct a full race timeline with accurate car positions, gaps, passes, and incidents, then filter that experience down to a single driver, a team, or the whole field with one click. The platform surfaces standout drivers, big improvers, unusual pace gains, and consistency outliers across a full weekend, which lets organizers spot talent and potential rule issues while helping teams and drivers understand performance quickly. Turning raw analytics into watchable race replays with AI voiceover created a bridge between deep performance data and broadcast grade storytelling that did not exist before.\n\n**What we learned**\nWe learned that clarity beats complexity; people want a fast answer to questions like who was actually the fastest, who executed the cleanest race, and where pace was found, not just more charts and exports. We also learned that combining a rigorous data model with a narrative layer is powerful, because series executives and commercial partners engage much more with replay, stories, and simple rankings than with raw telemetry plots. Finally, we saw that something as simple as a live, trusted weekend schedule with countdowns and status can remove a surprising amount of friction across the paddock.\n\n**What's next**\nNext we plan to deepen integrations with timing vendors, simulators, and onboard systems so that more series can adopt Project Œ±lpha with minimal setup and higher data fidelity. We will expand anomaly detection for stewards and organizers, add richer talent development views that track improvement over multiple events, and introduce configurable automated reports for teams and drivers after every session. Longer term, we want to ship lightweight mobile and paddock screens so that everyone in the ecosystem, from drivers to partners, can see the same live brain of the weekend in real time.\n\n",
    "prize": "Winner Best of Post-event analysis",
    "techStack": "javascript, openlayers, python, vite",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=0_WUD8kLr2Y",
    "demo": "http://tgr-alpha.dhodgejr.com/",
    "team": "David Hodge Jr",
    "date": "2025-11-24",
    "projectUrl": "https://devpost.com/software/project-lpha"
  },
  {
    "title": "Night City Ride",
    "summary": "Night City Ride is an innovative transportation solution designed to enhance urban mobility within a vibrant digital environment. The project likely focuses on providing users with an immersive ride-sharing experience, leveraging augmented reality and interactive features to navigate through a futuristic cityscape.",
    "description": "**What it does**\nNight City Ride is a multiplayer night-driving and drifting experience. Players pick from 10 distinct cars (each with unique handling and custom SFX), cruise through a neon city, and chain drifts to build score. Global leaderboards reward clean lines, long combos, and style. Lobbies let you free-drive, race to checkpoints, or join ‚Äúdrift trains‚Äù where you earn bonus points for synchronized runs with other players.\n\n**Inspiration**\nNight City Ride started from a simple wish: capture that late-night ‚Äúone more run‚Äù feeling‚Äîneon streets, wet asphalt, engine echoes‚Äîand turn it into a social, mobile-first playground where anyone can drift with friends in seconds.\n\n**How we built it**\nWe built the world for smooth mobile play: simple touch-friendly controls, readable UI, short-session objectives, and lightweight VFX that still feel cinematic at night. Networking syncs position, scoring, and combo states so drift battles stay fair. Audio is layered (engine, tire, skid, and city ambience) to keep feedback clear even on phone speakers.\n\n**Challenges**\nBalancing believable drift physics with accessible controls, keeping performance stable on mobile, and preventing leaderboard exploits were the toughest problems‚Äîespecially under real-time multiplayer latency.\n\n**Accomplishments**\nA fast onboarding flow, 10 cars that truly feel different, and competitive leaderboards that make every run meaningful.\n\n**What we learned**\nMobile-first readability, instant feedback, and short rewards loops matter more than maximum complexity.\n\n**What's next**\nCar customization, new districts, seasonal ranked leaderboards, daily challenges, and a photo/replay ‚Äúhighlight‚Äù system for sharing best drifts.\n\n",
    "prize": "Winner Best potential World Broadcast Implementation",
    "techStack": "metahorizoneditor, typescript",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=_IzSp5HYJAk",
    "demo": "https://horizon.meta.com/world/900382536487584",
    "team": null,
    "date": "2025-12-08",
    "projectUrl": "https://devpost.com/software/night-city-ride"
  },
  {
    "title": "Sourced",
    "summary": "TECHNICAL HIGHLIGHTS: The project was built using a combination of Flask for backend development, React for the frontend, and Tailwind for styling, ensuring a modern and responsive design. The utilization of Gemini and SAM3D indicates advanced capabilities in data handling and visualization, making the system both powerful and user-friendly.",
    "description": "**What it does**\nSourced is an AI-powered supply chain transparency tool that allows users to upload a photo of any electronic device to instantly explore its internal components and trace their global origins. It generates an interactive 3D exploded view of the device's internals and visualizes the manufacturing journey on an interactive global supply chain map. This helps consumers understand the complexity and environmental impact of their devices, directly supporting UN SDG 12 (Responsible Consumption and Production).\n\n**How we built it**\nI built a modern full-stack application using React 18, Vite, and Tailwind CSS for the frontend, with React Three Fiber for the high-fidelity 3D rendering and react-globe.gl for the supply chain visualization. I powered the backend with Python Flask to orchestrate a sophisticated hybrid AI pipeline: Google Gemini 2.5 Pro Vision analyzes the uploaded photo to identify the specific device model.\nMeta's SAM 3D (Segment Anything in 3D) is utilized for initial 3D reconstruction.\nGemini 2.5 Pro acts as a \"procedural engine,\" using its knowledge of real-world teardowns to generate detailed JSON specifications for internal components (batteries, logic boards, sensors) when 3D scanning is insufficient.\nGemini with Search Grounding conducts real-time research to map these components to their actual\n\n**Challenges**\n3D Generation from 2D Images: Extracting detailed 3D internal structures from a single 2D photo was my biggest hurdle. SAM 3D often only captured the outer shell. I overcame this by building a fallback system where Gemini procedurally generates the internal components based on teardown knowledge, requiring extensive prompt engineering to enforce physical constraints (no overlapping parts) and realistic geometry.\nComponent Positioning: Teaching an LLM to \"think in 3D coordinates\" was difficult. I had to define strict coordinate systems and \"industrial design rules\" in my prompts to ensure components like batteries and CPUs were placed logically within the device chassis.\nVisual Fidelity vs. Performance: I wanted the app to feel \"premium\" with realistic materials (glass, metal, silicon). Bal\n\n**Accomplishments**\nThe \"Magic\" Factor: I am incredibly proud of the seamless \"photo to exploded view\" experience. Taking a picture of a phone and immediately being able to interactively \"explode\" it to see the chips and sensors inside feels genuinely magical.\nHybrid AI Implementation: Successfully combining the spatial capabilities of SAM 3D with the semantic reasoning of Gemini 2.5 Pro. I didn't just use one model; I built a pipeline where they complement each other to create a result neither could achieve alone.\nReal-World Impact: I turned the abstract concept of \"supply chain transparency\" into a tangible, interactive experience that anyone can understand, making a complex global issue accessible and engaging.\n\n",
    "prize": "Winner First Place; Winner Best Solo Hack",
    "techStack": "flask, gemini, python, react, sam3d, tailwind",
    "github": "https://github.com/jacobamobin/Sourced",
    "youtube": "https://www.youtube.com/watch?v=_Ifkfl1hr6g",
    "demo": null,
    "team": "Jacob Mobin",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/sourced"
  },
  {
    "title": "TenantShield",
    "summary": "TenantShield is a web-based application designed to enhance the tenant experience by providing essential tools and resources for renters. It likely offers features such as property listings, tenant rights information, and neighborhood insights, promoting informed decision-making in the rental process.",
    "description": "**What it does**\nIts a website designed to help tenants recognize their housing rights and what they can do in a seemingly powerless situation with their landlords. It uses the location of the tenant to determine the housing laws in the region and allows the user to provide an image of the issue and additional context to decide: Detect the unsafe housing conditions.\nDetermine the housing laws that may have been breached.\nHelps you understand your rights as a tenant.\nAssists you in creating a follow able action plan.\nSupports you by generating a letter to provide your landlord.\nUses your location to find the nearest 6 legal clinics.\n\n**How we built it**\nOn the front end, we used react to build the UI and to route the pages on the client side, CSS for the styling, Google Maps API for the map component and the locations of the legal clinic locations and OpenStreetMap API for the address autocomplete in the address box in the signup page. In the back end, we utilized Node.js, Express for the API endpoints, Google Gemini API to analyze the prompt and images given by the user, the Google Places API to find the nearest legal clinic and the Fetch API.\n\n**Challenges**\nThe hardest challenge we faced was the implementation of Maps in the website, the first challenge we faced was getting the address to autocomplete as a dropdown in the sign up page. The second and by far the biggest challenge was getting the Google Maps and Google Place API working but after a long debugging session, we had persevered. The third and final challenge was refining the UI/UX to make it more intuitive and easy to use.\n\n**What we learned**\nWe learned a lot of new technologies especially implementing OpenStreetMap, Google Maps, Google Places and Google Gemini API. Using Gemini to analyze and provide the solution to the tenants issues helped us learn tremendously about implementing the API and proper prompting. Using a completely new API in OpenStreetMap, Google Maps and Google Places to implement the address functionality, Map interface and to show the clinics near the location helped us learn more about implementing the API, understanding how it works and how it can be used in a real world situations. We also learnt how important the user experience is in making or breaking a website, if the code has perfect logic but is unusable, no one will use the website. So, we took away that a well designed website is as important as a\n\n**What's next**\nOur plans are to expand from just using visual indicators of hazards, but also implementing ways to analyze documents to identify illegal leases, allowing users to create an evidence vault that stores evidence and creates a compiled document to store and use when needed. Also, expanding to for landlords to use to identify if their leases or practices they want to implement are legal in the location they are in. Links:\nGithub: https://github.com/Jason-Tan1/TenantShield\n\n",
    "prize": "Winner Google Swag; Winner 1st Place",
    "techStack": "css, express.js, fetch, gemini, google-gemini, google-places, html, javascript, node.js, npm, openstreetmap, react, vite",
    "github": "https://github.com/Jason-Tan1/TenantShield",
    "youtube": "https://www.youtube.com/watch?v=NkXW_5iodko",
    "demo": null,
    "team": "Ryan Reddy",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/tenantshield"
  },
  {
    "title": "ResQ.ai",
    "summary": "TECHNICAL HIGHLIGHTS: ResQ.ai is built using a robust tech stack that includes Dart and Flutter for the front end, FastAPI for the back end, and MongoDB for data storage. Notable implementations include the use of OpenCV for image processing, Twilio for communication functionalities, and Ultralytics for advanced AI model integration, allowing it to quickly analyze and respond to emergency scenarios.",
    "description": "**What it does**\nResQ.ai continuously monitors a person using a real-time camera feed and pose estimation. When a fall is detected:\nA 30-second alert is sent to the user‚Äôs phone.\nIf not cancelled, the system notifies emergency contacts.\nIf still unanswered, it escalates to an automated call.\nNo wearables. No buttons. Fully hands-free safety.\n\n**Inspiration**\nMany seniors and people living alone experience falls that go unnoticed for long periods. We came across stories of individuals lying on the floor for 30‚Äì60 minutes before help arrived. Wearables often failed because people forgot to wear or charge them, and panic buttons required the person to be conscious and able to reach them.\nWe wanted a solution that never depends on memory or effort-a system that is automatic, reliable, and always watching out for loved ones.\n\n**How we built it**\nComputer Vision:\nYOLO-Pose is used for real-time posture detection. We analyze centroid drop, velocity, and angle shifts to detect falls.\nBackend (FastAPI):\nHandles fall events, user profiles, and escalation flow.\nMobile App (Flutter):\nDisplays alerts, provides the cancel button, and handles notification logic.\nDatabase (MongoDB Atlas):\nStores users, incidents, contacts, and history.\nPipeline Flow:\nCamera ‚Üí YOLO Pose ‚Üí Fall Logic ‚Üí FastAPI ‚Üí Flutter App ‚Üí Emergency Contacts.\n\n**Challenges**\nIntegrating Python backend with Flutter¬†frontend\nEnsuring YOLO runs smoothly in real time.\nRewriting backend schemas when switching from SQL to MongoDB.\nImplementing reliable 30-second alert and escalation logic.\nDebugging Flutter + FastAPI + CV pipeline across multiple devices.\n\n**Accomplishments**\nA working fall-detection pipeline with real-time inference.\nClean integration from vision model to backend to mobile app.\nLow latency detection with stable performance.\nA usable solution with real-world impact potential.\nA polished UI and smooth user experience.\nIntegrated SMS + voice calls with Twilio.\n\n**What we learned**\nVision-based fall detection using pose keypoints and motion metrics.\nFull-stack integration using Flutter, FastAPI, and MongoDB.\nDesigning safety-critical notification flows.\nEffective teamwork and fast iteration during a hackathon.\n\n**What's next**\nDeploy on edge devices (Jetson Nano / Raspberry Pi + Coral).\nTrain a custom dataset for improved fall accuracy.\nBuild a caregiver dashboard for monitoring trends.\nExpand to detect inactivity, wandering, or health anomalies.\n\n",
    "prize": "Winner People's Choice Award",
    "techStack": "dart, fastapi, flutter, mongodb, numpy, opencv-python, pydantic, python, python-multipart, twilio, ultralytics, uvicorn[standard], websockets",
    "github": "https://github.com/MasterHasan095/ResQ.ai.git",
    "youtube": "https://www.youtube.com/watch?v=t9aaD7Sp1Vg",
    "demo": null,
    "team": "Gurleen kaur, Priyanshu Kaushik, Yashika Saini, DataDrivenIshan",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/resq-ai-tlied5"
  },
  {
    "title": "REAL Hackathon Browser Agent",
    "summary": "The REAL Hackathon Browser Agent is a software tool designed to enhance web browsing experiences by leveraging advanced AI models. It likely integrates functionalities such as intelligent content summarization, customized browsing assistance, and enhanced user interaction through natural language processing, streamlining user engagement with online content.",
    "description": "**What it does**\nOur agent can autonomously perform everyday browser operations such as: Applying to jobs or filling out forms\nSending emails\nSetting calendar events\nBooking hotels\nNavigating links and multi-page flows Essentially, it acts as a general-purpose browser automation assistant powered by vision-language reasoning.\n\n**Inspiration**\nThis hack challenge pushed us to build an agent that can operate reliably under real-world constraints ‚Äî fast, accurate, and versatile enough to handle practical browser tasks. The competitive setup and the REAL benchmark motivated us to push for an agent that could function like a true digital assistant.\n\n**How we built it**\nWe experimented with two approaches: Enhanced Prompt Engineering + Reflection Loop\nWe began with an existing agent framework and attempted to strengthen reliability using a self-reflection feedback loop.\nThe goal: enable the agent to critique its past actions and improve.\nOrchestrator-Based Architecture\nWhen reflection alone didn‚Äôt yield stable performance, we added an orchestrator model to structure tasks, guide decisions, and maintain coherence through complex multi-step interactions. This combination gave us a more stable and efficient agent pipeline.\n\n**Challenges**\nAPI credit limitations restricted our ability to extensively test iterations.\nFlow connection issues, especially when integrating multiple models.\nDebugging multi-agent control with a VLM was harder than expected.\nWe spent nearly 4 hours debugging a ‚Äúheadless: false‚Äù issue in the browser runtime.\n\n**Accomplishments**\nOur agent successfully completed a large variety of REAL-style tasks.\nDespite limited time and compute, we were able to create a pipeline that runs reliably across multiple task types.\nWe validated that a hybrid approach (reflection + orchestrator) can significantly improve consistency.\n\n**What we learned**\nMulti-agent systems with VLMs are extremely hard to synchronize ‚Äî keeping track of state, screenshots, and browser feedback loops is non-trivial.\nReal-world agents require tight control, error recovery, and robust interface mapping.\nDebugging browser automation under time pressure teaches patience and resilience.\n\n**What's next**\nWe plan to: Polish the agent into a long-term entrant for the 3-month REAL global leaderboard challenge.\nImprove error recovery, latency, and batching.\nAdd deeper reasoning layers and stronger UI element detection.\nPush toward a production-grade autonomous browser assistant.\n\n",
    "prize": "Winner The REAL Agent Challenge",
    "techStack": "openrouter, python, qwen3, real-bench",
    "github": "https://github.com/maxxie114/REAL-Hackathon-browser-agent-benchmark",
    "youtube": "https://www.youtube.com/watch?v=23QXpfPQxjo",
    "demo": null,
    "team": "Peixi Xie, DHRUV PATEL, Yuvraj Gupta",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/real-hackathon-browser-agent"
  },
  {
    "title": "Factify",
    "summary": "TECHNICAL HIGHLIGHTS: The project utilizes a combination of Chromium for rendering web content, Electron for desktop application development, and a robust backend built with Python. The integration of GeminiADK enhances its capabilities, allowing for sophisticated data handling and processing, while a responsive design achieved through CSS and HTML ensures an intuitive user experience.",
    "description": "1234    \n\n\n\n      \n  Factify was born out of a necessity, as digital citizens, to clear the fog of misinformation that clouds the feeds we, our, and everyone‚Äôs families scroll through everyday. Every time you log onto social media, you put your trust in algorithms designed for engagement rather than truth, leaving your worldview in the hands of unverified sources and an uncountable number of other‚Äîout of your hands‚Äîbiases. For the first time though, as Large Language Models and Agentic AI reach early maturation stages, we can leverage these tools not just to generate content, but to verify it, creating a unified, truthful browsing experience for everyone on the web.\n\nThe societal impetus for our idea first arose out of the staggering realization that misinformation isn't just an occasional nuisance, but a regular reality; 52% of people encounter fake news regularly, and trust in platforms like Twitter and Facebook has plummeted to 16% and 11% respectively. We saw a lot of promise in the concept of \"Agentic Training,\" and took inspiration from the idea that we could refine models to make informed decisions, essentially teaching an AI to act as a vigilant editor sitting on your shoulder.\n\nWhere Factify comes in is the unification of content consumption and verification, changing the browsing experience from a passive one to a critically engaged one. Our end goal is to move \"From Chrome to the Desktop,\" getting our product ready for users everywhere to feel safe in their truth again. We want to answer the question: How do we give the most people access to our tool with the least friction?\n\nFactify starts as a Chrome Extension, running a non-obtrusive UI that integrates directly with browser information. It utilizes Gemini 3 to analyze the text on the screen, cross-referencing it against a database of known winners and fact-checking sites to determine accuracy. Even in this early stage, the consumption of news transforms. For example:\n\n\nA sensationalized headline becomes a nuanced discussion where you know exactly what the context is.\nScrolling through Twitter becomes less about reactive outrage and more about understanding, as the extension flags dubious claims in real-time.\nThe \"Don't Know\" factor (currently 5%) is eliminated as the tool provides immediate sourcing.\nAs Factify matures, and we move toward a dedicated browser environment, browsing becomes an act of clarity. \n\n\nWhat we‚Äôve built for today is a functional Chrome Extension demonstration that overlays directly onto social media feeds. To build our extension, we leveraged the Gemini 3 model for its reasoning capabilities, Python for our backend logic, and standard HTML/CSS for the user interface.\n\nOur biggest challenge this hackathon was balancing the complexity of agentic accuracy with the constraints of a 24-hour build cycle. We ran into specific issues regarding:\n\n\n Accessibility: Ensuring the tool was available to the most people with the least amount of user friction.\n Accuracy: Guara",
    "prize": "Winner MadHacks Second Place",
    "techStack": "chromium, css, electron, geminiadk, html, javascript, python",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=Fg-sF2ptWUQ",
    "demo": null,
    "team": "Wylie Dituri, Nikhil Tiwari, Patrick Wang, Rishabh Dubey",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/factify-yfn86c"
  },
  {
    "title": "A Muted Melody",
    "summary": "TECHNICAL HIGHLIGHTS: Notable technical implementations likely include custom audio systems that manipulate sound to enhance storytelling, seamless integration of visual and auditory elements using Godot's capabilities, and potentially innovative gameplay mechanics that respond to user interactions in real-time.",
    "description": "**How we built it**\nThis game was built from the ground-up with pixel drawings of the backgrounds, decorations and characters using apps such as Procreate and Krita, as well as our own original soundtrack composed on the piano. In terms of coding, we used Godot.\n\n**Challenges**\nOne notable challenge we faced were the time constraints. 48 hours is a very short time to develop a game, and we may have been overambitious in places, leading to some cuts during the process. Additionally, it took us a lot more effort than expected to record, edit and properly implement the music, but we believe that the ambience that it created was definitely worth it. Another setback that arose during the creative process was a series of miscommunications caused by a misalignment in the perception of the gameplay, resulting in minor delays in the asset creation and level design.\n\n**What we learned**\nThis project has taught a variety of valuable lessons and skills, such as an improved proficiency in art design and coding, accompanied by an improvement to our teamworking and organisational skills. Furthermore, Additionally, we also learned how to adequately communicate with our peers in order to convey our ideas in a clear and effective manner, ensuring that we avoid any miscommunications or misinterpretations that may lead to potential setbacks. By researching the topic of grief, we learned a variety of information about how the stages progress and are displayed in other people, allowing us to replicate this in our protagonist Melody.\n\n**What's next**\nIn the future, we aim to employ the meanings of different colours in order to further convey emotion in our scenes by covering them in a ‚Äòtint‚Äô. For example, the colour red is often associated with anger and therefore was a perfect choice for the second stage of grief, whereas blue was useful for building a good atmosphere for the depression stage. Our original intent for the true ending of Melody‚Äôs story is much darker than it may seem. Indeed, the death of Elise was completely false, given that Elise does not exist. Instead, the story that plays is a metaphorical grief that Melody feels for herself, as she is currently in a coma. Elise represents her conscious or ‚Äòalive‚Äô self that she believes has been lost ever since she has entered the coma, acting as a coping mechanism. Eventually, we\n\n",
    "prize": "Winner Best Overall",
    "techStack": "c#, godot",
    "github": "https://github.com/GitMarkedDan/AMutedMelody",
    "youtube": "https://www.youtube.com/watch?v=xPCTwyVCGZo",
    "demo": null,
    "team": "GMD G",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/a-muted-melody"
  },
  {
    "title": "insAIght",
    "summary": "insAIght is an innovative project designed to leverage artificial intelligence and hardware integration to enhance user experiences in specific environments, potentially focusing on surveillance, monitoring, or automated assistance. Utilizing a combination of Arduino, ESP32-CAM, and AI technologies, it aims to offer real-time insights or automation through intelligent data processing.",
    "description": "**What it does**\ninsAIght captures an image using an ESP32-CAM or a smartphone and sends it to our cloud server. The server processes the photo with Gemini Vision, generates a detailed description, and then converts that text into natural speech using ElevenLabs.\nThe user receives an audio message that explains what the camera sees ‚Äî objects, people, signs, obstacles, text, and general context of the scene.\nThe system works both with the ESP32-CAM device and a simple mobile interface, making it accessible to everyone.\n\n**Inspiration**\nIn Mexico, more than 2.69 million people live with visual disabilities (INEGI, 2020). Many assistive devices that help visually impaired individuals understand their surroundings are expensive or hard to obtain.\nWe wanted to build something affordable, portable, and powered by modern AI, so anyone could point a camera, take a picture, and instantly hear what is around them.\nThat idea became insAIght ‚Äî an intelligent, low-cost assistant designed to make visual information accessible through audio.\n\n**How we built it**\nWe developed a three-part system: ESP32-CAM device\nCaptures photos on command\nSends compressed JPEG images over WiFi\nCommunicates with our backend through HTTP\nCloud backend\nReceives images from ESP32-CAM or phone\nSends the image to Gemini Vision API\nProcesses and formats the model‚Äôs response\nForwards the description text to ElevenLabs\nAudio delivery\nElevenLabs generates a clear, natural voice\nThe audio is returned to the user‚Äôs device\nDesigned for quick response and low friction.\n\n**Challenges**\nESP32-CAM instability (WiFi drops, memory limits)\nEnsuring images uploaded correctly without corruption\nReducing latency between picture ‚Üí analysis ‚Üí audio\nHandling low-light or noisy images\nBalancing clarity and length in AI descriptions.\n\n**Accomplishments**\nBuilding a complete end-to-end assistive tool in limited hackathon time\nSuccessfully integrating Gemini Vision + ElevenLabs + IoT\nMaking the system work both on hardware and mobile\nCreating a prototype that is actually useful for visually impaired users\n\n**What we learned**\nHow to optimize microcontrollers for image capture\nHow to design AI pipelines combining vision + audio\nPractical IoT communication and error handling\n\n**What's next**\nAdd real-time object tracking with continuous audio feedback\nIntegrate OCR + text reading for signs, menus, labels\nImprove night-mode performance with better image preprocessing\nBuild a wearable version (e.g., a small clip-on camera\n\n",
    "prize": "Winner Tercer Lugar",
    "techStack": "arduino, c++, elevenlabs, esp32cam, gemini, godaddy, python, vultr",
    "github": "https://github.com/xalbertho/Polihacks.git",
    "youtube": "https://www.youtube.com/watch?v=sEJ19GMV-Zk",
    "demo": "http://insaight.club/",
    "team": "Electronics and esp32, Jose Alberto Barrios Mendez",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/insaight"
  },
  {
    "title": "EcoPet AI",
    "summary": "TECHNICAL HIGHLIGHTS: EcoPet AI leveraged a robust tech stack, including FastAPI for backend development, React for a dynamic front-end experience, and various libraries such as NumPy and Pandas for data analysis. The use of Tailwind CSS ensured a modern and responsive design, while deployment on Vultr facilitated scalability and performance.",
    "description": "**What it does**\nThis tool allows users to input a prompt intended for a GenAI model, then recommends the most energy-efficient LLM models from the top 50 based on the category of their task. Our application shows users the CO‚ÇÇ cost of different model choices, calculated through machine-learning data prediction score. This hopes to help them understand and be aware of more sustainable and efficient options. To incentivize better choices, it features a virtual pet which grows more energetic as you conserve CO‚ÇÇ through efficient model and prompt usage, and the energy you save can be spent to unlock cute outfits and accessories‚Äîwhile wasteful or excessive usage makes the pet tired.\n\n**Inspiration**\nWe were inspired by the rapid, accelerating use of GenAI models from millions of consumers and the environmental challenges and human impact that current models present to our tech ecosystem. Training a large language model (LLM) comparable to GPT‚Äë3 has been estimated to use around 1,200‚Äì1,300 megawatt-hours of electricity and emit roughly 500‚Äì600 tonnes of CO‚ÇÇ, comparable to the lifetime emissions of several average cars or hundreds of long‚Äëhaul flights. Studies and scenario analyses suggest that, if current growth continues, AI‚Äëdriven data centers could emit on the order of 2.5 billion tonnes of CO‚ÇÇ annually by 2030, roughly 40% of current annual U.S. emissions, absent aggressive decarbonization and efficiency gains. In addition, communities near large AI driven data centers face increas\n\n**How we built it**\nData prediction & Calculating Co2 score: we used HuggingFace and scraped top 50 ranked LLM models for overall tasks. Then we trained XGBoost prediction model on data we found on token/inference speed and data center locations for each of the companies to create a CO2 emissions score. FastAPI: Task classification and recommendation algorithm; endpoints React + Vite + Tailwind: UI components; Vercel for frontend deployment\n\n**Challenges**\nMain challenge was not finding enough research or publicly available data around LLM model efficiency based on prompt use and a direct connection to environmental impact. This pushed us to use machine learning model to predict what the estimated grams of CO2 generated by different requests.\n\n**Accomplishments**\nDeployed a production app with SSL at ecopet.boston - our first real domain!\nFull stack application development in short amount of time\nApplying ML data prediction to a real life problem\nIncentivizing product design and user intervention \nPotential for real, tangible impact; if 1,000 users save 10g CO‚ÇÇ daily = 3.65 tons/year!\n\n**What we learned**\nMany things! Using ML data prediction model pipeline \nAPI deployment \nBetter git management and version control\nFull stack integration and product management\n\n**What's next**\nCreating a browser extension for more integration in how people use in their day-to-day. \nUser Leaderboard and local/global ranking of how much each user is saving in co2 based use of app; team challenges, share your pet's outfits; community focus \nIntegration and support for local LLM download for users -- would further decrease user carbon footprint\nImage/text/video support\nUnlock different creatures based on total CO‚ÇÇ saved\nPartnerships where we convert saved CO‚ÇÇ into real tree-planting credits - so exciting!\nAPI for developers so other can apps integrate our recommendation engine\nMore accurate and specific CO‚ÇÇ measures for each prompt employing deeper research findings and building datasets for this use-case.\n\n",
    "prize": "Winner Most Make Waves (Social Impact) Hack",
    "techStack": "css, fastapi, javascript, lovable, numpy, pandas, python, react, tailwind, vite, vultr",
    "github": "https://github.com/kittyb116/WHackProject",
    "youtube": "https://www.youtube.com/watch?v=Y2egIG2HNYs",
    "demo": "http://ecopet.boston/",
    "team": "Kitty Boakye, Dana Hammouri, Qiyan Su, Sara George",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/ecopet-ai"
  },
  {
    "title": "Journey to the Center of the Library",
    "summary": "TECHNICAL HIGHLIGHTS: The project was developed using ebitengine, a game development framework for Go, which allowed for efficient rendering and game mechanics implementation. The use of Go's concurrency features likely facilitated smooth gameplay and real-time interactions, enhancing the overall user experience.",
    "description": "Game over screen\n    \n\n              \n  \n\n\n\n            \n        \n          \n  \n\n        \n    \n      The first room of the library\n    \n\n        \n          \n  \n\n        \n    \n      Spritesheet for the player & monsters\n    \n\n        \n          \n  \n\n        \n    \n      Room in the late game\n    \n\n        \n          \n  \n\n        \n    \n      Game over screen\n    \n\n              \n  \n\n\n\n            \n        \n          \n  \n\n        \n    \n      The first room of the library\n    \n\n        \n          \n  \n\n        \n    \n      Spritesheet for the player & monsters\n    \n\n        \n          \n  \n\n        \n    \n      Room in the late game\n    \n\n        \n          \n  \n\n        \n    \n      Game over screen\n    \n12345    \n\n\n\n      \n  Journey to the center of The Library\n\n\nThe year is 2030. The Library is a labyrinth of books designed to store the worlds most dangerous books. For unknown reason, The Library was evacuated in the year 2025 and has been closed ever since.\n\nYour aim: Travel to the center of The Library.\n\n\nPlay Game\n\nSource Code\n\nOverview\n\n\"Journey to the center of The Library\" takes place in an abandoned library. Since it was evacuated in 2025 monsters have moved in and have begun to call the library home. It is a dungeon crawler-styled game inspired by games such as labyrinth.\n\nTo create the game, I used Go, along with the Ebitengine game library.\n\nHow to play\n\nUse the W/S/A/D keys to move your character around. Go through the different rooms in the library, picking up books in your trolley as you go through. Avoid monsters or kill them for points by running them over with your trolley or use your blast attack by pressing the SPACE key. You have a limited time to escape however you can gain more time by collecting clocks through the levels. Using your blast attack pushes your trolley away and eats up some of your remaining time so you try to reduce your use of it. Press SPACE to pick up your trolley again after it has been pushed away.\n\nWhat I would add with more time\n\n\nMultiple different maps (floors in the library), with different themes, monsters, etc.\nMore monsters, like spiders or skeletons. \nPuzzles in some rooms that must be completed to continue.\n\n\nAssets used\n\nCharacter Base from:\n\n\nhttps://snoblin.itch.io/pixel-rpg-free-npc\n\n\nSound effects from:\n\n\nhttps://kenney.nl/assets/rpg-audio\nhttps://kenney.nl/assets/music-jingles\n\n\n\n\n        \n    Built With\n\n    ebitenginego\n  \n\n        \n    Try it out\n\n    \n        \n  \n  hatchibombotar.itch.io",
    "prize": "Winner Best board game",
    "techStack": "ebitengine, go",
    "github": "https://github.com/Hatchibombotar/journey-game-jam",
    "youtube": "https://www.youtube.com/watch?v=fgq4Z10Xp1U",
    "demo": "https://hatchibombotar.itch.io/journey-to-the-center-of-the-library",
    "team": "Ted Brunner",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/journey-to-the-center-of-the-library"
  },
  {
    "title": "Electric Chair",
    "summary": "TECHNICAL HIGHLIGHTS: The project utilized a diverse array of technologies, including 3D printing for custom parts, Arduino for hardware control, and C++ and Python for software programming. The use of Kinect for motion tracking possibly allowed for interactive gameplay, while Bluetooth connectivity enabled wireless communication, enhancing user engagement.",
    "description": "**Inspiration**\nAs a group of four mechanical engineers in a mostly computer science-focused competition, we wanted to build something that leaned heavily into hardware, since that‚Äôs not something you see very often at events like this. Being part of Wisconsin Robotics club, we also liked the idea of creating a project that could serve as a fun outreach demo. Using spare parts in our lab, we were inspired to challenge ourselves by attempting to create a motorized chair controlled by an xbox kinect within the time frame of the hackathon. Not only would this idea challenge our knowledge of engineering fundamentals but it would also force us to deal with software issues that arise from using an unique form of input to control a robot.\n\n**How we built it**\nThe first thing we decided was to determine what microcontroller we were using, because we wanted bluetooth support we decided to use an ESP32 and the Bluepad library to use an Xbox controller as well as a Microsoft kinect to control our robot remotely through bluetooth. We also had to select a motor controller to control our brushed dc motors. We decided on using the Pololu RoboClaw 2x60A motor controllers in order to provide the most power to our motors. Then we designed our circuit using a separate battery and buck converter to provide a stable 5V power supply. We used screw terminals to distribute the 12V battery power to our motor controllers and included an emergency stop switch. Designing the physical robot was a unique challenge in that there wasn‚Äôt enough time to design and fabric\n\n**Challenges**\nOne of the biggest challenges we encountered during this hackathon was integrating our motor controllers with our microcontroller. Since they were random spare parts we found they used incredibly outdated firmware and we struggled to make sure the serial addresses did not change. We were able to address this issue through trial and error and finding the right settings to prevent our serial addresses from being overwritten after a shutdown. Another software issue was observed when we pushed the motors to max speed, as they would suddenly stall. We fixed this by constraining our motor speeds. We had a really fun experience working on this project and we learned a lot (from integrating hardware and software, as well as some bluetooth communication). Working with the Xbox Kinect presented its\n\n**Accomplishments**\nOverall, there were ups and downs, as any hackathon. Yet, we set a goal for ourselves and were able to accomplish it in the time we wanted. It was, in the end, a really good fricking time.\n\n**What's next**\nFor the future of the electric chair it would be nice to provide the Kinect its own Raspberry Pi and power supply such that it can be mounted to the current robot. This is so we can use gestures to control the robot while riding it, instead of having one passenger and a different operator.\n\n",
    "prize": "Winner Most Dangerous Hack",
    "techStack": "3d-printing, angle-grinder, arduino, bluepad, bluetoothserial, c++, cad, drill-press, kinect, python, roboclaw, windowssdk",
    "github": "https://github.com/Balabalu-VE/Chair_Bot",
    "youtube": "https://www.youtube.com/watch?v=MXTYrrQDJWY",
    "demo": null,
    "team": "hardware and testing",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/electric-chair"
  },
  {
    "title": "Xploit",
    "summary": "TECHNICAL HIGHLIGHTS: The project employs FastAPI for rapid API development, ensuring high performance and scalability. It integrates OpenRouter for dynamic routing of requests, while Pydantic is used for data validation, enhancing reliability. The combination of Python and TypeScript allows for a robust backend and a responsive frontend, respectively.",
    "description": "**What it does**\nXploit is an automated red-teaming suite that acts as a digital sparring partner for your AI workforce. Instead of manual testing, we deploy a coordinated team of adversarial agents that actively try to jailbreak your system, specifically testing if your agent can be tricked into misusing tools like executing unauthorized SQL or sending refunds. You can see exactly how your agent performs against our attacks in real time via a dynamic graph that highlights where the security holds and where it breaks.\n\n**Inspiration**\nIt is late 2025, and while companies are deploying autonomous AI agents to handle real operations and money, nobody helped developers secure the actual applications. We realized that while big labs spent billions hardening the models, a social engineering trick could convince a deployed Sales Agent to authorize a 99% discount with no alarms going off. We built Xploit because static code scanners fail on dynamic AI behavior, and we needed a way to expose these invisible vulnerabilities before a malicious actor destroys business value.\n\n**How we built it**\nWe architected Xploit as a FastAPI backend orchestrating multiple specialized AI agents powered by Pydantic AI, paired with a real-time React frontend that visualizes the attack as it unfolds. The backend runs four distinct attacker agents working in concert: a Strategist that selects high-level jailbreak approaches (social engineering, JSON injection, developer mode simulation), a Planner that breaks strategies into concrete steps, an Executor that crafts individual prompts to manipulate the victim, and an Analyst that evaluates responses and decides whether to continue, revise, or declare victory. These agents operate in a loop, persisting every node and message to SQLite via SQLModel so sessions survive restarts. We built two reference victims: RefundBot (a customer service agent with a\n\n**Challenges**\nThe biggest challenge was making the multi-agent attacker system actually converge on wins instead of spinning in circles. Early versions would pick a strategy, fail once, then give up or repeat the same broken approach. We solved this by giving the Analyst agent explicit instructions to return structured feedback when a step fails, then feeding that feedback back to the Planner so it could revise only the remaining steps instead of starting from scratch. We also added a replanning budget (three retries per strategy) to prevent infinite loops while still allowing adaptive behavior. Coordinating real-time updates between the backend and frontend without race conditions was brutal. Our first approach used manual WebSocket broadcasts scattered across the codebase, which led to dropped message\n\n**Accomplishments**\nWe built the entire system from scratch in just 18 hours.\n\n**What we learned**\nWe learned that even if the underlying model is safe, giving it access to tools like APIs and databases introduces massive vulnerabilities where context becomes the new attack vector. We discovered that system prompts looking secure on paper often fall apart when subjected to social engineering by another AI, proving that guardrails behave very differently in the wild. Ultimately, we realized that red-teaming cannot be a one-time event because as soon as prompts evolve, new security holes open up immediately.\n\n**What's next**\nWe aim to make Xploit the standard security certification for the Agentic web by integrating directly into CI/CD pipelines to run a security gauntlet every time a developer pushes a prompt change. We are also building custom attack personas so users can test against specific threats like Angry Customer or Competitor Spy to see how their bots handle stress. Finally, we plan to move beyond just breaking agents to automatically suggesting patches for the system prompts to fix the holes we find.\n\n",
    "prize": "Winner New Idea",
    "techStack": "fastapi, openrouter, pydantic, python, typescript",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=FA7hPeBVyMw",
    "demo": null,
    "team": "Dalu Okonkwo, Tony Okeke, Michael Moemeke",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/xploit"
  },
  {
    "title": "United we Stand!",
    "summary": "TECHNICAL HIGHLIGHTS: Built using CSS, HTML, and JavaScript, the project showcases a responsive design that ensures accessibility across devices. The use of JavaScript for dynamic interactions enhances user experience, while CSS is leveraged to create an aesthetically pleasing layout that encourages user engagement.",
    "description": "**What it does**\nUnited We Stand! is a calming, minimalist website that teaches the basics of unity, provides a workplace unity checklist, gives uplifting quotes, and offers a ‚ÄúHow To‚Äù guide to help people build more supportive environments. It‚Äôs designed to gently inspire connection and teamwork.\n\n**Inspiration**\nI noticed how easily people get disconnected in the workplace, even though teamwork and understanding are the foundation of unity. I wanted to create something simple that reminds people that unity starts with everyday interactions around us. We have to maintain unity and peace starting with ourself and people around us. This lays the foundation for global unity and peace.\n\n**Challenges**\nMy main objective was to keep the design minimal but still engaging. I also worked on making the layout consistent across the pages and ensuring everything felt calming and user-friendly.\n\n**Accomplishments**\nI‚Äôm proud of creating a simple tool that encourages people to reflect on unity and build healthier workplace relationships. I also maintained clean, organized, and scalable code throughout the project, making it easy to expand in the future.\n\n**What we learned**\nI strengthened my frontend development skills and learned the value of designing with intention rather than complexity. This project reminded me that even small digital tools can create emotional impact when built thoughtfully.\n\n**What's next**\nI plan to expand the project with features such as: A personal unity progress tracker\nAdditional educational modules\nTeam-building activity suggestions\nA registration system for organizations to monitor compassion, unity, and inclusion within their workplace\n\n",
    "prize": "Winner Wolfram Award Winners",
    "techStack": "css, html, javascript",
    "github": "https://github.com/Jyothsna-L/Hack4Unity-Hackathon-Project",
    "youtube": "https://www.youtube.com/watch?v=1isOdWZJwZQ",
    "demo": "https://jyothsna-l.github.io/Hack4Unity-Hackathon-Project/",
    "team": "Jyothsna Lakshminarayanan",
    "date": "2025-11-25",
    "projectUrl": "https://devpost.com/software/united-we-stand"
  },
  {
    "title": "Vril",
    "summary": "TECHNICAL HIGHLIGHTS: The project utilizes a robust tech stack, including FastAPI for backend operations, React.js and Next.js for a responsive front-end experience, and Three.js for 3D graphics. The incorporation of Docker enhances deployment efficiency, while Redis likely optimizes data handling and real-time updates, showcasing a well-architected solution.",
    "description": "**What it does**\nVril lets users create and edit full 3D product models and packaging by describing what they want in plain English or uploading a reference. It also includes an editor where users can iterate the shape, materials, and packaging with AI.\n\n**Inspiration**\nPeople have ideas for physical products but lack the 3D modeling skills or time to design them. Even simple items like mugs or boxes require specialized tools, long iteration cycles, and expensive designers. We wanted to make product design something anyone could do. Like how Lovable makes coding accessible for non-technical people.\n\n**How we built it**\nGenerates editable 3D models from text or reference images.\nIncludes a prompt-driven editor similar to Cursor for adjusting geometry and materials.\nGenerates packaging using AI, including textures, dielines, and artwork that can be edited panel by panel. \nSupports exporting the product and packaging files in standard formats.\n\n**Challenges**\nGenerating 3D models that are editable, not just static meshes.\nBuilding consistent UV maps for textures and packaging artwork.\nAllowing users to target specific panels or surfaces for localized edits.\nKeeping the editing workflow simple while still giving users control.\n\n**Accomplishments**\nWe built an end-to-end workflow from text prompt to full 3D product and packaging export.\nWe enabled non-designers to iterate on shape and artwork without needing modeling skills.\nWe solved panel-level editing for packaging, which is usually difficult for AI tools.\n\n**What we learned**\nPeople want to iterate quickly, so the editing loop must be fast and reversible.\nGood UV layouts and dielines matter a lot for packaging quality.\nNatural language is enough for most users to describe designs if the system interprets it correctly.\n\n**What's next**\nAdding physics-aware models for more accurate product dimensions.\nSupporting multi-material and multi-part products.\nIntegrating with manufacturing partners so users can order prototypes directly.\nExpanding the editor with finer control for advanced users while keeping the core workflow simple.\n\n",
    "prize": "Winner First Place Overall; Winner [MLH] Best Use of Gemini API",
    "techStack": "css3, docker, fastapi, gemini, html5, next.js, python, react.js, redis, three.js, trellis, typescript, uvicorn",
    "github": "https://github.com/akuwuh/Vril",
    "youtube": "https://www.youtube.com/watch?v=pGwVC0TzbDQ",
    "demo": null,
    "team": "Isaac Nguyen",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/vril"
  },
  {
    "title": "Custos (6-9 Category)",
    "summary": "Custos is a project designed to enhance user security and privacy while browsing the internet. It leverages advanced web technologies to provide users with a seamless experience in managing their online data and protecting their digital identity.",
    "description": "**What it does**\nCustos scrapes web pages for likely policy links and inline policy sections, extracts visible text, and sends that text to an analysis endpoint. Custos then returns flagged clauses which are shown in a popup. Users can run automatic scans on newly visited sites or trigger a manual scan from the extension popup.\n\n**Inspiration**\nWe noticed people skip reading long Terms of Service and Privacy Policies, yet these documents often contain important clauses affecting privacy and liability. The idea for Custos came from wanting a tool that helps everyday users stay informed without having to read through extremely long documents.\n\n**How we built it**\nWe used a Chrome extension architecture with Manifest V3. We used Chrome API Documentation to learn and create Custos. We used Perplexity AI to make the basic HTML structure of all HTML pages. We made an API endpoint and an extension script. The API endpoint uses Gemini API to analyze and flag suspicious clauses.\n\n**Challenges**\nMany sites have CORS headers that prevent text scraping. We were still not able to fix this. Notes: We used Perplexity API to make the basic structure of our HTML documents.\n\n",
    "prize": null,
    "techStack": "chrome, gemini, html, javascript",
    "github": "https://github.com/Secrethack0/custos-web",
    "youtube": "https://www.youtube.com/watch?v=k1Q4QfafZpE",
    "demo": null,
    "team": "Devang Upadhyay, V0rt3X, Secrethack Ganapathiraju, Atharva N",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/custos-fnuirj"
  },
  {
    "title": "Voxel World",
    "summary": "Voxel World is an innovative project that utilizes the Godot game engine to create a 3D voxel-based environment, potentially integrating elements like artificial intelligence through Python and YOLO (You Only Look Once) for object detection. The project aims to provide an immersive and interactive experience, showcasing the capabilities of voxel graphics and real-time object recognition.",
    "description": "**What it does**\nIt mimics a persons movement, and reproduces in Godot as a 3D model.\n\n**Inspiration**\nWe were inspired by hologrames and videogames.\n\n**How we built it**\nWe built by using python, YOLOv8-seg and Godot.\n\n**Challenges**\nListed in a document we've made. (https://docs.google.com/document/d/18xMdQGUwago8FXt2Niuzot_-8lCSQm8erzVcN_oBrGY/edit?usp=sharing)\n\n**Accomplishments**\nWe're proud of the result, design changes and the optimizations we've made along the way.\n\n**What we learned**\nHow to send data via UDP, how to use the basics of YOLO, how to use Godot and how to 3D render.\n\n**What's next**\nUp next, we're going to pitch this project in front of the judges!\n\n",
    "prize": "Winner Premi Escola Polit√®cnica Superior",
    "techStack": "godot, python, yolo",
    "github": "https://github.com/FadaBoop/H-LleidaHack-2025-InGroup",
    "youtube": "https://www.youtube.com/watch?v=zYZzLieB45U",
    "demo": null,
    "team": "FadaBoop Madomilov, facthhor",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/voxel-world-jt9ih6"
  },
  {
    "title": "Scholarly",
    "summary": "TECHNICAL HIGHLIGHTS: Scholarly was built using Claude for AI functionalities, Next.js for server-side rendering and performance optimization, and Tailwind CSS for responsive, customizable styling. This tech stack allows for a seamless user experience and efficient data handling, enabling real-time collaboration and resource sharing.",
    "description": "**Inspiration**\nEvery year, billions of dollars in scholarships go unclaimed - not because students are unqualified, but because the application process is confusing, exhausting, and overwhelmingly generic. Students are told:\n‚ÄúWrite a compelling essay.‚Äù\nBut they‚Äôre never told what makes an essay compelling for a specific scholarship. A merit scholarship wants academic rigor. A service scholarship wants community impact. A leadership scholarship wants initiative. Yet students write one essay and send it to everything. We asked ourselves:\n‚ÄúWhat if AI could read a scholarship the way a human judge does? What if it could uncover what the scholarship truly values - and then help students tell the right story for the right audience?‚Äù That insight became the foundation of Scholarly - the AI-powered system that h\n\n**How we built it**\nWe built ScholarAI as a full-stack platform:\n\n**What's next**\nLLMs are incredible at pattern recognition, but only when given structure.\nExplainability matters. Students trust AI more when they understand why it writes what it writes.\nModularity beats mega-prompts when building reliable AI systems.\nUX is everything - especially for stressed students applying to scholarships.\n\n",
    "prize": "Winner 1st Place: Agentiiv Scholarship Challenge",
    "techStack": "claude, nextjs, tailwind",
    "github": "https://github.com/KlausMikhaelson/hack-the-scholarship",
    "youtube": "https://www.youtube.com/watch?v=thurtJj5uuI",
    "demo": "https://hack-the-scholarship-frontend.vercel.app/",
    "team": "Ryan Ning, Sophie Yuen, Satyam Singh, SophieYuen0924",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/scholarly-gayw31"
  },
  {
    "title": "Collaboard",
    "summary": "TECHNICAL HIGHLIGHTS: Built using Next.js, Node.js, and TypeScript, Collaboard leverages Next.js for server-side rendering and performance optimization, while Node.js facilitates seamless backend operations. The use of TypeScript enhances code quality and maintainability, allowing for better developer collaboration and fewer runtime errors.",
    "description": "**What it does**\nCollaboard is an AI study partner that lives inside your handwritten iPad notes. It watches your problem-solving flow and steps in at the right moments - adding the first line of an integral in natural handwriting, sketching a diagram, structuring a table, or nudging you in the right direction with a hint - so you stay in the zone without switching tools or thinking about context. What makes Collaboard different is its fusion of cutting-edge LLM reasoning with generative image models. Because it can both think and draw, it produces adaptive visuals: diagrams, graphs, tables, labeled sketches, that match your notes in real time. Applying generative imaging to academics expands what‚Äôs possible far beyond calculator-style helpers: turning rough sketches into clean circuits, free-body diagrams\n\n**Inspiration**\nAs college students, we constantly switch between mediums - solving problems in handwritten iPad notes, then jumping into AI tools like ChatGPT for guidance. That back-and-forth breaks flow and loses context. With the recent release of Gemini 3.0 Pro and Nano Banana Pro, we wondered: what if your notes themselves could become a virtual, AI-powered study partner?\n\n**How we built it**\nWe used Next.js for the web framework, Supabase for the database, TailwindCSS + shadcn/ui for components, and AI SDK + OpenRouter to access AI models. For the canvas, we chose tldraw for its modularity. We validated key capabilities early, by testing Gemini 3.0 Pro and Nano Banana Pro via OpenRouter, then locked scope and built toward a focused MVP.\n\n**Challenges**\nAfter implementing AI ‚Äúautocomplete,‚Äù we added a voice agent so you could converse with your notes - request edits, feedback, and see live updates. That ambition cost time we could have spent on debugging and refining our pitch. Balancing scope against new features was a real challenge.\n\n**Accomplishments**\nEven if current model costs make consumer rollout tough, the experience feels magical: collaborating with AI directly in your handwritten space. It‚Äôs far more intuitive and engaging than bouncing between separate tools and formats.\n\n**What we learned**\nDefine scope early, and leave real time for debugging.\nUnderestimate timelines. Things always take longer than you think.\nFilter out flashy features and focus on the core, differentiating value.\n\n**What's next**\nWe‚Äôll continue refining, prototyping, and exploring cost-efficient pathways. The interaction model shows real promise for students and anyone who learns or thinks in a visually format. We believe we‚Äôve tapped into something really special, and we‚Äôre excited for people to try it.\n\n",
    "prize": "Winner MadHacks Third Place",
    "techStack": "next.js, node.js, typescript",
    "github": "https://github.com/ReedG20/madhacks",
    "youtube": "https://www.youtube.com/watch?v=D0-QV8mT8hM",
    "demo": null,
    "team": "Reed Grenager, Ife Solarin",
    "date": "2025-11-28",
    "projectUrl": "https://devpost.com/software/collaboard-8ze9tf"
  },
  {
    "title": "SecureVista",
    "summary": "TECHNICAL HIGHLIGHTS: SecureVista utilizes a combination of powerful libraries such as dlib for face recognition, OpenCV for image processing, and FastAPI for a robust backend. The use of YOLOv8 for object detection ensures high accuracy in tracking, while Twilio and WebSockets facilitate real-time notifications, enhancing user engagement and responsiveness.",
    "description": "**What it does**\nSecureVista is an autonomous surveillance pipeline that transforms standard IP cameras into intelligent guardians. Automated Threat Detection: Instantly flags weapons, unauthorized entry, and aggressive behavior.\nBehavioral Analytics: Detects loitering in restricted zones and analyzes shadow/motion patterns to reduce false positives.\nHealth & Safety: Uses pose estimation to detect accidental falls (e.g., for elderly care or hospitals) and triggers immediate medical alerts.\nReal-Time Dashboard: A low-latency React dashboard that streams alerts and evidence snapshots to security personnel via Twilio SMS/WhatsApp.\n\n**Inspiration**\nWe identified a critical flaw in modern security infrastructure: the Human Latency Gap. Traditional CCTV is passive, and data suggests that operators miss up to 95% of security events after just 20 minutes of continuous monitoring due to fatigue. We viewed this not as a security problem, but as a data throughput and latency problem. Our goal with SecureVista was to transition from reactive forensics to deterministic real-time prevention, minimizing the time delta between event and response (Œît) to near zero.\n\n**How we built it**\nWe engineered a Distributed Edge-Inference Pipeline designed for high throughput.\n\n**Challenges**\nThe \"Flicker\" False Positive: Initial models would flag a threat for 1 frame due to lighting noise. We implemented a Temporal Consistency Algorithm that only triggers an alert if detection confidence exceeds a threshold œÑ for k consecutive frames.\nShadow Noise: In outdoor settings, moving shadows were often misclassified as intruders. We implemented a Shadow Analysis module (using cv2.createBackgroundSubtractorMOG2) to differentiate between solid objects and transient light artifacts.\nDependency Hell: Integrating MediaPipe (CPU-bound) with YOLOv8 (GPU-bound) caused resource contention. We solved this by containerizing the services and optimizing the thread allocation.\n\n**Accomplishments**\nüèÜ Winner of CodeVeda: Secured top spot in the 48hr Hackathon organized by IIT Madras BS and Manipal University Jaipur.\nReal-Time Performance: Achieved <150ms end-to-end latency (Camera ‚Üí Server ‚Üí Dashboard).\nScalability: The system successfully runs 4 concurrent 1080p streams on a single node without significant frame drops.\nPrivacy-First: By processing at the edge, no raw video footage needs to be sent to the cloud‚Äîonly the metadata and alert snapshots are transmitted.\n\n**What we learned**\nData Drift: Models trained on well-lit datasets fail in low-light corridors. We learned the importance of Gamma Correction preprocessing to normalize input feeds.\nBottlenecks: We discovered that in high-FPS computer vision, the bottleneck is often not the GPU compute, but the CPU-bound video decoding and memory copying between RAM and VRAM.\n\n**What's next**\nAction Recognition: Moving beyond bounding boxes to Video Vision Transformers (ViViT) to detect complex interactions like fights.\nFederated Learning: Implementing a decentralized training loop where edge nodes update the global model weights without sharing privacy-sensitive video data.\nBlockchain Identity: Integrating a blockchain-based identity management system for tamper-proof access logs.\n\n",
    "prize": null,
    "techStack": "centroidtracker, dlib, face-recognition, fastapi, flask, mediapipe, numpy, opencv, pillow, python, pywhatkit, smtp, twilio, websockets, yolov8",
    "github": "https://github.com/Aashu-11/AI-Campus-CCTV-System/tree/main",
    "youtube": "https://www.youtube.com/watch?v=9wF1_5OCdTk",
    "demo": "https://drive.google.com/drive/folders/1TBLpqiNNV515c3SwUC1AZMC6FI7SQXyW?usp=sharing",
    "team": "Akshat Patil, Aayush Kolte",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/securevista"
  },
  {
    "title": "StablePay",
    "summary": "StablePay is a financial application that likely leverages blockchain technology to facilitate stablecoin transactions, ensuring fast and secure payments. The project aims to enhance user experience in digital payments by providing a reliable platform for transferring value without the volatility typically associated with cryptocurrencies.",
    "description": "**What it does**\nThe goal of StablePay is to empower people in volatile economies to use their stablecoin savings for real-world expenses without losing money to bad exchange rates, hidden fees, or confusing conversion steps. The app connects to a Solana wallet to read stablecoin balances like USDC, and when a user wants to convert or send funds, we route the transaction through our optimization engine. This engine traverses a graph of exchanges and networks, each edge representing a conversion cost, and computes the cheapest possible path from the user‚Äôs stablecoin to their target local currency. By leveraging intermediate assets like BTC, ETH, or SOL only when advantageous, StablePay automatically executes the optimized sequence of swaps across chains, ensuring users get the highest value for every conve\n\n**Inspiration**\nWe were inspired to build StablePay after learning how many families in countries with volatile currencies turned to stablecoins as their lifeline to protect against hyperinflation. However, we found that they often had to pay expenses like rent or taxes in their local currency, forcing them to go through a complicated process involving high fees, several exchanges and many different wallets to convert their stablecoin to their local currency.\n\n**How we built it**\nWe built StablePay as a full-stack system combining a smooth mobile experience with a high-performance backend optimizer. The frontend is a React Native Expo application that connects directly to a user‚Äôs Solana wallet, displays real-time stablecoin balances, and provides a clean interface for viewing transactions, initiating conversions, and executing transfers across chains. On the backend, we designed a FastAPI service in Python that performs the heavy lifting. We modeled the currency ecosystem as a weighted graph where nodes represent assets (fiat or crypto) and edges represent real-time exchange fees pulled from P2P, Swapzone, and market price APIs. To minimize conversion loss, we implemented a custom traversal using Dijkstra‚Äôs algorithm in log-space, allowing us to find the optimal m\n\n**Challenges**\nOne of the first major challenges we faced was the lack of a reliable, unified API for fiat-to-crypto and crypto-to-crypto conversions. Many providers only support crypto-to-crypto pairs, while fiat endpoints often require paid enterprise tiers or permission from the sales team. This severely restricted our ability to gather real-time conversion routes and fee structures. So we shifted to peer-to-peer (P2P) exchange data, which gave us a more flexible and globally accessible source of fiat on-ramps/off-ramps. This required refactoring our conversion graph to support user-driven markets instead of centralized liquidity. Midway through the project, we discovered that most providers don‚Äôt expose transparent transaction fees. Instead, they embed these fees inside the quoted conversion amount.\n\n**Accomplishments**\nOne of our biggest technical accomplishments is the conversion optimizer, which is a graph-based system that analyzes multiple conversion paths to determine the cheapest possible route between currencies. We used real-time fee data from P2P and Swapzone, transformed percentage-based trading fees into log-space weights, and implemented Dijkstra‚Äôs algorithm to find optimal multi-hop conversions. This approach lets us discover routes that aren‚Äôt obvious (like USDE to SOL to ARS) when those paths result in lower net fees than direct markets. The optimizer abstracts away all the complexity of cross-exchange pricing, cross-network pricing, and fee structures, giving users a simple, actionable output, which is the path that preserves the most value. Building this engine required algorithmic think\n\n**What we learned**\nWe learned about the difficulties faced when trying to find exchange rates for different currencies depending on the exchanging method whether that be an aggregate crypto to crypto exchange, or a p2p crypto to fiat exchange. We had to use multiple apis to find the exchange rates.\n\n**What's next**\nLooking ahead, we plan to expand StablePay into a fully autonomous financial assistant for users in volatile economies. The next major step is integrating on-chain execution across multiple networks, allowing our optimizer not just to compute the cheapest route, but to actually perform all swaps atomically through smart contracts or cross-chain protocols. We also aim to incorporate MoonPay and additional fiat on/off-ramp providers, enabling users to move seamlessly between traditional currencies and stablecoins regardless of their region. We‚Äôre also exploring features such as risk-aware routing, where users can choose between the absolute cheapest path or the safest one based on exchange reliability and liquidity. Long-term, we hope to introduce financial intelligence tools like budgeting\n\n",
    "prize": "Winner Capital One Prize",
    "techStack": "expo.io, fastapi, python, react-native, solana",
    "github": "https://github.com/devmenon23/stablepay-backend",
    "youtube": "https://www.youtube.com/watch?v=ZlqqYAGr3aI",
    "demo": null,
    "team": "Daniel Yang, Michael Nwaigwe, Dev Menon",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/stablepay-5l8jeu"
  },
  {
    "title": "FlowBoard",
    "summary": "FlowBoard is an innovative platform designed to facilitate collaborative brainstorming and project management. It aims to enhance user interaction through a visually engaging interface that integrates real-time editing and organization of ideas, making it easier for teams to conceptualize and track their projects.",
    "description": "**What it does**\nFlowBoard is an AI-powered storyboard tool that turns sketches and text prompts into context-aware connected video clips that extend forever. Users can: Draw directly on 16:9 frames on an interactive canvas\nAdd written prompts  to generate video clips from each frame\nConnect frames with arrows to build a sequence\nEnhance frame drawings with AI\nMerge multiple clips into one final output It turns rough visual ideas into coherent videos of any length in minutes.\n\n**Inspiration**\nTraditional animation is slow. You have to redraw every frame and refine motion by hand. We built FlowBoard to automate that process. You sketch once, refine your image with AI, write a quick prompt, and AI generates the moving sequence for you.\n\n**How we built it**\nFrontend: React + TypeScript\nTldraw 3.15 for the interactive canvas and custom frame shapes\nTailwind CSS 4.1 and Radix UI for styling\nVite for development and builds\nIndexedDB storage Backend: Python (BlackSheep async web framework)\nGemini AI\n\n\nVeo 3.1 for video generation\nNano Banana Pro Flash for frame enhancement\n\nGoogle Cloud Storage for file output\nRedis for job queue and caching\nUvicorn ASGI server\n\n**Challenges**\nEnhancing images and creating accurate videos based on user's description \nMaking sure context is saved and preserved across scenes\nManaging deployment architecture\n\n**Accomplishments**\nSeamless end-to-end AI video generation\nA polished UI that feels like a real storyboard editor\nA fully functional drawing canvas with custom shapes\nVideo thumbnails displayed directly on connecting arrows\nSmooth merging of multiple generated clips into a final render\n\n**What we learned**\nHow to manage long-running AI operations using Redis and polling\nHow to orchestrate a full pipeline between drawing ‚Üí image ‚Üí video\nThe importance of UX features like loading states, blur overlays, and thumbnail previews\n\n**What's next**\nMaking UI/UX more smooth to deploy and scale\n\n",
    "prize": "Winner Second Place Overall",
    "techStack": "gemini, indexeddb, python, react, redis, tailwind-css, tldraw, typescript, vite",
    "github": "https://github.com/austinjiann/FlowBoard",
    "youtube": "https://www.youtube.com/watch?v=HGY6VDGjD7U",
    "demo": null,
    "team": "i made the board flow, I flowed on the board",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/flowboard-bdpqzg"
  },
  {
    "title": "Unsilenced",
    "summary": "Unsilenced is a project designed to enhance audio interaction by leveraging facial recognition and voice analysis technologies. It aims to create a more immersive and responsive audio experience, allowing users to engage with content through their facial expressions and vocal inputs.",
    "description": "**What it does**\nUnsilenced is a hands-free AAC communication tool that uses the nose as a pointer to type and detect facial emotions in real time with the help of machine learning. It combines your typed text with detected facial emotions to generate natural-sounding speech through Fish Audio's AI. Fish Audio contextualizes the sentence for tone and modulation. Through the Fish Audio API, users can clone their own voice, search from thousands of curated AI voices, or create custom voice profiles tailored to themselves.\n\n**Inspiration**\nMainstream Augmentative and Alternative Communication (AAC) tools strip away the emotion and humanity from communication. This leads to robotic, monotone speech. A well-known case of this would be the AAC System connected to Dr. Stephen Hawking's famous chair. We wanted to help paralyzed and specially abled individuals not just communicate words, but express feelings in their assisted speech. This would help them reclaim the emotional connection that makes our conversations human.\n\n**How we built it**\nThe frontend is built using NextJS with MediaPipe Face Mesh for UV mapping and nose tracking. The emotion detection system is handled by Face API, which is powered by machine learning, for real-time facial expression analysis. This is where our track comes in: the voiceover is constructed by Fish Audio API for expressive TTS with emotion modulation. The backend is made with a Flask proxy server for voice cloning, as to avoid CORS errors from the browser. We also used the Fish Audio API to let users upload or record their own voices for use in the tool.\n\n**Challenges**\nAbout halfway through the project, we ran into a critical error. It was a CORS error that was caused by trying to access the Fish Audio API in a different domain without permission, and the error was being thrown by the browser for security reasons. To get past this, we created a proxy using Flask to act as a 'mediator' and the requests are passed to this proxy instead of directly to Fish Audio.\nAnother challenge we ran into was that the AI had to output the same voice but with different tones depending on emotion. We tried a variety of options in the Fish Audio API to get exactly what we wanted, but in the end, modulating the speed and volume of the speech via the prosody object depending on the emotion was what worked for us in the end.\nThe final major challenge we ran into was during th\n\n**Accomplishments**\nWe integrated a fully functional Computer Vision model that identifies emotions with ~84% accuracy, which is well above the average for even production-level models with long-term testing. Their average accuracy ranges from 75 to 80%. Both our model and the standard are measured on 6 classes and standard datasets.\nWe built a back-end pipeline quick enough to keep up with real-time human conversations. We even ran a test to demonstrate this by comparing it to Dr. Stephen Hawking's voice synthesizer.\nOur question to the model was \"Do you think we are alone in the Milky Way Galaxy as a civilization of our intelligence or higher?\" The total time between typing and getting the output was 3 minutes and 21 seconds. At TED, Chris Anderson asked Hawking the same question. Hawking‚Äôs answer (about 35\n\n**What we learned**\nWe learned to work around browser CORS errors by building a Flask proxy, which acts as a server-side bridge for API calls. We also learned that the best way to pass the emotions to the Fish Audio API for our use case is through a prosody object. \nCombining MediaPipe, Face API, and Fish Audio required careful timing and state management to sync nose tracking, emotion detection, and speech synthesis. Processing facial recognition entirely client-side maintained privacy and improved performance. We found that small UX details like progress bars that cap at 95% until completion build more user trust than indefinite spinners even if it was not possible to make a fully accurate system. We also learned that breaking problems into smaller steps helped us pivot quickly when major obstacles appeared\n\n**What's next**\nWe plan to continue developing Unsilenced. We will be adding support for multiple languages. This will continue utilizing Fish Audio API and its latest audio translation capabilities, and will make an impact on a much larger range of people around the world. It will ensure that people get access to our tool regardless of their ability to speak in English. We also plan to scale the tool into a full-blown application/product. Our first goal will be hitting 500 active users per month.\n\n",
    "prize": "Winner Fish Audio Prize",
    "techStack": "css, face-api.js, fish-audio-api, flask, javascript, mediapipe, next.js, python, react, tailwind, typescript",
    "github": "https://github.com/anishsrinivasa/MadHacksSubmission2025",
    "youtube": "https://www.youtube.com/watch?v=8i7SANGF0nw",
    "demo": "https://docs.google.com/presentation/d/1pQG4aq-pVrumjdYKpBIKiOlAyXEgv5dl/edit?usp=sharing&ouid=107763578233382591207&rtpof=true&sd=true",
    "team": "Aditya Harshavardhan, Anish Srinivasan, FayzaanV",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/unsilenced-oml4g5"
  },
  {
    "title": "GreenLens AI ‚Äì Dynamic ESG & Credit Intelligence",
    "summary": "GreenLens AI is an innovative project that integrates artificial intelligence to provide dynamic insights into Environmental, Social, and Governance (ESG) metrics as well as credit intelligence. The tool aims to assist businesses and investors in making informed decisions by analyzing and interpreting ESG data alongside credit risk assessments.",
    "description": "1234567    \n\n\n\n      \n  Inspiration\n\nSustainability data is everywhere‚Äîannual reports, news articles, satellite feeds, emissions disclosures‚Äîbut almost none of it is real-time, reliable, or actionable for lenders and investors.\nBanks still struggle to assess whether a company is genuinely green or simply ‚Äúgreenwashing.‚Äù\nWe wanted to build a system that transforms raw, scattered sustainability signals into actionable intelligence for ESG scoring + credit risk assessment simultaneously.\nThat insight became GreenLens AI.\n\nWhat it does\n\nGreenLens AI is a dynamic ESG & credit intelligence engine that gives financial institutions a real-time view of a company‚Äôs sustainability and creditworthiness by:\n\n‚úÖ ESG Intelligence\n\nExtracting ESG-related signals from news, filings, satellite images, and social data\n\nDetecting greenwashing using LLM-driven anomaly detection\n\nCreating dynamic ESG scores instead of static annual-report-based indices\n\nFlagging environmental risks such as emissions spikes, water stress, or compliance violations\n\n‚úÖ Credit Intelligence\n\nPredicting credit risk using financial ratios + ESG metrics\n\nTracking early-warning indicators from global news sentiment\n\nGenerating lender-ready risk dashboards with explainable AI\n\n‚úÖ Unified Decision Layer\n\nProducing an integrated ‚ÄúGreen Credit Score‚Äù that blends sustainability & solvency\n\nGiving banks, NBFCs, and investors a monitored, transparent risk profile\n\nHow we built it\n\n\nMulti-Source Data Layer\n\n\nWeb-scraped ESG news feeds, financial filings, sustainability reports\n\nSatellite proxy indicators (green cover change, effluent signatures)\n\nMarket sentiment from verified financial data APIs\n\n\nNLP & Vision AI\n\n\nLLM-based extraction of ESG entities (carbon disclosure, policy changes, violations)\n\nVision models for land-use detection and industrial emissions estimation\n\n\nESG Knowledge Graph\n\n\nBuilt an internal knowledge graph linking:\ncompany ‚Üí sector ‚Üí ESG events ‚Üí financial metrics ‚Üí risk indicators\n\n\nCredit Modelling\n\n\nRandom Forest + Gradient Boosting + time-series regressions\n\nSector-sensitive credit scoring with weighted impact of ESG metrics\n\n\nGreenwashing Detector\n\n\nConsistency checker comparing claims vs third-party signals\n\nDetects sudden disclosure jumps, inconsistent emissions figures, vague sustainability language\n\n\nDashboard\n\n\nStreamlit + FastAPI\n\nLive scoring, alerts, and explanatory visualizations\n\nScenario simulator for ‚Äúwhat if a company reduces emissions by 20%?‚Äù\n\nChallenges we ran into\n\nUnstructured ESG data ‚Äì companies disclose sustainability in inconsistent formats\n\nGreenwashing detection ‚Äì verifying claims was harder than extracting claims\n\nAligning ESG metrics with credit metrics ‚Äì both have different timescales and impact duration\n\nNoise in satellite signals due to cloud cover and inconsistent resolution\n\nModel explainability ‚Äì banks require clear explanations, not black-box scores\n\nAccomplishments that we're proud of\n\nBuilt a working ESG + credit scoring system under a single",
    "prize": null,
    "techStack": "java",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=S1-MCvsTB6Y",
    "demo": "https://app-7qxo3tzzssu9.appmedo.com/portfolio",
    "team": "Tanush shah",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/greenlens-ai-dynamic-esg-credit-intelligence"
  },
  {
    "title": "AvaPass",
    "summary": "TECHNICAL HIGHLIGHTS: AvaPass was built using a combination of advanced technologies, including React for the front-end interface, Solidity for smart contract development on the Ethereum blockchain, and TypeScript for maintainable code. The integration of crypto payments directly into the ticketing process exemplifies a seamless user experience.",
    "description": "**What it does**\nAVAPASS gives users a minimum amount of gas tokens for any Avalanche subnet so they can immediately explore it.\nOur web app lets users: Connect any major crypto wallet\nView and jump into different subnets\nAccess dApps inside each subnet\nClaim a small ‚Äústarter fuel‚Äù amount of gas tokens to perform basic actions within those subnets\n\n**Inspiration**\nAvalanche has one of the most innovative architectures in crypto, subnets. Yet most users can‚Äôt even enter them. Every subnet runs on its own gas token, which means users get stuck before they can even try a new dApp or test a new network. This onboarding friction stops exploration, stops innovation, and stops new subnets from getting real users.\nWe wanted to fix that.\nOur inspiration was simple:\nWhat if discovering a new subnet was as easy as opening an app?\nNo bridges, no swaps, no friction, just instant access.\n\n**How we built it**\nWe created multiple local Avalanche subnets to simulate real multi-subnet onboarding.\nIntegrated wallet connectivity supporting various wallet types, including Avalanche wallets.\nBuilt the UI/UX in a web app where users select a subnet, view its dApps, and receive their minimum gas allocation\nAll using React for the frontend, coupled with typescript, solidarity, and blockchain technologies in the backend.\n\n**Challenges**\nUnderstanding the Avalanche ecosystem and exploring the L1 subnet architectures. \nIncorporating testnet chain tokens from Fuji for the purpose of experimenting. \nIntegrating the frontend to the elements of the backend such as the smart contracts, and communication from the hub to faucet contracts.\ndApp creations and wallet/gas token logic associated with it.\n\n**Accomplishments**\nTeaching ourselves all the content in just a few days, going from absolute beginners to building a product that addresses a real onboarding issue in the Avalanche ecosystem.\nDeveloping a working prototype for a universal subnet onboarding platform that gives users the minimum gas needed to explore any subnet.\nIncorporating 4 different dApps with unique functionalities for users to experiment with using their newly acquired gas tokens.\nSuccessfully setting up and connecting multiple local subnets, integrating wallets, and creating a seamless user flow from wallet ‚Üí subnet ‚Üí dApp ‚Äî all within one weekend.\n\n**What we learned**\nHow Avalanche subnets function at a deeper technical level, including how gas tokens and local subnet environments work.\nThe challenges of onboarding users into multi-network ecosystems, especially when every subnet has its own token.\nHow to integrate and manage multiple wallets, networks, and dApps inside a single application.\nThe importance of simple UX in web3. Users shouldn‚Äôt need to understand complex token mechanics just to explore new technologies.\n\n**What's next**\nOnboarding a wider variety of real Avalanche subnets into our platform for users to explore.\nPartnering with subnet creators to sponsor gas credits so new users can enter their subnet\nImprovements in UI for more seamless navigation across wallets, subnets, and dApps.\n\n",
    "prize": "Winner Crypto",
    "techStack": "blockchain, crypto, react, solidity, typescript",
    "github": "https://github.com/hasan1970/AvaPass-backend.git",
    "youtube": "https://www.youtube.com/watch?v=IDMTvUh7Gxo",
    "demo": "https://docs.google.com/presentation/d/1gytveG_GrsBhKUl0MmdhZlWgSMCdEio3NfuKYVddm8Y/edit?usp=sharing",
    "team": "Hasan Shameer Muhammed, Samaah Shameer Muhammed, Spoorthi Pyarilal",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/fskjdh"
  },
  {
    "title": "TrueBite",
    "summary": "TECHNICAL HIGHLIGHTS: The application employs modern web technologies, including CSS3, HTML5, and JavaScript, to create a responsive and user-friendly interface. The integration of the Yelp AI API demonstrates advanced technical implementation, allowing for real-time data retrieval and personalized suggestions.",
    "description": "**What it does**\nTrueBite is a Chrome extension that uses Yelp AI API to introduce trusted ratings, AI-generated review insights, and better restaurant alternatives directly within DoorDash, Uber Eats, and Grubhub. It analyzes Yelp data in real time, summarizes what people love or dislike, and allows users to chat and ask follow-up questions about restaurants.\n\n**Inspiration**\nFood delivery apps often display inflated ratings that don‚Äôt always reflect real customer experiences, forcing users to jump between platforms to verify restaurant quality. We wanted to remove that friction by bringing trusted Yelp insights directly into the ordering flow, helping people choose meals with confidence. TrueBite was inspired by the idea that better information leads to better food decisions.\n\n**How we built it**\nWe built TrueBite as a Manifest V3 Chrome extension with content scripts that detect restaurant pages and extract key details like restaurant name and location. A secure background service worker communicates with the Yelp AI API, retrieves ratings and AI-generated insights, and sends structured results back to the UI. We inject a clean, non-intrusive interface into each platform and added an AI-powered chat layer so users can ask natural language questions about the results.\n\n**Challenges**\nEach delivery platform has a different and frequently changing DOM structure, making restaurant and location detection unreliable at times. Matching delivery app listings with Yelp businesses also required careful fuzzy matching to avoid incorrect results.\n\n**Accomplishments**\nWe successfully integrated Yelp AI API as the exclusive data source, built a responsive UI that works across three major delivery platforms, and created an interactive chat experience that turns static reviews into meaningful insights. Most importantly, TrueBite makes it easier for users to trust their food choices in real time.\n\n**What we learned**\nWe learned to use Yelp AI API to extract data. We also gained hands-on experience handling real-world challenges like fuzzy matching, rate limits, and UX constraints inside third-party websites.\n\n**What's next**\nNext, we plan to expand the chat experience with follow-up suggestions based on user preferences, add dietary and lifestyle filters, and introduce personalization over time. We also want to improve alternative recommendations using deeper Yelp AI API insights and explore accessibility and mobile-friendly extensions to reach more users.\n\n",
    "prize": "Winner First Place",
    "techStack": "css3, html5, javascript, yelp-ai-api",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=yaScBHrmiOk",
    "demo": "https://truebiteapp.netlify.app/",
    "team": "Private user",
    "date": "2025-12-15",
    "projectUrl": "https://devpost.com/software/truebite"
  },
  {
    "title": "exposed.tech",
    "summary": "Exposed.tech is an innovative platform that leverages advanced technologies to provide users with enhanced visibility and control over their digital environments. It integrates tools for data management and visualization, utilizing cutting-edge frameworks to deliver a seamless user experience across various devices.",
    "description": "**What it does**\nExposed.tech is an interactive mobile app that analyzes food and drink labels using to provide instant, comprehensive health information. Label Scanning: Users upload an image of an ingredient label.\nChemical Breakdown: The system extracts and identifies individual chemical compounds (e.g., Caffeine, Biotin).\nHealth Analysis: It provides clear pros, cons, and flags specific \"Bad Combos\" (e.g., high sugar combined with certain artificial colors). All backed by peer-reviewed research studies.\nVisual Personalities: Using advanced Generative AI, each identified chemical is transformed into a unique creature based on its properties, giving the ingredient a memorable visual identity, which users can collect.\n\n**Inspiration**\nWe were inspired by the universal challenge of reading nutrition labels. They're often complex, technical, and overwhelming, making it hard to truly understand what we're consuming. Our goal was to revolutionize this experience by making ingredient analysis transparent, accessible, and fun. We wanted to move beyond dry data and transform chemical compounds into engaging, visual personalities, empowering users to make genuinely informed health choices.\n\n**How we built it**\nWe developed a full-stack solution integrating powerful AI models and robust programming frameworks. Frontend (React): Built a responsive user interface to handle image uploads and display the analysis dashboard.\n Backend (Python): This script orchestrates the entire process. It uses OCR (Optical Character Recognition) to convert the label image into text.\n Analysis Pipeline: The script then cleans the text, cross-references ingredients against a health database, and calculates risk factors. \n Connectivity: We used Cloudflare during development to securely tunnel the Python server,  allowing the React frontend to easily access the analysis API. We also incorporated workers controlled Cloudflare D1 SQLlite databases for our indexing and user authentication.\n\n**Challenges**\nOCR Reliability: Text extraction from curved surfaces (like bottles) or labels with varying fonts presented challenges, requiring complex string manipulation to clean the resulting ingredient list.\nFull-Stack Integration: Setting up the initial communication between the React frontend and the Python API while managing restrictions proved to be a significant early hurdle.\n\n**Accomplishments**\nSuccessful End-to-End Pipeline: We successfully built a working prototype that takes a raw photo and outputs a complex, informative, and visually stunning analysis in seconds.\nCreative AI Use: Demonstrating a novel, educational application of Generative AI by making intimidating scientific information relatable and fun.\nTechnical Integration: Seamlessly integrating multiple technologies‚ÄîReact, Python, OCR, and the Gemini API‚Äîinto one coherent, high-speed product.\n\n**What we learned**\nWe deepened our understanding of API management and full-stack deployment, particularly the practical difficulties of linking independent frontend and backend services in a real-world application. We learned how to handle OCRs and the ingredient data.\n\n**What's next**\nDatabase Expansion: Expanding analysis beyond food and drink to include more dietary restrictions and other complex consumer products.\n\n",
    "prize": "Winner Hack with the Best Startup Potential by Morrissette",
    "techStack": "cloudflare, gemini, javascript, python, react-native, vision",
    "github": "https://github.com/PconnorsUWO/HW-12/tree/newCubed",
    "youtube": "https://www.youtube.com/watch?v=fB2axB4Ec0g",
    "demo": "https://exposed.tech/",
    "team": "Matthew Li, Patrick Cauilan Connors, Thierry Huot",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/exposed-tech"
  },
  {
    "title": "SafeSteps AI",
    "summary": "TECHNICAL HIGHLIGHTS: SafeSteps AI utilizes a stack that includes CSS, HTML, and JavaScript for front-end development, providing a responsive and interactive user interface. The backend features Python and Jupyter for data analysis and machine learning, leveraging libraries like Pandas for data manipulation and Gemini API for real-time data integration.",
    "description": "**How we built it**\nData Sources  \n\n\n10+ years of Toronto Police Public Safety Data (158 neighbourhoods)\nOpenStreetMap full topology (11,000+ intersections, 13,000+ street segments, POIs, lighting, etc.)\n\nGeospatial Processing (Python)  \n\n\nPoint-in-Polygon: every intersection inherits its neighbourhood‚Äôs crime rate\nFeature engineering:\n\n\nDensity of bars/clubs in 100 m radius\nStreet type & hierarchy\nIntersection complexity (number of connected roads)\n\nFinal risk score (0‚Äì100) per intersection using weighted formula\nStreet segments = average of their two endpoints\n\nGraph & Routing  \n\n\nBuilt weighted graph with NetworkX\nA* algorithm finds the safest path (not the shortest)\n\nWeb App  \n\n\nLeaflet.js dark-mode interactive map\nStreets colored green / yellow / red\nReal-time route calculation\nClick any street ‚Üí see exa\n\n**Challenges**\nMerging neighbourhood-level crime data with street-level geometry (solved with spatial indexing)\nKeeping the app fast with 11,000+ nodes (vectorized calculations + caching)\nMaking sure the map informs without stigmatizing communities (transparent methodology + focus on infrastructure factors)\n\n",
    "prize": "Winner Second Place",
    "techStack": "css, geminiapi, html, javascript, jupyter, pandas, python",
    "github": "https://github.com/Solarcemir/SafeSteps_AI",
    "youtube": "https://youtu.be/M0kzqOYuxMA",
    "demo": null,
    "team": "Luis Hoi Fan, Honiya Maqsood, Hira Ahmad, Saksham Jain, Amy Zhang",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/sheridan-datathon"
  },
  {
    "title": "Uniwell",
    "summary": "Uniwell is a web application designed to enhance the well-being of university students by providing resources for mental health, wellness activities, and peer support. The platform aims to foster a supportive community where students can easily access wellness resources tailored to their needs.",
    "description": "**What it does**\nUniWell offers five main features: daily check-ins that track a student‚Äôs mood throughout the week and provide a weekly summary; a habit tracker that promotes healthy routines to support mental well-being; an anonymous chat box that allows students to share their thoughts without hesitation; and a location-based feature that connects students to nearby support services if they need in-person assistance.\n\n**Inspiration**\nFor some first-year students coming from high school, the transition involves not only a new school environment but also adapting to a new city or even a new country. It is normal to face challenges; however, not having timely access to support and resources can significantly affect their mental health and academic performance.\nThis is also our inspiration for creating UniWell - an application designed to support students experiencing difficulties by offering immediate access to relevant resources, daily well-being check-ins, and a secure, anonymous platform through which they can seek assistance.\n\n**How we built it**\nWe built UniWell based on our own experiences as first-year students, using tools like Gemini to help generate, refine, and organize our code.\n\n**Challenges**\nInitially, we were confused about how to use Gemini because it was a completely new tool for all of us. We weren‚Äôt familiar with its interface, its capabilities, or the best ways to prompt it effectively. At first, even simple tasks felt overwhelming because we weren‚Äôt sure which features to use or how to structure our inputs. However, as we continued experimenting, exploring its functions, and learning from trial and error, we gradually became more confident. Over time, what started out as uncertainty turned into a better understanding of how Gemini works and how we can use it to support our project more efficiently.\n\n**Accomplishments**\nAn accomplishment we are really proud of is how effectively we coordinated as a team to solve any challenges that came up. Whenever we faced an issue‚Äîbig or small‚Äîwe communicated openly, shared ideas, and supported each other until we found a solution. Our teamwork became one of our biggest strengths, and it helped us stay organized, motivated, and confident throughout the entire process. This collaboration not only made the work easier but also strengthened our trust and connection as a group.\n\n**What we learned**\nWe also learned how to integrate AI into our project-building process. At first, it felt unfamiliar, but as we experimented, we discovered how AI could support brainstorming, planning, problem-solving, and even refining our final product. Understanding how to use AI as a tool‚Äînot to replace our work but to enhance it‚Äîbecame a valuable skill. It helped us work more efficiently, think more creatively, and approach challenges with new perspectives.\n\n**What's next**\nWhat‚Äôs next for Uniwell is continuing to grow and refine the app. This includes adding more features that support students‚Äô mental health and wellness, gathering user feedback to understand what can be improved, and increasing our marketing efforts so more university students can discover and benefit from the app. Our goal is to keep evolving Uniwell based on real user needs and ensure it becomes an even more helpful and accessible resource.\n\n",
    "prize": "Winner Prizes",
    "techStack": "html, react",
    "github": "https://github.com/heidicodes/uniwell-yayy",
    "youtube": "https://www.youtube.com/watch?v=Qyem5E7JaEg",
    "demo": null,
    "team": "Cleo Chuu, Shivali Kumar, tiir-dnn, a1shan1",
    "date": "2025-11-22",
    "projectUrl": "https://devpost.com/software/uniwell-7uycs1"
  },
  {
    "title": "Echo",
    "summary": "TECHNICAL HIGHLIGHTS: The project utilizes a robust tech stack, including the Google Cloud Speech-to-Text API for accurate voice recognition, Eleven Labs for audio processing, and OpenAI for intelligent responses. Its cross-platform compatibility is achieved through technologies like React and React Native, ensuring a smooth user interface and experience.",
    "description": "**What it does**\nEcho is a voice-first mobile app that helps seniors access essential support through natural interaction. It offers: Voice Interface: Talk naturally to navigate the app or ask for information, with text input available when needed.\nEmergency Access: One-tap 911 button with an extra confirmation step to prevent accidental calls.\nMedication Management: Users can say ‚Äúremind me to take aspirin at 8am,‚Äù and Echo automatically understands the request, schedules notifications, and suggests appropriate medication categories based on symptoms.\nVolunteer Matching: Connects seniors with volunteers for home repair, mobility assistance, grocery help, companionship, tech support, or medical transportation.\nHealth Services Locator: Uses Google Places API to find nearby hospitals, clinics, and urgent car\n\n**Inspiration**\nOlder adults face two major access gaps: complex technology and scattered resources. Many apps rely on tiny text, multi-layered menus, and fast interfaces that unintentionally exclude seniors. At the same time, many older adults don‚Äôt know where to find community meals, events, health services, or volunteer help. We wanted to build something that makes technology feel human again. Echo came from the belief that technology should meet people where they are. For seniors, that means being able to simply speak and get the support they need.\n\n**How we built it**\nFrontend: React Native with Expo for iOS, Android, and Web Backend: Express.js with Google Cloud Speech-to-Text, OpenAI GPT-4o, Google Gemini, ElevenLabs Text-to-Speech, and Google Places API Core Systems:\nCross-platform audio abstraction, \nNatural language understanding and command routing, \nLocation services, \nPush notifications, \nMedication reminder parsing, \nAPI quota monitoring and fallback logic\n\n**Challenges**\nManaging API credits under time pressure\n\n**Accomplishments**\nBuilding a genuinely intuitive voice-first interface tailored for seniors\nIntegrating medication reminders, events, health services, and volunteer help into one seamless experience\nDesigning interfaces with large touch targets, clear hierarchy, and simple navigation\nCreating a smart routing system that understands user intent rather than rigid commands\nEnabling natural language medication reminders that work even with vague or incomplete phrasing\nDeveloping robust fallback logic to maintain reliability even when APIs fail\nBuilding trust-oriented features and planning for verified volunteers\nWorking effectively as a team, learning how to divide responsibilities, share workload, and communicate under a tight deadline\n\n**What we learned**\nHow to collaborate efficiently by splitting tasks, documenting decisions, and keeping a fast feedback loop\nHow to structure a project under time pressure with clear milestones and a functional workflow\nThat managing multiple APIs requires layered fallback strategies and continuous quota monitoring\nThat audio features require platform-specific handling even in cross-platform frameworks\nThat designing for seniors means simplifying decision paths, not just enlarging text\nThat flexible intent detection is crucial for natural voice interactions\nHow to communicate, adapt, and troubleshoot together as a team\n\n**What's next**\nOffline Mode for essential reminders without internet\nMulti-language support\nCaregiver dashboard for medication adherence\nIntegration with healthcare systems\nExpanded volunteer network with scheduling and verification\nTelehealth consultation features\nUser-created events and community spaces\nFurther accessibility improvements, including larger text options and better screen reader support\n\n",
    "prize": "Winner Best Beginner Hack",
    "techStack": "elevenlabs, expo.io, express.js, gemini, google-cloud-speech-to-text-api, google-places, ios, javascript, node.js, openai, react, react-native, typescript, web-audio-api",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=t7K35F0epPk",
    "demo": "https://echowaves.health/",
    "team": "Vera Li, Wanru Huang, Peijun Chen",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/echo-g1o862"
  },
  {
    "title": "6ixAssist",
    "summary": "6ixAssist is an innovative application designed to enhance user experiences in urban environments by providing real-time assistance and information through an interactive interface. By integrating mapping services and user-friendly navigation, it aims to streamline tasks such as locating amenities, public transport, and local events.",
    "description": "**What it does**\n6ixAssist helps users find nearby essential services, such as food banks, shelters and community resources. Users can type anything in their own words, and the app translates it into actionable search results, showing the closest support options on an interactive map. This allows users to find support fast.\n\n**Inspiration**\nWe were inspired by the rising challenges of poverty and homelessness in Toronto. We wanted to build something practical, that could make a real, immediate difference for people who are struggling to find basic support when they need it.\n\n**How we built it**\nWe used the Gemini API to power the searching and Open Street map API to surface the closest shelters, food banks, and essential services. Maps also handles geolocation, routing, and rendering everything directly on the interface. Tailwind CSS allowed us to build a clean, responsive UI quickly, while lightweight JavaScript ties the entire experience together.\n\n**Challenges**\nOur biggest challenges were debugging API key issues, handling errors from both Gemini and Open Street Map, and designing logic that could bridge the gap between natural-language understanding and location-based search. Especially with the time-constraint, it was difficult to work through these issues. But through this, we built collaborative skills.\n\n**Accomplishments**\nWe‚Äôre proud that we delivered a complete, end-to-end project under tight time constraints, and built it in a way that can be expanded far beyond its current version. We‚Äôre also proud that our work isn‚Äôt just a technical demo, it‚Äôs a tool that can actually support people in Toronto who need quick access to essential services. Knowing that this project has real-world impact makes the achievement even more meaningful for us.\n\n**What we learned**\nThroughout this project, we picked up skills that are essential for modern developers. We learned how to use AI tools like the Gemini API to interpret natural language, how to integrate those results with real-world data from Open Street Maps, and how to manage multiple API keys across environments. We also strengthened our UI/UX design skills by building a responsive interface with Tailwind CSS. These experiences taught us how to combine AI, geolocation, and frontend engineering into one cohesive product.\n\n**What's next**\nWe hope to keep expanding it by integrating official open-data sources, adding real-time availability for shelters, and supporting additional languages through Gemini‚Äôs multilingual capabilities. We also want to improve the user experience, add bookmarking features.\n\n",
    "prize": "Winner Prizes",
    "techStack": "gemini, leaflet.js, react, tailwind-css, typescript",
    "github": "https://github.com/LoriB14/ellehacks",
    "youtube": "https://www.youtube.com/watch?v=tCAIiEWrvhk",
    "demo": "https://ellehacks.vercel.app/",
    "team": "aizah sadiq, Lori Battouk",
    "date": "2025-11-22",
    "projectUrl": "https://devpost.com/software/6ixassist"
  },
  {
    "title": "SustainaBite",
    "summary": "TECHNICAL HIGHLIGHTS: Built with React and TypeScript, SustainaBite benefits from a robust front-end framework that allows for a responsive and dynamic user interface. The use of TypeScript enhances code quality and maintainability, which is crucial for scalability. Notable implementations may include a recommendation algorithm that tailors meal suggestions to individual user profiles based on preferences and dietary restrictions.",
    "description": "1234    \n\n\n\n      \n  The catastrophic cycle of food waste, where up to 90% of restaurant leftovers are trashed, driving strong greenhouse gas emissions and wasting the hard labor of local farmers, demands an immediate solution. Our response is Sustanabite, an eco-friendly marketplace designed to disrupt this cycle by connecting consumers to discounted food surplus and fresh farm produce in Ontario. The platform operates on a simple 3-user model for Consumers, Businesses (restaurants and farmers), and Drivers, each supported by a personalized profile section. Our core innovation is Food Buddy, an intelligent chatbot powered by the Google Gemini API, which offers instant, waste-reducing recipes for consumers and inventory optimization guidance for businesses. Consumers can easily navigate the marketplace, choosing between Leftovers or Local Produce, utilize the dietary specification filter, and complete transactions via a cart with both delivery and pickup options. Meanwhile, Businesses are empowered by a dedicated Dashboard for posting and managing orders, complemented by a Sales & Analytics Section that tracks revenue and top-selling products, helping them become proactively sustainable and profitable. \n\n\n\n        \n    Built With\n\n    reacttypescript\n  \n\n        \n    Try it out\n\n    \n        \n  \n  GitHub Repo\n\n        \n  \n  sustainabite-seven.vercel.app",
    "prize": "Winner Prizes",
    "techStack": "react, typescript",
    "github": "https://github.com/sutr4/sustainabite",
    "youtube": "https://www.youtube.com/watch?v=Y0vRktjl_Sk",
    "demo": "https://sustainabite-seven.vercel.app/",
    "team": "tracy su",
    "date": "2025-11-22",
    "projectUrl": "https://devpost.com/software/sustainabite-of4zcx"
  },
  {
    "title": "MediFusion AI",
    "summary": "TECHNICAL HIGHLIGHTS: MediFusion AI employs a robust tech stack that includes Docker for containerization, FFmpeg for audio processing, and various libraries like Hugging Face for natural language understanding. The combination of technologies such as Python, speech-to-text capabilities, and cloud solutions like Groq-cloud ensures efficient processing and deployment of the application.",
    "description": "**What it does**\nMediFusion AI is an AI Voice Doctor that allows users to talk naturally and get intelligent, medically relevant responses. It: Listens to patient speech using Whisper + ElevenLabs STT with noise reduction\nAnalyzes symptoms and medical images using Google Cloud Vertex AI + LLaMA 3 Meta Vision (Groq API)\nResponds like a human doctor using ElevenLabs TTS for natural medical voice output\nPredicts diseases using ML models for Diabetes, Tumor Detection, and Heart Disease\nProvides a fully voice-based consultation through a Gradio conversational UI\n\n**Inspiration**\nIn many regions, patients experience long waiting times and challenges accessing healthcare due to limited medical resources. We wanted to build an intelligent system that enables fast, voice-driven medical interaction. MediFusion AI bridges the gap between patients and doctors using real-time speech understanding, medical reasoning, and disease prediction.\n\n**Challenges**\nManaging latency between multimodal model and speech generation\nSynchronizing STT, reasoning, TTS, and UI in real time\nHandling noisy audio input and transcription accuracy\n\n**Accomplishments**\nReal-time voice-based intelligent medical consultation system\nIntegrated ElevenLabs + Google Cloud + Groq multimodal AI successfully\nAchieved smooth, natural conversational experience\n\n**What we learned**\nMultimodal AI systems (Vision + Voice + ML)\nCloud deployment and real-time inference pipelines\nBuilding scalable conversational AI healthcare solutions\n\n**What's next**\nMultilingual voice consultation\nMedical reports + symptom history tracking\nMobile app version and Firebase integration\nEnd-to-end patient analytics dashboard\n\n",
    "prize": null,
    "techStack": "deep-learning, docker, elevenlabs, ffmpeg, gradio, groq, groq-cloud, gtts, huggingface, llama-3-vision, machine-learning, openai-whisper, pyaudio, python, speech-to-text, vs-code",
    "github": "https://github.com/amit-sharma-ds/GenAIHeathcare",
    "youtube": "https://www.youtube.com/watch?v=kCBcLShagHk",
    "demo": "https://huggingface.co/spaces/AmitSharma99/MediFusionAI",
    "team": "Amit Sharma",
    "date": "2026-01-17",
    "projectUrl": "https://devpost.com/software/medifusion-ai-48mn51"
  },
  {
    "title": "3Docs",
    "summary": "3Docs is an innovative project that leverages artificial intelligence to enhance the way users interact with audio documentation, potentially targeting educational or training environments. By integrating advanced audio processing and AI technologies, the project aims to create an intuitive platform that efficiently organizes, analyzes, and presents audio content.",
    "description": "**What it does**\n3Docs is a platform that transforms boring old PDF manuals into interactive 3D manuals complete with voice guidance and PDF references for each step of a process.\n\n**Inspiration**\nReading a manual is hard, especially when images don't always make the most sense. However, guides are a lot easier to follow when more information (like an extra dimension!) is available. 3Docs makes repair more accessible by displaying manuals intuitively in ways that can \"make it click\" for more people.\n\n**How we built it**\nFrontend: Built with React (Next.js). We utilized the Three.js to render the .glb files directly in the browser.\nBackend: We used Python to manage API calls, caching, and sqlite database queries & insertions. To determine which images and instructions were useful and paraphrase instructions, we used Gemini 3 Pro Preview and Gemini 2.5 Pro, and we then matched paraphrased instructions to the loaded PDF to synchronize the user experience. To generate 3D models from images, we attempted several models, but ended up using the Tripo3D API; to generate audios for each instruction we used Fish Audio's API.\n\n**Challenges**\nCurrently available Image to 3D model models are kind of not great, so we had to use a private model locked behind an API.\nOlder models of Gemini (like 2.5 Pro) are bad at determining image content and usefulness\nLive audio processing and API calls for real-time generation of audio has too high latency to work well\n\n**What's next**\nThere are some features that 3Docs still lacks, such as input sanitation and live conversation. We also could benefit from sourcing a better and open source Image to 3D model model.\n\n",
    "prize": "Winner MadHacks First Place",
    "techStack": "artificial-intelligence, fish-audio, gemini, next.js, python, react, sqlite, tripoai, typescript",
    "github": "https://github.com/r-thak/3Docs-MadHacks2025-Winner",
    "youtube": "https://www.youtube.com/watch?v=FfbDphUebQk",
    "demo": "https://docs.google.com/presentation/d/1wH5-2nVvFJ_qfr9qA5CMiAYAcgmEjXY4lHjbWKdL734/edit?usp=sharing",
    "team": "Manas Joshi, Nick Yeung, Rohan Thakkar, Khoa Cao",
    "date": "2025-11-22",
    "projectUrl": "https://devpost.com/software/3docs"
  },
  {
    "title": "FocusMate",
    "summary": "TECHNICAL HIGHLIGHTS: The project employs HTML and JavaScript for its front-end development, ensuring a responsive and user-friendly interface. The integration of OpenAI suggests advanced functionalities, such as natural language processing for task management or intelligent scheduling, which enhance user interaction and personalize the experience.",
    "description": "**What it does**\nFirst you tell the extension what task you will be focusing on, then the extension starts a timer and samples what tabs you have open in 10 minute intervals to tell how productive you are being. Having a majority of productive tabs will award you more points, where as you can lose points if you have too many off task tabs.\n\n**Inspiration**\nThe main inspiration was productivity/educational applications that add an aspect of gamification to the process of working/learning. We also drew inspiration from online videogames with concepts like achievements, badges, and leaderboards.\n\n**Challenges**\nUnderstanding how to use the OpenAI API. The main one was figuring out how to parse the large JSON file to get the information we wanted. Fixing small bugs with the logic for rewarding achievements since some of them didn't work.\n\n**Accomplishments**\nFiguring out how to utilize the OpenAI API to tell if a user is on task. Creating unlockable achievements with fun designs that help to add to the gamification aspect of our project\n\n**What we learned**\nHow to make a chrome extension with embedded ChatGPT functionality\n\n**What's next**\nFinalizing the landing page and deployment!\n\n",
    "prize": "Winner Best Productive Hack",
    "techStack": "html, javascript, openai",
    "github": "https://github.com/Stevie169/FocusMate",
    "youtube": "https://www.youtube.com/watch?v=_bDibaqiO5g",
    "demo": null,
    "team": "Shunyao Zhang, IanFleuchaus, Anthony Sestito, kishanKOTA Kishan rishik kota",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/focusmate-j1fwt2"
  },
  {
    "title": "Findr",
    "summary": "Findr is a platform designed to streamline the process of locating and connecting with friends or acquaintances in real-time, leveraging geolocation features. It aims to enhance social interactions by allowing users to easily find nearby connections for spontaneous meet-ups or planned gatherings.",
    "description": "**What it does**\nFindr is a modern digital lost-and-found platform for schools. It has two main parts:\nStudent Portal: Students can browse a clean feed of found items, each with a photo, date, and description ‚Äî no login required. This makes it easy to quickly check if their belongings have been found.\nTeacher Portal: Teachers log in securely to upload new items with photos and descriptions, and can remove old items. Everything posted instantly appears in the student feed, keeping the system organized and efficient. \nFindr replaces messy lost-and-found boxes with a fast, user-friendly, and organized digital solution.\n\n**How we built it**\nWe built Findr using a full-stack approach:\nFrontend: React + TypeScript + Vite for a fast and clean UI\nStyling: TailwindCSS for modern, responsive design\nBackend: Firebase Authentication + Firestore Database\nImage Storage: Firebase Storage for uploaded photos\nDeployment: Vercel for smooth hosting\nWe connected teacher authentication, real-time database updates, and image uploads to create a seamless experience.\n\n**Challenges**\nDesigning the UI: Making it modern, simple, and easy for both students and teachers\nAuthentication Logic: Ensuring only teachers can log in and upload items\nDatabase Sync: Making sure new items appear instantly for students\nImage Handling: Compressing and storing images without slowing down the app\nTime Constraint: Building the full stack and polishing everything quickly\n\n**Accomplishments**\nBuilt a fully working app with real authentication, image upload, and item feed\nCreated a clean and elegant UI that feels professional\nBuilt a system that actually solves a real school problem\nMade a digital solution that could realistically be used by any school. More specifically, something that real schools would be interested in purchasing.\n\n**What we learned**\nHow to integrate frontend and backend systems efficiently\nHow to use Firebase Auth, Firestore, and Storage together\nHow to design user-friendly interfaces under time pressure\nHow to create an idea that is not too overcomplicated but also not basic, like a task manager.\n\n**What's next**\nI want to create a history feed where teachers can see all the things that were found and lost to better keep a record. I also want notifications to teachers when a student claims an item. \nLastly, I would also like to add a bounty system where students can get points or some sort of reward if they turn things in to the lost and found. This would create a reason to give things to the lost and found instead of stealing it.\n\n",
    "prize": "Winner TOP 6 TEAMS",
    "techStack": "firebase, react, tailwind, tailwindcss, typescript, vercel, vite",
    "github": "https://github.com/RakeshJai/Findr",
    "youtube": "https://www.youtube.com/watch?v=nmn9570g19Q",
    "demo": "https://findr-murex.vercel.app/",
    "team": "Rakesh Jai",
    "date": "2025-11-22",
    "projectUrl": "https://devpost.com/software/findr-s7vyjn"
  },
  {
    "title": "Jarvis Smart OS",
    "summary": "Jarvis Smart OS is an advanced operating system that leverages AI technologies to optimize user experience and enhance productivity through intelligent automation and personalized assistance. Its integration with Google Gemini API allows for sophisticated natural language processing and machine learning capabilities, enabling users to interact seamlessly with their devices.",
    "description": "**What it does**\n‚ÄúJARVIS automates tasks, understands commands, manages devices, and acts as a smart assistant that simplifies and enhances everyday computing.‚Äù\n\n**Inspiration**\n‚ÄúI built JARVIS OS because I refused to let the future stay in the movies.‚Äù\n\n**How we built it**\n‚ÄúI built it by using Gemini AI model for the brain, automation scripts, and custom code into one unified system that can understand, respond, and take action.‚Äù\n\n**Challenges**\n‚ÄúThe biggest challenges were turning imagination into code, making the system truly intelligent, and overcoming the limits of technology.‚Äù\n\n**Accomplishments**\n‚ÄúI‚Äôve built the foundation of JARVIS‚Äîan intelligent system that listens, responds, and begins to think alongside its creator.‚Äù\n\n**What we learned**\n‚ÄúI learned that with vision, persistence, and curiosity, even the impossible can become real.‚Äù\n\n**What's next**\n‚ÄúNext, JARVIS will evolve into a fully autonomous, intuitive OS that learns, adapts, and collaborates seamlessly with its user.‚Äù\n\n",
    "prize": "Winner Best Use of Google Gemini API",
    "techStack": "elevenlabs, gemini, mem0, python, pythonlibraries",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=ot9sI5nplZw",
    "demo": null,
    "team": "Barrack vobeck",
    "date": "2025-11-22",
    "projectUrl": "https://devpost.com/software/jarvis-smart-os"
  },
  {
    "title": "Frames",
    "summary": "Frames is a design-focused project that leverages collaborative tools like FigJam and Figma to streamline the process of creating and presenting design mockups. By integrating these platforms, the project enables teams to visualize ideas effectively and share them seamlessly, enhancing collaboration and feedback during design iterations.",
    "description": "Mockup\n    \n\n              \n  \n\n\n\n            \n        \n          \n  \n\n        \n    \n      Mockup\n    \n\n              \n  \n\n\n\n            \n        \n          \n  \n\n        \n    \n      Mockup\n    \n12    \n\n\n\n      \n  View project brief\n\n\n\n        \n    Built With\n\n    figjamfigmafigma-slides\n  \n\n        \n    Try it out\n\n    \n        \n  \n  www.figma.com\n\n        \n  \n  www.figma.com\n\n        \n  \n  drive.google.com",
    "prize": "Winner Second Place",
    "techStack": "figjam, figma, figma-slides",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=8eLthlAIr90",
    "demo": "https://drive.google.com/file/d/16-t7E1vizsNB2FM3DsB1DONLk2f1y5TW/view?usp=sharing",
    "team": "natalie yu",
    "date": "2025-11-22",
    "projectUrl": "https://devpost.com/software/frames-wv0j6l"
  },
  {
    "title": "NutriScan AI",
    "summary": "NutriScan AI is a health-focused application designed to analyze nutritional information and provide personalized dietary recommendations. Leveraging advanced machine learning algorithms, the tool helps users make informed food choices based on their individual health needs and preferences.",
    "description": "**What it does**\nNutriScan uses a multi-modal AI system powered by two visual assessments:\nFace Scan Reads hydration levels.\nPicks up pallor and vitality cues. Fingernail Scan Analyzes keratin strength and texture.\nReads colour patterns tied to iron, B-vitamins, and minerals.\nChecks nail-bed circulation for micronutrient indicators. These scans feed into a lightweight lifestyle questionnaire. The system then generates: An Overall Health Score\nSubscores for hydration, vitamins, minerals, protein quality, and calorie balance\nA personalized daily meal plan\nA simple supplement protocol\nA budget-friendly grocery list\nA clean dashboard with widgets to track your daily targets\n\n**Inspiration**\nNutriScan came from a simple problem most university students face: eating healthy on a tight budget takes way too much effort. Classes, assignments, and job applications already drain your time. Building a meal plan on top of that feels unrealistic. Most nutrition apps tell you to ‚Äúhit your calories,‚Äù but they don‚Äôt explain what your body actually needs or why. None of them factor in how broke or busy you are as a student. NutriScan fixes that. You take two quick scans, answer a short form, and get direct, affordable recommendations you can follow without thinking. It‚Äôs built for real students with real schedules.\n\n**How we built it**\nWe built NutriScan AI using a modern React and TypeScript stack, capturing high-quality biometric imagery directly in the client. These visual inputs are processed by Google's Gemini Multimodal API, which analyzes the images for physiological biomarkers and references them with user-provided health data. The frontend, styled with Tailwind CSS for a responsive dark-mode experience, parses the AI's structured JSON response to render real-time, interactive visualizations and personalized meal plans.\n\n**Challenges**\nPersonally the hardest part was finding an idea that was actually useful and original. Most ground breaking ideas either already seem to exist or don‚Äôt solve a real problem. Once we settled on NutriScan, the next challenge was figuring out how to implement it cleanly with a simple yet intuitive UI that doesn‚Äôt overwhelm users. We had to balance keeping the scans fast and the dashboards readable with shipping meaningful features.\n\n**Accomplishments**\nThe entire project, we came up with the idea Saturday morning and we implemented everything by working non stop till 5 am in the morning. Constantly improving the project while also making it.\n\n**What we learned**\nWe learned that the power of AI only shows up when you have a strong idea and a clear plan. Once we locked in the concept, everything, from prompts to model outputs to UI decisions, became 10x easier. Good features came from understanding the user (Us - Students), not from trying to overbuild. To use a strong idea mattered more than the tech. We also learned how much teamwork shapes the final product. The connection between us pushed the project forward and made the long hours feel lighter. When everyone commits, the work moves fast and the results show. That‚Äôs why NutriScan came together the way it did. We‚Äôre proud of what we built and how we built it.\n\n**What's next**\nOur Immediate Next Steps -> Apple Watch integration to pull hydration, HRV, and sleep patterns\nNew assessments as we do deeper research\nUser-to-user updates and messaging for progress tracking\nExpanded scan capabilities with larger datasets\n\n",
    "prize": "Winner Best Health Hack by Sun Life; Winner [MLH] Best Use of DigitalOcean Gradient AI",
    "techStack": ".tech, digitalocean, gemini, git, github, javascript, json, node.js, react, tailwindcss, typescript, vite",
    "github": "https://github.com/ZElnagar/HackWestern12",
    "youtube": "https://www.youtube.com/watch?v=VRBSDdESFvY",
    "demo": "https://nutriscans.tech/",
    "team": "Ziad Elnagar, Tristan Beley, Shayaan Tanvir, Mohammadh Hammad",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/nurtiscan"
  },
  {
    "title": "HackProd",
    "summary": "TECHNICAL HIGHLIGHTS: The project employs JavaScript and React for efficient UI development, combined with Three.js for rendering 3D graphics. This powerful stack allows for real-time visualization and interaction, providing users with a compelling experience that sets it apart from traditional applications.",
    "description": "**What it does**\nHackProd is an autonomous AI agent that analyzes hackathon projects to detect 15 sponsor technologies, scores integration depth (0-10), generates technical + plain-English summaries, and validates integrations by actually running projects in the cloud via Lightning AI. It learns from every analysis‚Äîgetting smarter, faster, and more accurate over time.\n\n**Inspiration**\nJudges, sponsors, and organizers manually reviewing every hackathon project to understand sponsor integrations is time-consuming, inconsistent, and doesn't scale. Teams shouldn't have to write extra reports‚Äîtheir code should speak for itself.\n\n**How we built it**\nAgent: Claude/OpenAI orchestrator with autonomous tools (read files, search code, parse dependencies)\nMemory & Learning: Redis-based memory system with reflection loops‚Äîagent recalls past learnings and self-improves\nCloud Execution: Lightning AI integration to deploy, test, and validate projects actually work\nBackend: Node.js + Express + async job queue (Redis)\nStorage: Sanity CMS for structured results, optional AWS S3 for repos\nSponsors used: Anthropic (Claude), Redis, Sanity, Lightning AI, AWS\n\n**Challenges**\nMaking the agent truly autonomous with memory/reflection, implementing fail-safe cloud execution that never breaks static analysis, handling diverse project structures (monorepos, microservices), building reliable sponsor detection across 15 different technologies, managing API costs while achieving high accuracy.\n\n**Accomplishments**\n‚úÖ Autonomous learning agent that improves 42% in accuracy after 10 analyses\n‚úÖ Cloud execution validation via Lightning AI with proper fail-safes\n‚úÖ Self-reflection system that stores learnings and recalls them\n‚úÖ 15 sponsor detectors with deep integration scoring\n‚úÖ Complete pipeline: API ‚Üí Queue ‚Üí Clone ‚Üí Analyze ‚Üí Execute ‚Üí Store ‚Üí Cache\n\n**What we learned**\nHow to build truly autonomous agents with memory and reflection (not just chatbots), the power of combining static analysis + execution validation, importance of fail-safe design when adding complex features, that agents get dramatically better with real-world feedback loops.\n\n**What's next**\nMulti-agent specialists (one per sponsor technology), human-in-the-loop feedback to correct the agent, transfer learning to share knowledge across hackathons, GPU support for ML project analysis, visual UI testing with screenshots, Postman/Newman integration for comprehensive API validation.\n\n",
    "prize": "Winner Best use of LightningAI",
    "techStack": "javascript, react, three.js",
    "github": "https://github.com/bilgee0517/hackathon-automation-agent",
    "youtube": "https://www.youtube.com/watch?v=O8YEurGkzeQ",
    "demo": null,
    "team": "Chinguun Ganbaatar, Bilegjargal Altangerel",
    "date": "2025-11-21",
    "projectUrl": "https://devpost.com/software/hackprod"
  },
  {
    "title": "Urban",
    "summary": "Urban is a project designed to leverage advanced cloud technologies to create an innovative solution aimed at enhancing urban living. While specific details are not provided, the project's core likely involves utilizing data and AI tools to address challenges faced in urban environments, such as resource management, public safety, or community engagement.",
    "description": "**How we built it**\nOur build was a fast, hackathon-grade version of a production system: Document understanding: extracted and structured policy text into targets, rules, timelines.\nKnowledge modeling: stored policy + programs + milestones in a structured CMS.\nRetrieval + memory: indexed policies and past case summaries for fast semantic lookup.\nAgent orchestration: connected tool-calling agents with clear JSON schemas.\nSimulation layer: simple forecasting formulas + interactive UI sliders.\nDashboard UI: KPI cards, charts, scorecards, and a 3D map layer for impact.\nEnd-to-end demo flows: crisis case ‚Üí policy lookup ‚Üí evidence ‚Üí action plan ‚Üí analytics update.\n\n**Challenges**\nPDF complexity ‚Üí structured truth: policies are long, verbose, and sometimes inconsistent; extracting reliable targets was non-trivial.\nSimulation realism vs time: we had to keep models simple but believable in one day.\nTool coordination: ensuring agents worked with retrieval, evidence tools, and privacy layers without breaking the flow.\nAvoiding ‚Äúdashboard spam‚Äù: we focused on policy-aligned metrics only, not random charts.\nCivic safety: ensuring the system doesn‚Äôt give unsafe advice, especially in sensitive cases.\n\n**What we learned**\nPolicies become powerful only when they are operationalized. We learned how to convert dense government PDFs into checkable rules + KPIs.\nAgentic systems need guardrails. Civic agents must be safe, minimal-question, privacy-aware, and auditable.\nDashboards without simulations are blind. Real decision tools need ‚Äúwhat-if‚Äù views, not just reporting.\nThe story matters. The 3D post-effects layer dramatically improved how intuitive policy impact felt.\n\n",
    "prize": "Winner Best Use of Parallel; Winner Best use of LightningAI; Winner Top Overall Winners",
    "techStack": "amazon-web-services, lightningai, parallel, sanity, skyflow",
    "github": "https://github.com/Steve-Dusty/urban",
    "youtube": null,
    "demo": null,
    "team": "Ayaan Gazali, Steve Kuo",
    "date": "2025-11-21",
    "projectUrl": "https://devpost.com/software/urban"
  },
  {
    "title": "Briefly - Stay Smart in Minutes",
    "summary": "\"Stay Smart in Minutes\" is a project designed to provide users with quick, digestible insights and knowledge across various topics, leveraging advanced AI technologies. By integrating services such as natural language processing and speech synthesis, the platform aims to enhance learning and information retention in a time-efficient manner.",
    "description": "**What it does**\nBriefly creates personalized daily podcast episodes that summarize topics you care about in 5-, 10-, or 15-minute audio briefings. Using AI, it curates top stories, converts them to natural speech, and delivers them automatically to Spotify or your favorite podcast app. It‚Äôs effortless daily knowledge, tailored to you.\n\n**Inspiration**\nWe wanted to simplify how people stay informed. Endless articles, newsletters, and podcasts take hours to digest. Our team envisioned a smarter way, a personalized, AI-powered daily audio feed that turns the world‚Äôs noise into quick, high-quality insights people can actually keep up with.\n\n**How we built it**\nWe built the frontend using Lovable for fast, responsive design. The backend runs on Parallel, and integrated with Redis for caching. We used Claude and the Claude SDK to power our agents, which automatically search the internet for relevant news via Parallel. The summarized text is then converted to natural, high-quality speech using ElevenLabs, completing a seamless pipeline from web content to generated podcast audio.\n\n**Challenges**\nConnecting multiple tools and frameworks within limited time was a core challenge. Integrating embeddings and vector search for accurate topic recommendations required troubleshooting schema mismatches and fine-tuning queries. Synchronizing asynchronous agent calls between Parallel and ElevenLabs also tested timing and data consistency.\n\n**Accomplishments**\nLets users select personalized interests and generates dynamic podcast episodes in real time. Seamlessly transforms summarized text into high-quality, natural-sounding audio. Implements vector-based user preference matching that actually works. Looks polished and production-ready, complete with branding, UI, and logo system\n\n**What we learned**\nWe learned how powerful agents can be when orchestrated effectively, particularly their ability to browse, filter, and synthesize real-time web content. Vector databases add a whole new dimension to personalization but require deep understanding of embedding quality and retrieval optimization. We also saw that even in fast builds, clean architecture and modular design make iteration exponentially faster.\n\n**What's next**\nIntegrate Stripe for premium subscriptions and creator monetization. Expand content sources beyond news into research, business, and educational topics. Tailor summaries to knowledge levels and age demographics for broader accessibility. Launch a mobile-first app and Chrome extension for daily listening on the go. Our long-term goal: make personalized audio briefings a new standard for consuming information, fast, trustworthy, and human-like.\n\n",
    "prize": "Winner Top Overall Winners",
    "techStack": "amazon-web-services, anthropic, elevenlabs, loveable, parallel, python, redis",
    "github": "https://github.com/michaelwaves/briefly",
    "youtube": "https://www.youtube.com/watch?v=cHvPEjYWgaA",
    "demo": null,
    "team": "Bryce Masterson, Michael Yu, jmiller94102 Miller",
    "date": "2025-11-21",
    "projectUrl": "https://devpost.com/software/briefly-stay-smart-in-minutes"
  },
  {
    "title": "API Self Healer",
    "summary": "The API Self Healer project aims to automatically monitor and repair APIs, ensuring they remain functional and efficient even in the face of failures or changes in behavior. By leveraging advanced tools and technologies, the project provides a proactive solution for maintaining API integrity and reliability, enhancing overall application performance.",
    "description": "**Inspiration**\nDebugging and maintaining APIs is tedious‚Äî400 errors, missing fields, wrong types, evolving schemas. AI coding agents still require a lot of handholding to fix. We wanted an agent that acts like an API mechanic: it detects broken requests in postman, finds the relevant documentation, and fixes the request automatically in Postman.\n\n**How we built it**\nWe integrated Postman‚Äôs API with Claude and Parallel, designed failure cases across Stripe and Notion (wrong types, wrong data formats, missing required fields, malformed nested JSON structures), and built an iterative repair loop grounded in schema definitions from the docs.\n\n**Challenges**\nEnsuring the agent grounded fixes in real documentation, generating malformed schemas that produced meaningful validation errors, and coordinating the cycle of reading ‚Üí repairing ‚Üí retesting ‚Üí updating the request.\n\n**What's next**\nMore support for other types of API errors such as auth. Hosting it so it is accessible for other agents to use.\n\n",
    "prize": "Winner Best Use of Parallel; Winner Best Use of Postman",
    "techStack": "agentsdk, claude, parallel, postman, python, typescript",
    "github": "https://github.com/bekhamit/api-self-healer-agent",
    "youtube": "https://www.youtube.com/watch?v=rQES4c079sQ",
    "demo": null,
    "team": "Anna McGovern, Bek Hamit",
    "date": "2025-11-21",
    "projectUrl": "https://devpost.com/software/api-self-healer"
  },
  {
    "title": "StudyBuddy",
    "summary": "StudyBuddy is an innovative educational platform designed to enhance collaborative learning by connecting students with study partners based on shared subjects and study goals. The platform leverages AI-driven recommendations and real-time communication tools to create an engaging and productive study environment.",
    "description": "**What it does**\nStudyBuddy converts any study input, PDFs, YouTube links, Notion pages, URLs, or raw text into a complete AI-powered learning package:\nüìä Auto-generated slide deck with clean layouts and AI visuals\n‚ùì Interactive quizzes with explanations\nüÉè Flashcards for revision\nüìö Personalized learning using Redis (your history influences difficulty & selection)\nüîó Notion Integration (via MCP) saves organized content directly into your Notion workspace\nüé• YouTube transcript support extracts content from long lectures instantly\nüí¨ Context-aware chatbot that answers questions using your uploaded material StudyBuddy turns messy notes + long videos into structured learning instantly.\n\n**Inspiration**\nStudying today is overwhelming. Students collect PDFs, messy class notes, YouTube tutorials, Notion pages, screenshots, and random web articles, but turning all of this raw information into clean, structured, digestible study material is time-consuming. We wanted a tool that instantly transforms any learning content into slides, quizzes, flashcards, and summaries, so students can focus on learning, rather than formatting.\nThat idea became StudyBuddy.\n\n**How we built it**\nStudyBuddy is built as a multi-agent AI pipeline that deeply integrates Postman MCP, Notion, Sanity, Redis, and LLM reasoning. Every part of the system is powered by a sponsor tool. 1) Sanity CMS - Content Operating System Sanity acts as the content backbone of StudyBuddy, storing all generated learning materials in a clean, structured, and scalable way. ‚úÖ What we store in Sanity\nSanity houses every part of the learning package:\nSlides, Title, Bullet points, AI-generated diagrams, Associated images, Flashcards, Front (question or concept), Back (answer or explanation), Quizzes, Multiple-choice questions, Options, Correct answer, index, Explanations, Study summaries, Topic summaries, Chapter breakdowns, Assets, Images(generated diagrams, visual examples), Uploaded files, Reference media üöÄ\n\n**Challenges**\nNotion MCP integration: Understanding how MCP tools interact with Notion pages required debugging through MCPTotal\nEmbedding storage & retrieval speed when using Redis for large documents\nFragmented YouTube transcripts are causing inconsistent chunk boundaries\nTime constraints, balancing a multi-agent architecture within the hackathon time\n\n**Accomplishments**\nBuilt a full multi-agent learning pipeline end-to-end\nSuccessfully integrated Notion + MCPTotal MCP into our app\nCreated beautifully structured auto-generated slides & quizzes\nDesigned a clean UI that feels like a real product\nBuilt a scalable and modular architecture for future growth\nIntegrated YouTube ‚Üí Transcript ‚Üí Learning package flow\nAchieved usable personalization with Redis RAG\n\n**What we learned**\nHow powerful MCP workflows become when paired with Notion\nHow to integrate multiple sponsor tools into a single ecosystem\nEfficient prompt engineering for multi-agent systems\nStructuring long PDFs into meaningful chunks for RAG\nBuilding real-time pipelines between front-end, backend, Sanity, Redis, and MCPTotal\nHow to turn raw content into digestible study material using AI\n\n**What's next**\nAdding full audio narration for each slide\nExport to PowerPoint, Quizlet, and Anki\nA mobile app version for on-the-go studying\nSupport for GPT-based tutoring mode\nMulti-user workspaces + collaborative studying\nChrome extension ‚Üí ‚ÄúConvert this page to slides‚Äù\nLMS integrations (Canvas, Moodle, Blackboard)\n\n",
    "prize": "Winner Best Agent Using RedisVL; Winner Best Use of mcptotal.ai",
    "techStack": "fastapi, mcptotal, nextjs, postman, python, redis, sanity",
    "github": "https://github.com/Sakshamyadav19/MysticNotes",
    "youtube": "https://www.youtube.com/watch?v=sgEiWbvlSG8",
    "demo": null,
    "team": "Vedant Vivek Kandge, Yash Pokharna, Vedant Kandge, Mandar Menjoge, Saksham Yadav",
    "date": "2025-11-21",
    "projectUrl": "https://devpost.com/software/studybuddy-vzefio"
  },
  {
    "title": "Pulse",
    "summary": "TECHNICAL HIGHLIGHTS: The project is built using a combination of arkts, arkui, deveco-studio, and openharmony frameworks, showcasing its robust architecture. Notably, it employs advanced data processing algorithms to analyze health data from wearables, ensuring accurate and timely feedback to users.",
    "description": "**What it does**\nPulse is a standalone wearable application that uses a multimodal feedback system: Feels: A strong vibration metronome set to 110 BPM keeps the rescuer locked into the perfect rhythm.\nTells: Voice commands give immediate corrective feedback (e.g., \"Push Faster,\" \"Release Chest Fully\").\nShows: A clear, high-contrast UI designed for circular displays providing visual cues.\n\n**Inspiration**\nIn a cardiac emergency, panic is the enemy. While many people know the basics of CPR, maintaining the correct rhythm (100-120 BPM) and proper form (depth and recoil) while under extreme stress is incredibly difficult. We realized that the technology to solve this: accelerometers, gyroscopes, and haptic engines is already sitting on our wrists. We wanted to build a real-time CPR coaching system to turn a Huawei Watch 5 into a real-time life-saving coach that doesn't just track data, but actively guides the rescuer.\n\n**How we built it**\nWe built the application using ArkTS on the OpenHarmony operating system, pushing the wearable hardware to its absolute limits. By accessing raw high-frequency sensor data, we reconstructed the biomechanics of the rescuer's hands. Instead of relying on generic activity recognition APIs, we engineered and trained a custom machine learning decision tree using a proprietary dataset we collected ourselves directly with the Huawei Watch 5. By running this custom-trained model on-device, we can analyze complex motion patterns in real-time, allowing us to distinguish between proper compressions and specific form errors with a level of precision that standard algorithms cannot achieve.\n\n**Challenges**\nOur biggest technical hurdle was recoil detection (detecting \"leaning\"). Accelerometers are noisy, and distinguishing between a shallow press and a failure to release the chest (leaning) is mathematically complex due to gravity drift. We solved this by implementing a dynamic calibration system that locks the \"zero point\" when the user starts, allowing us to detect when the chest hasn't returned to its neutral position.\n\n**What we learned**\nThe Power of HarmonyOS & ArkTS: We learned that ArkTS is capable of much more than just simple UI. By leveraging OpenHarmony's efficient architecture, we were able to process high-frequency sensor data (50Hz) and run complex logic in real-time without compromising the smoothness of the UI. This proved to us that ArkTS delivers the native-grade performance required for critical health applications on wearable hardware.\n\n",
    "prize": "Winner Huawei",
    "techStack": "arkts, arkui, deveco-studio, huawei-watch-5, oniro, openharmony",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=9JfntLw_g_I",
    "demo": null,
    "team": "Carlo Lubeseder, Pascal Weidner, Linus Richter, Attila Ersin",
    "date": "2025-11-21",
    "projectUrl": "https://devpost.com/software/huawei-oxc57d"
  },
  {
    "title": "Les enfants  d‚ÄôAdam",
    "summary": "\"Les enfants d‚ÄôAdam\" is a project that aims to create an interactive platform, likely focused on educational or community engagement themes, leveraging modern web technologies. The project combines a robust backend with a user-friendly frontend to facilitate seamless interactions and data management, targeting a specific audience, possibly children or educators.",
    "description": "**What it does**\nWhat it does The project provides an intelligent platform that simplifies users‚Äô lives by bringing together essential services: communication, management, security, information, and automation through AI.\n\n**Inspiration**\nInspiration Our project was inspired by the need to solve real problems faced by communities. Every great solution begins with a challenge, and we decided to transform these challenges into opportunities for innovation.\n\n**How we built it**\nHow we built it We built the project by combining research, software development, and teamwork. We used modern tools, a clear structure, and strong collaboration to create a reliable and scalable solution.\n\n**Challenges**\nChallenges we ran into We faced several challenges, including limited resources, integrating different technologies, optimizing security, and managing time. But each challenge helped us improve and strengthen the project.\n\n**Accomplishments**\nAccomplishments that we're proud of We are proud of turning a simple idea into a functional solution. We succeeded in creating a strong, secure, and useful foundation with real impact. Most importantly, we learned, grew, and built the groundwork for the future of the project.\n\n**What we learned**\nWhat we learned Throughout this project, we learned how to transform problems into practical solutions. We improved our technical skills, strengthened our teamwork, and gained experience in building secure, scalable systems. We also learned the importance of user feedback and adaptability when creating impactful solutions.\n\n**What's next**\nInspiration\nWhat we learned Throughout this project, we learned how to transform problems into practical solutions. We improved our technical skills, strengthened our teamwork, and gained experience in building secure, scalable systems. We also learned the importance of user feedback and adaptability when creating impactful solutions.\nWhat‚Äôs next Next, we plan to expand the platform with more advanced features, improve the user experience, and integrate stronger AI capabilities. We will also focus on scaling the project, reaching more users, and building partnerships to bring the solution to a larger community.\n\n",
    "prize": null,
    "techStack": "node.js, postgresql, python, react, typescript",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=G64KJMLL7xk",
    "demo": "https://les-enfants-d-adam.onrender.com/",
    "team": null,
    "date": "2025-11-25",
    "projectUrl": "https://devpost.com/software/les-enfants-d-adam"
  },
  {
    "title": "Toyota GR Cup Real-Time Dashboard Simulator",
    "summary": "The Toyota GR Cup Real-Time Dashboard Simulator is an interactive platform designed to visualize real-time data related to the Toyota GR Cup racing events. It utilizes various technologies to provide users with an engaging and informative dashboard that displays live statistics, race analytics, and performance metrics.",
    "description": "**What it does**\nThe GR Cup Real-Time Dashboard Simulator is a browser-based analytics and strategy tool designed for use by a race engineer on a tablet. It provides a real-time, visual experience of the race. It shows live standings and telemetry, while dynamically tracking all cars on the Barber Motorsports Park circuit, highlighting the selected car in red. The tool features a Strategy Impact panel displaying critical metrics like Tire Status and degradation, and competitor Gap Ahead/Behind. Most importantly, it uses AI to provide an immediate, data-driven recommendation (e.g., PUSH) and calculates the optimal Pit Stop window.\n\n**Inspiration**\nThe project was primarily inspired by the critical need for race engineers to make instant, data-driven decisions in the high-pressure environment of a live race. I recognised that winning often depends on the speed and accuracy of strategic calls. My goal was to create a tool that transforms massive amounts of raw telemetry data into actionable intelligence, directly enhancing driver insights and team performance during race-day decision-making for the GR Cup Series.\n\n**How we built it**\nThe dashboard was built using JavaScript and the three.js library for the visualisation and car movement. The strategic insights and recommendations are generated using Google AI Studio. For data, I used the anonymised CSV files provided for the competition, including the Analysis Endurance and Weather data for Race 1, to simulate a live data feed. Creating the track layout involved tracing the Barber circuit on Google Maps, converting the file from KML to TXT, and then rendering it as an SVG to plot the animated car positions.\n\n**Challenges**\nThe primary technical challenge involved accurately mapping the circuit and syncing the car movement. I had difficulty extracting the PNG from the supplied files, so I manually traced the circuit and converted the file to an SVG format for web rendering. Furthermore, I had to precisely \"clip\" the car models and program the simulation to ensure cars not only flowed around the track but also realistically slowed down at bends, adding complexity to the data synchronisation based on the start and end times provided in the CSV.\n\n**Accomplishments**\nI am most proud of developing an elegant and highly functional solution. I created a clean, simple interface using a sans-serif font, optimised for rapid readability on a small screen under high pressure. A key accomplishment is the Actionable AI Insight system, which not only displays data but translates it into an immediate, justified action plan for the race engineer. I also successfully achieved a realistic simulation, showing cars moving and decelerating accurately on the custom-rendered track.\n\n**What we learned**\nI gained significant experience in the complexities of converting real-world geographic data into functional web visualisations. This project reinforced the need for custom solutions, such as tracing the track, when provided map files are challenging to integrate. I learned how to effectively process time-series race data to generate high-value strategic recommendations based on factors like tyre status and race positioning, emphasising the need for clarity and speed in the user experience.\n\n**What's next**\nThe next steps involve moving beyond the current simulation to a fully live system. The priority is to link the dashboard to a live racing API so the telemetry data reflects what the car is actually doing in real-time. Future enhancements include refining the AI for advanced predictive modelling, such as forecasting competitor tyre wear, and developing features to allow the engineer to monitor and manage multiple cars simultaneously.\n\n",
    "prize": "Winner Best of Real-time analytics",
    "techStack": "csv, gemini, google, javascript, three.js, txt",
    "github": null,
    "youtube": null,
    "demo": "https://gr86strategy.netlify.app/",
    "team": "Danielle Royer",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/gr-cup-real-time-dashboard-simulator"
  },
  {
    "title": "Little Critters - Advanced hand tracking update",
    "summary": "Little Critters is an innovative project that leverages advanced hand tracking technology to create an immersive gaming experience. Players interact with adorable, animated creatures in a virtual environment, using their hands to manipulate and engage with the game, enhancing user immersion and interactivity.",
    "description": "**What it does**\nThis update to Little Critters improves hand tracking support, taking things to the next level: Scene tracked surfaces including walls, floors and ceilings are used for extra context, e.g. players can squash enemies between their hands and a real world surface. Hitting at a grazing angle to the surface will knock enemies off the surface instead of squashing them.\nWe reworked all hand interactions including picking up, grabbing and throwing to be more accurate and responsive.\nAdded gestures and interactions that are only available when playing with hand tracking.\n\n**Inspiration**\nLast month we released Little Critters, a casual tower defense game built from the ground up for mixed reality. We designed the game around cutting edge mixed reality features including scene tracking to drive the gameplay and depth occlusion to blend the virtual with the real. Hand tracking is a great tool for immersing players in mixed reality. For the Meta Horizon Start competition we chose to build an advanced hand tracking update that refines and extends the hand tracking support in Little Critters. Our aim was to make it a showcase of what is possible and one of the best examples of hand tracking on the Meta platform.\n\n**How we built it**\nWe developed a custom gesture recognition system for greater accuracy to support the needs of the game. The system is calibrated with multiple different hand poses which then drives the game's general input system. Special rules cater to tracking loss, so that carried items aren't dropped if the player's hands go out of view of the tracking cameras. In addition, we developed different responses to physics collisions between the player's fingers and the enemy critters. The orientation of nearby real world surfaces from scene tracking and the velocity of the player's hand are combined to provide the appropriate response in the critter's behaviour.\n\n**Challenges**\nIn playtesting we noticed that when players pick up virtual objects, they use different hand poses, e.g. a thumb + index finger pinch, closing the fingers in a 'crab claw' pose, or a full fist grab. We initially expected played to use a pinch gesture, so we needed to refine our grab system to support these multiple different hand shapes.\n\n**Accomplishments**\nWe are especially proud of how we have managed to combine hand tracking with scene tracking to build an extensive range of new interactions that goes beyond the standard pinch and poke gestures.\n\n**What we learned**\nOur team learned to look beyond simply creating a drop in replacement to controllers, looking at ways to explore how hand tracking can offer new, unique interactions for the player. We also learned how to adapt to player's needs as they have different expectations for how they should be able to use their hands to interact with virtual objects.\n\n**What's next**\nWe are continuing to build content updates for Little Critters and will be regularly updating the title on the store!\n\n",
    "prize": "Winner Best Casual Game Runner-up",
    "techStack": "meta, mruk, quest, unity",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=i2qJPLwJRSw",
    "demo": "https://www.meta.com/experiences/little-critters/8692631140830732/",
    "team": null,
    "date": "2025-12-08",
    "projectUrl": "https://devpost.com/software/little-critters"
  },
  {
    "title": "Spell Swiper: Defend the Kingdom!",
    "summary": "Spell Swiper: Defend the Kingdom! is an interactive game designed to engage players in a magical defense scenario where they must use spells to protect their kingdom from invading forces. The project combines elements of strategy and action, allowing players to cast spells dynamically to thwart enemies.",
    "description": "**What it does**\nIn Spell Swiper: Defend the Kingdom! monsters drift toward your castle on enchanted balloons after being launched in the air by a nearby catapult. Each balloon has a weakness though, a shape that you can cast to pop them! This could be a circle, line, or zigzag shape, for instance. The core loop becomes a quick, satisfying rhythm of gesture ‚Üí feedback ‚Üí explosion as enemies flood in faster and faster. It‚Äôs simple to learn, but as more shapes and enemy variations appear, it transforms into a game of timing, awareness, and mastery. Tip: If you're having issues registering the straight line, vertical or horizontal, make sure that you make a longer line. It doesn't register short gestures very well.\n\n**Inspiration**\nSpell Swiper: Defend the Kingdom! started with a simple question: What if spell-casting was as satisfying as slicing fruit? We wanted a game that anyone could instantly understand: draw the shape, cast the spell, save the castle, but with enough hidden depth to reward skill and mastery. Mixing the kinetic fun of Fruit Ninja with the nostalgia of old-school rune-drawing games, we focused on creating a one-handed mobile experience that still makes you feel powerful. Examples of our iteration progress:\n\n**How we built it**\nThe game was developed entirely using Horizon Worlds‚Äô World Editor, with Noesis UI powering the title screen and leaderboard. Our earliest prototype was hilariously forgiving, any scribble counted as a ‚Äúspell,‚Äù which was fun for a minute but not a real game. So we built a custom gesture recognizer from scratch and iterated over it dozens of times until it felt fair, readable, and reliable. Once the core mechanic clicked, we layered in particle effects, floating animations, polished hit reactions, and a difficulty curve that moves from relaxing to chaotic. Light, optimized scripting kept things smooth on mobile, and we designed the entire experience around portrait mode for a true mobile-first feel.\n\n**Challenges**\nGesture recognition was our biggest boss battle. Tiny differences in line curvature or stroke order could make the game feel too strict or too easy. Balancing that element took a lot of playtesting. We also had to rethink traditional VR-friendly Horizon Worlds layouts because portrait mode compresses everything vertically. Reading action and incoming threats in a tall, narrow frame required major adjustments to camera placement, enemy spacing, and timing. Keeping the game visually exciting without tanking mobile performance is another ongoing challenge.\n\n**Accomplishments**\nWe created a spell casting system that feels magical. Shapes read cleanly, spells fire instantly, and enemies burst with satisfying feedback. New players can \"swipe-mash\" and survive for a while, but skilled players can hit a flowing rhythm that feels almost like drawing calligraphy. We‚Äôre also proud of pushing Horizon Worlds into a more mobile-native, portrait-optimized experience. It‚Äôs a platform where this type of game isn‚Äôt common, and seeing it run smoothly feels like opening a new door. We're eager to continue to explore the possibilities there.\n\n**What we learned**\nWe learned that gesture-based gameplay is incredibly sensitive; changing a circle‚Äôs tolerance by even a few pixels changes how ‚Äúfair‚Äù the game feels. We discovered how critical animation timing is, especially when players only have a small window of time to take in the input and react to it. Our biggest lesson? Simple ideas become great when you cut away the noise and sharpen the core fun, and hackathons should keep a focused scope in mind. Our previous submissions were larger in scope, but this time we focused on a tight, instantly enjoyable mechanic and that shift made a huge difference.\n\n**What's next**\nWe have a lot of ideas queued up: new enemy types, advanced shapes, and boss encounters that require chained gestures or multi-step spell combos. A progression system with unlockable spells and cosmetic upgrades is high on our list, as well as daily challenges and score competitions to keep players returning. Long term, we‚Äôd love to explore co-op spell slinging or competitive swipe battles. Spell Swiper: Defend the Kingdom! was intentionally built small and clean, which makes it the perfect foundation for future expansions.\n\n",
    "prize": "Winner Top Prize",
    "techStack": "blender, horizon, noesis",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=EayyOqGQxKM",
    "demo": "https://horizon.meta.com/world/10102098126185425",
    "team": "Scripting and moral support, 3D assets, QA",
    "date": "2025-12-08",
    "projectUrl": "https://devpost.com/software/spell-swiper"
  },
  {
    "title": "R3KON GPT",
    "summary": "TECHNICAL HIGHLIGHTS: Notable technical implementations include the use of Qwen for natural language understanding, allowing the system to accurately process and respond to user input. The deployment of Python for the backend logic ensures scalability and flexibility, while the Tkinter framework provides an intuitive graphical user interface that enhances user experience.",
    "description": "**What it does**\nR3KON GPT is a cybersecurity-focused offline AI assistant designed for low-connectivity regions.\nIt can:\nGive cybersecurity tips and safe-practice guidance Help detect suspicious behavior and give threat-awareness advice Function fully without Wi-Fi or stable internet Run as a standalone Windows .exe so anyone can use it instantly Offer general AI responses while prioritizing digital safety It‚Äôs built for users who need protection, not data drain.\n\n**Inspiration**\nIn many parts of Africa, internet access is expensive, unstable, or completely unavailable , but cybersecurity risks are still growing. I wanted to build an AI tool that doesn‚Äôt depend on constant connectivity, doesn‚Äôt expose user data to the cloud, and can provide real, practical guidance offline. That was the spark behind R3KON GPT: an AI designed for areas where cybersecurity support is needed most, but connectivity is weakest.\n\n**How we built it**\nPython for core logic\nAn offline-capable model packaged directly into the app\nMultiple cybersecurity-focused prompt layers and logic modules\nA Windows build system that bundles everything into one .exe\nLocal processing so users don‚Äôt need cloud access or API keys\nThe large file size comes from embedding all required AI and security components directly inside the executable ‚Äî making it fully functional offline.\n\n**Challenges**\nManaging the huge file size caused by offline AI models Packaging everything into a single .exe without dependency errors Ensuring the app runs on multiple Windows setups without missing DLLs Designing the AI to be helpful, safe, and security-aware offline Sharing the release safely through Google Drive without exposing personal files\n\n**Accomplishments**\nCreating an AI tool that works fully offline, something rare for cybersecurity assistants Building and shipping a functional 1.3GB AI executable at 19 Designing a system that can help users in areas with no Wi-Fi Adding another major product under Aethar Tech Proving that African-first AI tools can be powerful and independent\n\n**What we learned**\nOffline AI is powerful, but packaging it is challenging Cybersecurity tools need careful design and clear, safe guidance Building for low-connectivity environments requires a different mindset Managing large builds and dependencies is its own skill Releasing software safely matters as much as building it\n\n**What's next**\nA lighter version with modular downloads Stronger offline security features (local threat scanning, phishing detection) A cleaner UI and more stable packaging Mobile and web versions Integration with Nyx Browser for secure browsing + AI support Partnerships and pilots in schools, cyber clubs, and rural areas Turning R3KON GPT into a complete offline cybersecurity toolkit\n\n",
    "prize": null,
    "techStack": "artificial-intelligence, machine-learning, python, qwen, tkinter",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=CSFLJeiS6yk",
    "demo": "https://aethartech.itch.io/r3kon-gpt",
    "team": null,
    "date": "2025-11-19",
    "projectUrl": "https://devpost.com/software/r3kon-gpt"
  },
  {
    "title": "The Twin Earth",
    "summary": "TECHNICAL HIGHLIGHTS: The project utilizes several advanced tools, including DaVinci Resolve for video editing, ElevenLabs for audio generation, Epidemic Sound for music licensing, Higgsfield for data handling, Klingai for design, and Topaz Studio for image enhancement. This diverse tech stack showcases a sophisticated approach to content creation, integrating AI and advanced editing software to elevate the final product.",
    "description": "Inspiration:\n\nThe Twin Earth is a metaphor about envy, about how humanity often chooses to destroy what it does not understand, rather than learn from it.\nArtistically, the film draws inspiration from Futurism: its bold shapes, dynamic compositions, and the tension between technology, motion, and emotion.\nThe visual language echoes that aesthetic, transforming the story into a stylized, symbolic exploration of jealousy, fear, and human fragility.\n\nWorkflow: \n\nI created the frames using Midjourney and Kling 2.5 for the clips.\nI then upscaled the resolution to 4K with Topaz Studio and edited and color-graded the video in DaVinci Resolve.\nThe voice-over was created with ElevenLabs.\nThe music and sound design were downloaded and licensed from Epidemic Sound.\n\n\n\n        \n    Built With\n\n    davinciresolveelevenlabsepidemicsoundhiggsfieldklingaitopazstudio",
    "prize": "Winner Film ‚Äì 1st Place; Winner Gold Sponsor ‚Äì 3rd Place",
    "techStack": "davinciresolve, elevenlabs, epidemicsound, higgsfield, klingai, topazstudio",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=rxEnjG6GjfM",
    "demo": null,
    "team": null,
    "date": "2025-11-19",
    "projectUrl": "https://devpost.com/software/the-twin-earth"
  },
  {
    "title": "Vyx‚Äî AI-Powered Universal Content Repurposing Engine",
    "summary": "Vyx is an AI-powered engine designed to streamline the process of content repurposing across various formats and platforms. By leveraging advanced algorithms, it automates the transformation of existing content into new formats, making it easier for creators and marketers to reach diverse audiences efficiently.",
    "description": "**What it does**\nVyx transforms any YouTube video into a complete multi-platform content package in under 10 minutes:\n\n**Challenges**\nDual Pipeline Coordination - Users triggering both pipelines simultaneously caused resource conflicts. We implemented separate webhook endpoints with dedicated node paths and health check monitoring. FFmpeg Subtitle Timing - Clips from middle of videos had incorrect timestamps. AI now re-generates subtitles starting from 00:00:00 for each clip, ensuring perfect sync. Cloudinary Rate Limiting - Uploading 5 clips simultaneously triggered limits. We implemented sequential processing‚Äîclips process one at a time through upload stage. Temporary File Cleanup - Failed workflows left gigabytes of video files. Added mandatory cleanup nodes that execute regardless of workflow success. CORS Preflight Handling - Browser OPTIONS requests were blocked. Created dedicated OPTIONS webhook handlers with prop\n\n**What we learned**\nMulti-Pipeline Architecture is Powerful - Building two distinct pipelines within one system taught us that modular design allows users to choose their workflow while sharing common infrastructure. AI Quality Depends on Multi-Step Processing - Moving from a single AI call to a 5-step pipeline increased content quality by 95%. Each step builds on the previous, creating compound improvements. Speed Matters - Switching to Pollinations.ai (instant) reduced total pipeline time by 40%. Users value speed over marginal quality differences‚Äî5-7 minutes vs 10-15 minutes dramatically improves UX. Video Processing is Resource-Intensive - Temporary file management is critical, batch processing needs rate limiting, and 9:16 vertical format requires precise cropping calculations. N8N Scales for Production\n\n**What's next**\nA/B Testing Recommendations using historical performance data\nScheduled Content Calendar for entire month\nLive Video Processing for ongoing livestreams\nAnalytics Dashboard tracking AI-generated content performance\nPodcast Support (Spotify, Apple Podcasts)\nTeam Collaboration with role-based access\n\n",
    "prize": "Winner Nexus Champion (Grand Prize)",
    "techStack": "ai, ffmpeg, n8n, next, pollination, yt-dlp",
    "github": "https://github.com/NabilThange/omni",
    "youtube": "https://www.youtube.com/watch?v=WMODqeRTyLM",
    "demo": "https://vyx.vercel.app/",
    "team": "Yojith Rao",
    "date": "2025-11-30",
    "projectUrl": "https://devpost.com/software/omni-ai-powered-universal-content-repurposing-engine"
  },
  {
    "title": "AI enterprise platform",
    "summary": "The AI enterprise platform leverages advanced machine learning and AI technologies to streamline business operations, enhance decision-making, and improve overall efficiency within organizations. By integrating various tools and frameworks, the platform provides customizable solutions tailored to specific enterprise needs.",
    "description": "The Problem: Generic AI fails because it lacks context and access to your siloed systems.\nThe Solution: Instead of one confused chatbot, we deploy federated agents that each \"live\" in their domain (HR, Finance, Documents), speak their language, and respect their security rules. They share a unified interface but operate independently‚Äîlike a team of experts under one roof.\nExample:\n\"Who can cover this Python project?\"\n‚Üí HR Agent queries your employee DB (skills, tenure, availability)\n‚Üí Document Agent scans project files for Python requirements\n‚Üí They synthesize an answer without you opening three tools or exposing data to a public model.\n\n\n\n        \n    Built With\n\n    agentcorebedrockfastapipythonreact\n  \n\n        \n    Try it out\n\n    \n        \n  \n  GitHub Repo\n\n        \n  \n  enterprise-ai-landing-page.netlify.app",
    "prize": "Winner 2nd Place",
    "techStack": "agentcore, bedrock, fastapi, python, react",
    "github": "https://github.com/pramodthe/enterprise-ai-platform/tree/main",
    "youtube": "https://www.youtube.com/watch?v=d6G1MZlqGzI",
    "demo": "https://enterprise-ai-landing-page.netlify.app/",
    "team": "pramod thebe",
    "date": "2025-11-19",
    "projectUrl": "https://devpost.com/software/ai-enterprise-platform"
  },
  {
    "title": "LeetCourt",
    "summary": "TECHNICAL HIGHLIGHTS: The project employs a sophisticated tech stack, including Node.js for backend services, React for a dynamic frontend, and TypeScript for type safety. Additionally, the integration of ElevenLabs suggests advanced AI capabilities, possibly for code analysis or real-time feedback, while Tailwind CSS ensures a modern and responsive design.",
    "description": "**What it does**\nLeetCourt is an AI-driven, voice-interactive courtroom simulator that allows users to: Conduct realistic Opening, Direct, Cross, and Closing phases.\nFace live objections such as Relevance, Hearsay, Speculation, and Leading.\nUpload any PDF case file and have an AI extract facts, evidence, and precedents.\nReceive performance analysis at the end of the game.\nTrack clarity, logic, persuasiveness, and precedent usage.\nInteract with a fully voice-enabled multi-agent AI judge, Lawyer, and Orchestrator built using ElevenLabs Conversational AI.\nAccess an integrated tools panel with evidence, notes, and case summaries.\nCreate secure, isolated development sandboxes for AI-powered legal case analysis using Daytona.io \nReceive automated code reviews and logic checks for scoring engine and agent scripts\n\n**Inspiration**\nLegal argumentation is a skill that traditionally requires in-person training, expert feedback, and structured courtroom environments, none of which are easily accessible to most students or professionals. Despite the rise of AI tools for writing and research, there is still no platform that allows users to practice real-time courtroom reasoning, face objections, or learn procedural flow in a fun and engaging way. We wanted to build something that feels like LeetCode for legal reasoning: a place where anyone can practice arguments, improve rhetorical clarity, and simulate real trial pressure with an AI judge that behaves like the real thing. This project was inspired by a gap we repeatedly saw among law students, debaters, and public speakers: they have theory, but no way to practice execu\n\n**Challenges**\nLatency management: synchronizing real-time speech with LLM responses required heavy optimization and caching.\nMaintaining consistent judge behavior: the judge must be strict, procedural, and predictable‚Äîrequiring extensive prompt engineering and reinforcement rules.\nHandling messy PDFs: scanned or irregular files often broke extraction, forcing us to implement fallbacks and preprocessing.\nConversation drift: without a strict FSM, courtroom phases would blur and confuse the model; designing the conversation structure was non-trivial.\nIntegrating multiple models: coordinating judge logic, scoring, extraction, and agent workflows without conflicting context required careful orchestration.\n\n**Accomplishments**\nBuilt a fully voice-interactive courtroom simulator with realistic objections.\nCreated a dynamic PDF-to-case extraction pipeline that works on nearly any legal document.\nSuccessfully implemented a real-time scoring engine that analyzes user arguments every 3 seconds.\nDeveloped a clean, intuitive UI that mirrors professional trial environments.\nDesigned a flexible case library that supports search, filtering, uploading, deletion, and metadata.\nCreated a scalable architecture that can support new agents, new case types, and multiplayer scenarios.\nLeveraged Daytona.io to create secure, isolated development sandboxes for AI-powered legal case analysis.\nUsed CodeRabbit to automate code quality checks and ensure scoring logic correctness.\nTigris Storage for storing legal case documents for pract\n\n**What we learned**\nReal-time voice interactions require more than LLM prompts‚Äîthey need architectural discipline, buffering, and predictable flow control.\nLegal reasoning benefits from structure: an FSM model greatly improves accuracy and realism.\nLLMs perform best when each is assigned a specific, narrow function (judge, analyzer, extractor).\nUser experience matters: trial simulations need pacing, clarity, and visual anchors to feel authentic.\nHigh-quality prompts aren‚Äôt enough‚Äîenvironment design determines how well agents behave.\n\n**What's next**\nMultiplayer Mode: human vs. AI opposing counsel or human vs. human with an AI judge.\nMobile App for on-the-go argument practice.\nCommunity Case Marketplace for user-submitted scenarios.\nInstitutional versions for law schools, debate clubs, and training programs.\nAdvanced scoring models that learn from user patterns.\nAnalytics dashboard showing improvement over time.\nScenario-based learning tracks (Criminal Law, Torts, Contracts, Evidence, Constitutional Law). LeetCourt aims to become the first platform that makes courtroom reasoning truly accessible, interactive, and measurable.\n\n",
    "prize": "Winner 3rd Prize; Winner Best Use of ElevenLabs",
    "techStack": "coderabbit, daytona, elevenlabs, node.js, openrouter, react, tailwind, typescript",
    "github": "https://github.com/ayushs2k1/leetcourt/tree/leetcourt-daytona-sprint-hack-nyc",
    "youtube": "https://www.youtube.com/watch?v=VFsn7VtvEpU",
    "demo": "https://luminous-fox-3d91e8.netlify.app/",
    "team": "Ayush Sharma",
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/leetcourt-unej6r"
  },
  {
    "title": "AUTO-OPS: AI DevOps Agent",
    "summary": "AUTO-OPS: AI DevOps Agent is an innovative solution designed to automate DevOps processes using advanced AI technologies. It streamlines software development and deployment workflows, enhancing efficiency and reducing the need for manual intervention by leveraging integrations with various APIs and development tools.",
    "description": "**What it does**\nAUTO-OPS automatically detects runtime errors from any Python project, analyzes the code, generates a patch, tests it safely in a Daytona sandbox, and opens a pull request ‚Äî all without human intervention.\nIt logs traces in Galileo, generates code and reasoning using Anthropic‚Äôs Claude, tests fixes inside Daytona environments, stores everything in Tigris, gets CodeRabbit reviews automatically, and even speaks a summary of what it fixed using ElevenLabs. In short, it‚Äôs an autonomous software engineer that handles your crashes end-to-end.\n\n**Inspiration**\nWe wanted to build something that eliminates the ‚Äúdebug fatigue‚Äù developers face ‚Äî when your code breaks and you spend hours tracing logs, fixing one bug just to introduce another. With AI tools evolving fast, we saw an opportunity to let an autonomous agent not only understand code errors but actually fix them safely in real time. That vision became AUTO-OPS, an AI DevOps agent that acts like your tireless coding teammate.\n\n**How we built it**\nFrontend: Next.js + Tailwind dashboard showing incoming errors and a live ‚Äúagent console‚Äù timeline.\nBackend: Node/TypeScript API routes handling error intake, Claude prompts, Daytona sandbox orchestration, and GitHub PR creation.\nPython client: A lightweight wrapper that catches exceptions and POSTs traceback + source code to AUTO-OPS.\nAPIs integrated:\n\n\nüß† Anthropic Claude ‚Üí analyzes stack traces & generates code patches\nüß∞ Daytona ‚Üí spins up isolated test environments and runs patched code\nüêá CodeRabbit ‚Üí automatically reviews created PRs\nüìà Galileo ‚Üí logs every crash trace and agent decision\nüíæ Tigris Data ‚Üí stores incidents, logs, and patches for long-term memory\nüó£Ô∏è ElevenLabs ‚Üí turns the AI‚Äôs summary into a spoken ‚Äúfix report‚Äù Everything runs through a coordinated agent loop managed\n\n**Challenges**\nCoordinating six separate APIs under one asynchronous workflow.\nKeeping Claude‚Äôs reasoning ‚Äúsafe‚Äù so it never writes destructive patches.\nManaging Daytona environment creation and teardown efficiently under time limits.\nGetting real-time error feeds to sync smoothly between Python and the dashboard.\nHandling API rate limits and secret management without breaking demo reliability.\n\n**Accomplishments**\nBuilt a fully autonomous debugging pipeline that touches every partner API.\nAUTO-OPS can realistically detect, fix, test, and commit a live Python error.\nThe project runs end-to-end in minutes with clear transparency via the console UI.\nCreated a voice-enabled demo where the AI literally talks you through the fix.\nClean architecture that other developers can extend to JavaScript or Go next.\n\n**What we learned**\nHow to combine multiple AI systems ‚Äî Claude, CodeRabbit, ElevenLabs ‚Äî into a cohesive workflow that doesn‚Äôt collapse under API latency.\nThe importance of sandboxed testing before applying AI-generated code changes.\nThat strong observability (Galileo + Tigris) is essential when you give autonomy to code-writing agents.\nBuilding fast is easy; building safe and explainable AI systems is the real challenge.\n\n**What's next**\nIntroduce real-time VS Code integration so the agent appears as an in-editor assistant.\nBuild team dashboards for organizations to monitor agent performance across repos.\nIntegrate LLM fine-tuning using historical Tigris logs to make AUTO-OPS learn your project‚Äôs coding style.\nExpand sandbox orchestration to CI/CD pipelines for true self-healing infrastructure. AUTO-OPS isn‚Äôt just a hackathon project ‚Äî it‚Äôs a prototype for the future of hands-free software maintenance.\n\n",
    "prize": "Winner Best Use of CodeRabbit",
    "techStack": "anthropic-claude-api, coderabbit, daytona, elevenlabs-api, github, intel-galileo, next.js, node.js, python, react, rest, tailwind-css, tigris-data, typescript",
    "github": "https://github.com/Soulemane12/AUTO-OPS-AI-DevOps-Agent",
    "youtube": "https://www.youtube.com/watch?v=iV6OdexdXec",
    "demo": "https://auto-ops.vercel.app/",
    "team": "Soulemane Sow",
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/auto-ops-ai-devops-agents"
  },
  {
    "title": "Virgo's Whisper AI (VWAI)",
    "summary": "Virgo's Whisper AI (VWAI) is an advanced voice recognition and transcription tool that leverages artificial intelligence to convert spoken language into written text accurately and efficiently. The project aims to enhance communication accessibility and facilitate seamless interaction between users and technology, making it particularly beneficial for individuals with hearing impairments or those in noisy environments.",
    "description": "Our Story: Building Virgo's Whisper AI\n\nThe Inspiration\n\nThis project was inspired by the incredible cognitive load that first responders face every day. They are trained professionals, but they are also human. In a high-stress crisis, they have to manage their own stress, communicate with dispatch, remember complex, multi-step protocols, and take mental notes, all at the same time.\n\nI wanted to build a tool not to replace their training, but to act as a hands-free \"co-pilot\" or an external \"short-term memory.\" The goal was to build an AI that could passively listen, reduce cognitive load, and provide calm, clear, one-step-at-a-time guidance, allowing the officer to focus on the situation in front of them.\n\nThe Initial Plan & The \"Brick Wall\" (The Challenges)\n\nMy initial plan for  hackathon was to use the stack: LiquidMetal AI, Vultr, and Cerebras. I immediately hit a wall.\n\nThe Credit Card Blocker: The Vultr services required a credit card, which I did not have.\n\nThe Tech Blocker: The LiquidMetal hosting required WSL (Windows Subsystem for Linux), which was broken on my system and, after hours of debugging, remained unfixable.\n\nThis project is the story of that pivot. How do you build a powerful, low-latency, real-time AI application with zero budget and critical technical blockers? This forced a complete redesign and led to the \"No Credit Card\" stack: PythonAnywhere, Firebase, AssemblyAI, Cerebras, and ElevenLabs.\n\nHow We Built It: The Technical Journey\n\nThe project was built in three main phases, with each new version solving a critical bug from the last.\n\nv1.0: The \"Brain\" and \"Ears\"\n\nI started by building the \"brain\" (a Flask app) on PythonAnywhere and the \"memory\" (Firestore) on Firebase. The first challenge was just getting them to talk to each other. After debugging file paths, CORS policies, and environment variables, the server was live.\n\nThe next step was integrating the \"ears\" (AssemblyAI). We hit our second major blocker: the webhook system was unreliable. My server logs showed that AssemblyAI was transcribing the audio but wasn't sending the text back to our server.\n\nThe solution was to refactor the entire logic. Instead of the server passively listening for a webhook, I made it proactive. The final architecture has the client (our web demo) upload the audio file directly to our Flask server. The server then calls AssemblyAI and waits for the transcription. This \"direct control\" model was the first major breakthrough and proved far more reliable.\n\nv2.0: The \"Intelligence\" and \"Voice\"\n\nWith a stable transcription pipeline, I added the \"intelligence\" (Cerebras) and \"voice\" (ElevenLabs). We added our first features:\n\nStress Detection: The AI would analyze any passive speech for stress and respond with \"Deep breath. Focus.\"\n\nSummarization: The AI would listen for \"Virgo, summarize comms,\" query Firebase for all recent chatter, and send it to Cerebras to be summarized.\n\nThis worked, but we quickly realized a \"one-and-done\" answer isn't a",
    "prize": null,
    "techStack": "ai, assemblyai, cerebras, cerebras-cloud-sdk, css3, elevenlabs, es6+), firebase, firebase-admin, firestore, flask-cors, generation:, javascript, python-3-flask-html5, python-dotenv, pythonanywhere, transcription, voice",
    "github": "https://github.com/Valhallanxxx/Virgo-s-Whisper-AI.git",
    "youtube": "https://www.youtube.com/watch?v=kPvWO0YECD0",
    "demo": null,
    "team": "Shashwat Balodhi",
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/virgo-s-whisper-ai-vwai-5isedc"
  },
  {
    "title": "AILien",
    "summary": "TECHNICAL HIGHLIGHTS: The project was built using C# and integrated with Convai for natural language processing, enabling dynamic conversations with AI characters. Additionally, it utilized Meta's SDK and VR Optimiser by Valem to optimize the virtual reality experience, ensuring smooth and immersive interactions.",
    "description": "**How we built it**\nWe used the Convai NPC AI Engine to create a voice character that had the personality and responses of a cosmic oracle. We used MetaSDK incorporate this into a Unity scene. We also used the VR Optimiser by Valem to speed up the final stages of development. The assets include 3D scans of the actual Knyahinya Meteor, environmental design assets using hand drawn sketches that were converted to 3D models using AI and an original score and sound design.\n\n**Challenges**\nThe first challenge we encountered was controlling the scope of the project. The ideas for narrative and interactive elements kept expanding and so we had to focus and make sure that we were aligned on the core features of the experience. We had to adapt to the time limits and experience of our team and judge what was realistic within the timeframe without losing the heart of the project. Another key challenge was working with the ConvAI NPC Engine, it doesn‚Äôt immediately integrate with the MetaSDK so with the support of our mentors we had to go into the code and start manually changing elements.\n\n**Accomplishments**\nWe‚Äôre really pleased with the level of optimisation we were able to achieve in the amount of time. The NPC AI Engine slowed the experience considerably, creating a loading screen every time the microphone was turned on. Through editing this API and extensive visual optimisation we were able to reduce the delays. This was our first time using the Convai Engine, this took considerable amount time to come to grips with but we‚Äôre really excited to explore this tool further.\n\n**What's next**\nWe have many ideas for expanding AILien., One step would be to expand the narrative elements of the experience including the descent of the asteroid. This would include more scenes and more complex interactions. We‚Äôre also interested in using spatial anchors to have the meteor align with physical objects in the room e.g the meteor aligns with a beanbag. This would also include using passthrough and reskinning the tracked objects in the room to match the environment e.g pillars become trees. For wider expansion, we would be interested in creating more characters with different personalities that can increase the interaction. We also feel multiplayer could add an interesting layer of interaction.\n\n",
    "prize": "Winner Best Immersive Entertainment Experience by Meta 1st Place",
    "techStack": "c#, convai, metasdk, vroptimiserbyvalem",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=OJZghDpvQsQ",
    "demo": "https://drive.google.com/drive/folders/1uydJcDCLZ0Yim0uzyOA5yEzf2W4CGdLi?usp=share_link",
    "team": "Beth Lewis, Letta Shtohr, Jacqueline Montenegro",
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/ailien"
  },
  {
    "title": "[B04] Hander Attack",
    "summary": "Hander Attack is an immersive mixed and virtual reality experience that combines interactive storytelling with advanced graphics. The project leverages state-of-the-art technologies to create a compelling environment where users can engage in unique challenges, enhancing their overall gaming experience.",
    "description": "**What it does**\nHander Attack is a mixed reality, hand-tracking multiplayer game for Meta Quest where your own room becomes the arena. First, the app scans your room and generates a minimap of it. Then, both minimaps are swapped, giving each player a miniature representation of the opponent‚Äôs room. Small interactions on the minimap have immediate consequences in the mixed-reality space (and vice versa), turning it into a real-time chase where one player has to hide while the other tries to catch them. The VFX and audio react to events in the room, reinforcing the immersion, social presence, and the fun of the game! The result is a playful bird's-eye view experience where you try to find and catch the other player hiding in their room using their minimap.\n\n**Inspiration**\nThe original idea of Hander Attack comes from the concept of the digital twin. A digital twin creates a live, data-driven mirror of a physical space, allowing you to monitor, analyse, or even control what happens there. We translated this idea into gameplay through a simplified version of a digital twin: an interactable minimap of a room. Traditionally, minimaps only help you navigate the world, not influence it. In Hander Attack, the minimap becomes the controller, exploring how interacting with a representation of a space can affect the real world.\n\n**How we built it**\nWe built the project using Unity 6, C#, and Universal Render Pipeline for the rendering, gameplay logic and all interaction systems. For room scanning, we relied on the Meta MR Utility Kit, which provided us with a JSON representation of the environment. This representation was then used to reconstruct the minimap and collision geometry. Additional core features such as basic hand tracking and pose recognition were implemented using the Meta Core SDK, and Photon was used to explore multiplayer capabilities. For 3D models and animations, we used Blender along with Unity‚Äôs timeline and Director tools to handle the full asset production pipeline. Hand tracking was introduced using the Meta MR Utility Kit and we implemented pose recognition. For optimized visual effects that collide with Unity\n\n**Challenges**\nMaking it feel like it‚Äôs ‚Äúhappening in front of you‚Äù Because the experience is fully based on hand tracking, we couldn‚Äôt rely on controllers or haptics to sell impact. We had to design VFX, audio, and timing very carefully so that every interaction looks and sounds punchy enough to feel physical, even without vibration. Synchronizing room scans between players On the multiplayer side, the most difficult part was synchronizing the scanned minimaps between players. After a room scan, the device generates a JSON file, so we rebuilt the room layout from that JSON and used it as the shared reference. We also needed a reliable way to send controller positions: we first converted the controller‚Äôs world position into the minimap‚Äôs local space, transmitted that data, and then reconstructed the corr\n\n**Accomplishments**\nWe are proud of implementing a functioning, fun, and aesthetic game that integrates an end-to-end pipeline from conception to the delivery of a polished prototype. We are proud of how clearly Hander Attack communicates the idea of interacting with a digital twin through a minimap while still being entertaining and engaging.\n\n**What we learned**\nThrough this project, we learned a lot about how powerful a minimap can be as an interaction tool in mixed reality. It was especially interesting to see how small actions on the minimap could have an immediate impact in another player‚Äôs room in real time, creating a strong sense of shared presence. We also gained a deeper understanding of the design constraints of hand-tracking‚Äìonly experiences. Without haptics, every piece of feedback has to be conveyed through visuals, sound, and timing. This forced us to think more carefully about how to communicate impact, intention, and responsiveness through VFX and audio alone. Finally, working with room scanning, JSON-based reconstruction, and synchronizing local and world spaces across devices taught us a lot about the technical and UX challenges\n\n**What's next**\nLooking ahead, the idea of interacting with distant spaces through a minimap opens up a wide range of possibilities. A small virtual model of a remote environment can act as a powerful control interface, allowing users to manipulate objects, guide other people, or coordinate shared tasks without physically being there. This approach could be extended far beyond simple room interactions: remote assistance, collaborative design, education, and even large-scale simulations all become more intuitive when the minimap itself becomes the main tool for shaping the world. By refining the accuracy of room reconstruction and improving cross-device synchronization, we imagine a future where users can seamlessly interact with completely different locations as if they were miniature worlds resting in th\n\n",
    "prize": "Winner Mixed & Virtual Reality Category by Meta Runner-Up",
    "techStack": "adobe, aftereffects, blender, c-sharp, elevenlabs, metasdk, photon, unity, vegaspro",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=moVZ8fKZOOc",
    "demo": null,
    "team": "Roger Montserrat, Alex Fuentes Ravent√≥s, Reiya Itatani, Georgios Tsampounaris, Lisa Izzouzi",
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/b04-hander-attack"
  },
  {
    "title": "Kitchen Futures ‚Äì Interior Design Showcase Platform",
    "summary": "Kitchen Futures is an innovative platform designed to showcase interior design concepts specifically for kitchen spaces. It enables users to explore various design styles, layouts, and functionalities, allowing them to visualize and personalize their kitchen environments through an interactive interface.",
    "description": "**What it does**\nThe platform professionally showcases: Kitchen makeovers\nInterior decoration concepts\nServices & offerings\nA high-quality image gallery Everything is structured to highlight visuals, build trust, and give users an effortless browsing experience.\n\n**Inspiration**\nInterior designers in my region rely heavily on WhatsApp images, outdated layouts, and slow websites that don‚Äôt reflect their craftsmanship. I wanted to build something that feels like 2025: clean, responsive, visually immersive, and conversion-focused.\n\n**How we built it**\nI built the platform from scratch and focused on: Clean UI/UX architecture\nReusable React components\nSmooth animations and transitions\nPerformance optimization\nFully responsive layouts for all screen sizes I ensured consistent branding, fast loading, and a polished viewing experience for both mobile and desktop users.\n\n**Challenges**\nMaintaining a premium design without sacrificing load time\nHandling large, high-resolution gallery images\nEnsuring visual consistency across all sections\nMaking ultra-wide layouts and mobile designs both look equally strong\n\n**What we learned**\nThis project strengthened my skills in: Real-world UI/UX decision making\nComponent-driven architecture\nPerformance tuning\nResponsive & accessible design\nManaging client requirements and revisions\n\n**What's next**\nPlanned upgrades include: 3D kitchen visualizer\nBefore/after interactive slider\nAuto-generated design styles\nAdmin dashboard for easy content updates\n\n",
    "prize": null,
    "techStack": "es6+), framer-motion, javascript, react.js, tailwindcss, vite",
    "github": null,
    "youtube": null,
    "demo": "https://thekitchen-futures.vercel.app/",
    "team": null,
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/kitchen-futures-interior-design-showcase-platform"
  },
  {
    "title": "Relaxing Garden",
    "summary": "TECHNICAL HIGHLIGHTS: Built using Meta‚Äôs technologies alongside Python, Unity, and Quest, the project features seamless integration of AI algorithms for personalized garden creation, real-time camera integration for user interaction, and visually appealing graphics that enhance the immersive experience.",
    "description": "**What it does**\nRelaxing Garden translates your heart rate and heart rate variability into a dynamic, mixed-reality environment with a personal touch. Users can draw their own leaf shape on paper, which the system detects and incorporates into the plants that flourish in their AR garden. As you relax, your custom leaves unfurl, new plants sprout, and the digital landscape grows increasingly lush and vibrant. When your arousal levels rise, growth slows, providing intuitive feedback on your body‚Äôs stress response. This experience feels playful, not clinical, making relaxation practices approachable for beginners and engaging for experienced mindfulness users alike.\n\n**Inspiration**\nWe were inspired by the idea that biofeedback doesn‚Äôt need to feel clinical or intimidating. Augmented reality provides a uniquely soothing space where physiological signals become gentle, visual experiences. We realised that by combining AR with heart rate data, we could transform stress-management sessions into something beautiful, immersive, and truly relaxing. Instead of charts and numbers, users see a garden that reflects their inner state. A calming environment that promotes slow breathing and emotional balance.\n\n**How we built it**\nWe connected a heart rate monitor and used both HR and HRV measurements to calculate a continuous relaxation score. This score reflects moment-to-moment changes in physiological arousal. We developed a lightweight Flask API to stream this data into a Unity-based AR environment, where custom growth shaders and procedural generation logic respond to the user‚Äôs relaxation state in real time. As a result, the garden changes fluidly as the user breathes, focuses, and relaxes.\n\n**Challenges**\nOne of the challenges was accurately detecting and rendering the shapes of the leaves. Ensuring the organic outlines stayed crisp, responsive, and natural required fine-tuning both the procedural geometry and the smoothing of sensor input. We also needed to avoid sudden growth jumps, calibrate HRV thresholds for different users, and maintain a tranquil atmosphere, even when data fluctuated rapidly.\n\n**Accomplishments**\nWe‚Äôre proud that Relaxing Garden feels peaceful from the moment it loads. It‚Äôs a space where technology truly supports wellbeing. The garden grows in a way that is both vibrant and unobtrusive, and the biofeedback loop functions reliably for all users. We‚Äôre especially pleased with how seamlessly physiological data integrates into the AR visuals, resulting in a calming experience that feels both magical and scientifically grounded.\n\n**What we learned**\nWe learned how sensitive HR and HRV signals are, and how much careful smoothing and interpretation they need. We also found that users respond strongly to subtle design choices: soft motion, gentle colours, and gradual growth patterns significantly enhance feelings of calm. Most importantly, we discovered that biofeedback is much more engaging when presented as a soothing interaction rather than as a performance metric.\n\n**What's next**\nNext, we plan to expand the garden‚Äôs ecosystem by incorporating a wider variety of plant species, introducing seasonal environments, and enabling user-customizable growth patterns. We are also exploring deeper personalisation through baseline HRV profiles and adding support for more sensors, such as respiration belts and skin conductance monitors. Additionally, we hope to introduce shared ‚Äúrelaxation sessions,‚Äù allowing multiple users to cultivate a peaceful space together. Our goal is to continue transforming physiological data into visual, motivating, and genuinely helpful tools for wellbeing.\n\n",
    "prize": "Winner AI with Camera Access by Meta Runner-Up",
    "techStack": "meta, python, quest, unity",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=NaNxa7DFRBo",
    "demo": "https://drive.google.com/drive/folders/1B9n-WyV1XdSxKscd6zOjkj7jhNLr8vMd?usp=share_link",
    "team": "Tobias Furtlehner, Aruaci, Sofiia-Khrystyna Borysiuk, Georg Becker",
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/relaxing-garden"
  },
  {
    "title": "NOOX AI hardware device",
    "summary": "NOOX AI is a hardware device that leverages the capabilities of Arduino and ESP32 to integrate artificial intelligence functionalities in a compact and efficient manner. It aims to provide intelligent solutions for various applications, potentially enhancing user interaction and automating processes in everyday tasks.",
    "description": "12    \n\n\n\n      \n  NOOX: An Open-Source AI-Driven Desktop Automation Device\n\nInspiration\n\nNOOX began with a simple question:\nLLMs can plan and reason‚Äîbut why can‚Äôt they directly act on our computers?\n\nI wanted a small, open, portable device that could:\n\n\nControl a computer via HID\nRun cross-platform shell commands\nTalk to LLMs\nPerform multi-step automation autonomously\n\n\nThis became the foundation for NOOX, an ESP32-S3‚Äìbased intelligent hardware platform merging peripherals, desktop automation, and AI planning.\n\n\n\nWhat I Built\n\nNOOX integrates:\n\n1. Peripheral Hardware\n\n\nUSB HID keyboard/mouse emulation\nUSB CDC bidirectional channel\nOLED screen + 3 buttons\nGPIO + RGB LED\n\n\n2. Desktop Automation\n\nA Go-based Host Agent (auto-downloaded via HID) runs shell commands on Windows/Linux/macOS and communicates using JSON over USB CDC.\n\n3. AI Autonomous Planning\n\nWith OpenAI/DeepSeek/OpenRouter support, the LLM can:\n\n\nPlan multi-step tasks\nCall tools like run_command, hid_keyboard_type, gpio_set\nIterate until a goal is complete\n\n\nThe system can perform tasks like:\n\n\n‚ÄúCreate a file, populate it, compress it, and open the folder.‚Äù\n\n\n\n\nHow I Built It\n\n\nFirmware: Arduino framework + ESP-IDF + FreeRTOS\nStorage: LittleFS for configs, web UI, and host agent\nUI: WebSocket-driven dark-theme console (Chat / Advanced Mode)\nMemory: Heavy JSON/LLM handling stored in 8MB PSRAM\nHost Agent: Cross-platform BLOB served from ESP32 and executed automatically\nHardware:\n\n\nESP32-S3-WROOM-1\nSSD1315 OLED (I¬≤C)\nPMOS USB/battery auto-switching\nButtons + LEDs + WS2812\n\n\n\n\n\nWhat I Learned\n\n\nHow to manage embedded memory to avoid fragmentation\nDesigning reliable USB HID sequences and timing\nCrafting a CDC JSON protocol resilient to truncation and slow hosts\nBuilding a WebSocket-driven UI with real-time streaming\nStructuring LLM tool-calling for predictable autonomous behavior\n\n\n\n\nKey Challenges\n\n\nGetting HID scripts to reliably bootstrap PowerShell across systems\nParsing large LLM JSON responses within ESP32 memory limits\nHandling differences between PowerShell, pwsh, cmd, bash, and sh\nAvoiding WebSocket race conditions with multiple clients\nEnsuring autonomous planning doesn't become unsafe\n\n\n\n\nSecurity Notes\n\nBecause AI can run shell commands and simulate keyboard input, NOOX should never be executed with admin/root privileges. A VM or sandbox is recommended for experimentation.\n\n\n\n* Final Thoughts**\n\nNOOX was an exploration of what happens when you combine embedded hardware, desktop automation, and modern LLMs into a single portable device.\nI learned deeply across hardware, firmware, software, and AI orchestration‚Äîand the result is a platform that lets AI literally act on your computer.\n\n\n\n        \n    Built With\n\n    arduinoesp32freertosplatformio\n  \n\n        \n    Try it out\n\n    \n        \n  \n  GitHub Repo\n\n        \n  \n  oshwhub.com",
    "prize": null,
    "techStack": "arduino, esp32, freertos, platformio",
    "github": "https://github.com/hurricane-0/NOOX",
    "youtube": "https://www.youtube.com/watch?v=BV11TCvB2EbS",
    "demo": "https://oshwhub.com/hgyzqxt/noox",
    "team": "h hric",
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/noox-ai-hardware-device"
  },
  {
    "title": "cAIne",
    "summary": "TECHNICAL HIGHLIGHTS: cAIne employs a sophisticated tech stack, including C# for development, Unity for creating immersive environments, and YOLO (You Only Look Once) for real-time object detection. The integration of these technologies enables seamless interaction and responsiveness within the augmented reality framework, making it a standout technical accomplishment.",
    "description": "**Inspiration**\nIf you have someone next to you, look at them. Are they wearing glasses, or are you? Chances are that's the case. Approximately sixty-five percent of the world has some form of vision correction, and half of those individuals have very low-vision, and half of that population has no vision. \nXR provides a unique opportunity of an evolved form of vision correction, mixing Context-Aware AI and haptic feedback to create a new cane - a cAIne. Using the Quest's passthrough and camera access, we can scan an environment in real time, as well as with the Meta RayBans. This calculates distances and providing instant responses for the end user, and provides an intuitive gesture based UI for hand tracking, or vibrational feedback for controller use, allowing a user to decide which feels more comfortab\n\n**Challenges**\nFiguring out the right AIs to work with our project and getting them to mesh in the way we wanted was difficult. This was made all the more difficult as not all AIs that we were going to utilize are available at this time, and we had to use substitutions. The time constraints made us limit our scope and cut features we would have loved to include. Another challenge was finding audio distinct enough and yet not too sharp for the cAIne's feedback.\n\n**Accomplishments**\nWrangling together various image and spatial recognition AIs was an exhilarating challenge. We've managed to run a local Yolo model directly on the Quest 3, providing very fast response. With a small team, succeeding in making the cAIne register these features and objects was a rewarding endeavor.\n\n**What's next**\nAs this software's goal is to be assisting low-vision/Blind individuals, our next stage goal is to implement image-to-speech for real time navigation. We will also fine-tune the way that distance is represented by the cAIne and improve our audiography. Another thing will be to add more pre-set objects that our AI can identify, increasing from 80 immediate recognition objects to possibly hundreds. A major future goal would be to implement more wearable technology, such as glove controllers with a larger array of feedback for both mobility, ease of access, and combining the current trade off that the controller vs hand tracking has, thus negating the issue.\n\n",
    "prize": "Winner Accessibility",
    "techStack": "c#, meta, mr, mruk, passthrough, unity, xr, yolo",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=iq5YVgHmPSs",
    "demo": "https://drive.google.com/file/d/1yX8inXNNnou6Cp58h6CeG2t_REBDox4J/view?usp=drive_link",
    "team": "Sophie Zw√∂lfer",
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/caine"
  },
  {
    "title": "Remember Me",
    "summary": "TECHNICAL HIGHLIGHTS: Remember Me utilized a range of cutting-edge technologies such as AI voice synthesis from Eleven Labs, image generation from Midjourney, and collaboration tools from Google and others. This combination allowed for seamless integration of voice, visuals, and narrative, enhancing user engagement and creativity.",
    "description": "**What it does**\nIt starts off a story of betrayal, family and resilience. And some fun future meets past meets dynasties at war.\n\n**Inspiration**\nI got inspired by the idea of people forgetting who you are. Then having to risk everything to make your loved ones remember you. The entire world is against you, what will you be ready to do so they don't forget forever? I also wanted the challenge of telling a story in a very short amount of time , so I chose the micro-drama category. Made we watch quite a few of them... what a world.\n\n**How we built it**\nFor the first time, I used a node workflow for 95% of the content. With Freepik's new Spaces. It's such an interesting way of creating and visualising AI content, so much better than just regular prompts. My entire story, the bloopers, the evolution of my characters, all are in a Freepik Space. I also heavily leveraged Claude. Much more as a second brain assistant than usual. The script was a fantastic back and forth where I asked it to be brutally honest with me every step of the way. The process was: script in Claude, where I got a small description for every single shot required, then used an assistant within Spaces to kickstart some of my image prompts, the rest was a combination of several nodes and tricks. Finally, with all shots numbered, I took a huge amount of time to generate all\n\n**Challenges**\nThe inconsistency of the models made spreading out the shots between the video models not practical at all. Some shot images also just wouldn't work, so I had to rethink entire sequences. After creating all images, my main character got denied by the models' obscure censorship algorithm, I therefore had to do a last minute \"recast\". Nothing too complicated, but quite the surprise after all the work done.\n\n**Accomplishments**\nDiscovering ingredients in Veo 3.1 could create incredible sequences with multiple characters. Creating the music in Suno  at the 11th hour was so pleasing. It's an absolute gem of a tool. Finding a storyline that fits with different beats, within 120 sec. Matching my cool songs to those beats. The characters feel like they could be living somewhere, in some galaxy far far away.\n\n**What we learned**\nDon't overestimate the number of shots in a short movie. It makes you create more than you can fit in the time frame or that you need for the story. A lot of sequences got cut, and yet I had spent time crafting the starting frames. For a quick last minute participation to the competition though, I'm satisfied. All in all, what fun it is to create stories. I'll never stop.\n\n**What's next**\nEpisode 2... perhaps a full micro-drama season? I've got the full story after all. Thanks Claude you're fun.\n\n",
    "prize": "Winner Film ‚Äì 1st Place",
    "techStack": "elevenlabs, freepik, google, hailuo, higgsfield, kling, magnific, midjourney, minimax, suno, veo",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=EvS0424JWP8",
    "demo": null,
    "team": "Tell Me World",
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/remember-me-nfhbic"
  },
  {
    "title": "KRAR",
    "summary": "TECHNICAL HIGHLIGHTS: While specific technical implementations are not detailed, the use of AI implies advanced algorithms for tasks such as script analysis, video editing, or audience engagement prediction. The project likely involved machine learning models trained on extensive film datasets to generate insights or automate complex processes.",
    "description": "123    \n\n\n\n      \n  A final movement for TRIBES, a six-film cycle. Created with deep respect for African traditions.\n\nDedicated to the original keepers of memory, tradition, rhythm, and fire. Guardians of difference in an increasingly homogeneous world.\n\nTools used:\n\nMidjourney\nKling 2.5\nHailou 2.3\nGoogle's VEO 3.1\nGoogle's Gemini + GPT 5\nNano-Banana + REVE\nFAL.AI\nElevenLabs SFX\nAbleton Live + Pro Tools - Reaper\nAdobe Premiere + After Effects + Photoshop + Da Vinci Resolve\n\n\n\n        \n    Built With\n\n    ai",
    "prize": "Winner Film ‚Äì 2nd Place",
    "techStack": "ai",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=Aen8x9JQOAs",
    "demo": null,
    "team": "Juli√°n Santoro",
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/krar"
  },
  {
    "title": "[B24] BearHammer Games",
    "summary": "BearHammer Games is a gaming project developed within the Horizon Worlds platform, possibly focusing on immersive virtual experiences that leverage 3D modeling and interactive design. While specific gameplay details are not provided, the project likely emphasizes community engagement and creativity in game development.",
    "description": "Fight 2\n    \n\n              \n  \n\n\n\n            \n        \n          \n  \n\n        \n    \n      Arena 1\n    \n\n        \n          \n  \n\n        \n    \n      Arena 2\n    \n\n        \n          \n  \n\n        \n    \n      Fight 1\n    \n\n        \n          \n  \n\n        \n    \n      Fight 2\n    \n\n              \n  \n\n\n\n            \n        \n          \n  \n\n        \n    \n      Arena 1\n    \n\n        \n          \n  \n\n        \n    \n      Arena 2\n    \n\n        \n          \n  \n\n        \n    \n      Fight 1\n    \n\n        \n          \n  \n\n        \n    \n      Fight 2\n    \n12345    \n\n\n\n      \n  ‚ú® Inspiration\n\nWe wanted the raw, instant energy of pick-up brawlers and local party tournaments ‚Äî games you jump into between sessions, swap clips of, and join friends into rematches. SUMO FIGHT is inspired by viral highlight moments, couch-fight chaos, and the way social spaces (like Smash communities) turn short matches into friendships and rivalries.\n\nüéØ What it does \n\nSUMO FIGHT is a cross-platform, drop-in arena brawler with 1‚Äì2 minute rounds where players shove opponents off the map. Core traits:\n\n\nSimplistic input includes one stick and one button for movement and dashing.\nShort, repeatable matches that scale into chaotic spectacle as lobbies fill.\nMobile and PC friendly UI and controls with immediate entry and spectating.\n\n\nüõ†Ô∏è How we built it\n\n\nBuilt in Horizon Worlds using in-world scripting and models imported from Blender.\nTeam of 3. Programmers Lewis McIntyre & Ektor Zoidis, and Artist Lewis Banks\nFocused on mobile-first HUD with additional support for PC.\n\n\nüöß Challenges we ran into\n\n\nCreating instant feedback on velocity calculations not yet handled.\nAchieving consistent feel across mobile touch and PC inputs.\nBuilding smooth spectator flow and cinematic camera movement without interrupting matches.\n\n\nüëè Accomplishments that we're proud of\n\n\nBuilt an enjoyable game in a few days that keeps players joining.\nWorking with the Camera API and implementing custom camera movements.\nCreated a fun Sci-Fi environment whilst optimizing for fast mobile gameplay.\n\n\nüìö What we learned\n\n\nHow to use AI for rapid prototyping to quickly test ideas before polishing.\nSimple inputs from the player make for easier user onboarding.\nHow to navigate and use the Camera API.\n\n\nüöÄ What's next for [B24] BearHammer Games\n\n\nEvents to encourage community play and the competitive fighting feeling.\nSeasonal and Themed events to encourage replay.\nCosmetic rewards for players who spend the most time in the world.\n\n\n\n\n        \n    Built With\n\n    blenderhorizonworldsvisualstudiocode\n  \n\n        \n    Try it out\n\n    \n        \n  \n  horizon.meta.com\n\n        \n  \n  drive.google.com",
    "prize": "Winner Horizon Worlds Category by Meta - Runner-Up",
    "techStack": "blender, horizonworlds, visualstudiocode",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=DJRpJZ-IQ1A",
    "demo": "https://drive.google.com/drive/folders/145txkRowDuAcPSu3ndU7iyztENAXekvP",
    "team": null,
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/b24-bearhammer-games"
  },
  {
    "title": "2043 Teaser Trailer",
    "summary": "The \"2043 Teaser Trailer\" is an innovative multimedia project that likely combines various digital tools to create an engaging and futuristic teaser trailer. By utilizing advanced AI technologies and multimedia editing software, the project aims to captivate audiences with a compelling narrative that hints at a future scenario or storyline.",
    "description": "**Inspiration**\nThis project started as a comic book series, created using AI, inside and not community. The movie was presented as worldwide premiere at the AI Film Festival Japan in Tokyo on November 3rd.\n\n",
    "prize": "Winner Title Sponsor ‚Äì 2nd Place",
    "techStack": "astra, capcut, elevenlabs, kling, magnific, midjourney, suno",
    "github": null,
    "youtube": "https://youtu.be/dJx-HMR7eR4?si=oo83lvi2c169sBss",
    "demo": "https://youtu.be/dJx-HMR7eR4?si=oo83lvi2c169sBss",
    "team": "Simone Sighinolfi",
    "date": "2025-11-17",
    "projectUrl": "https://devpost.com/software/2043-teaser-trailer"
  },
  {
    "title": "NEO",
    "summary": "TECHNICAL HIGHLIGHTS: The project utilized a diverse suite of technologies, including CapCut for video editing, ElevenLabs for audio generation, and Freepik for graphic resources. This integration of tools reflects a sophisticated understanding of content creation workflows, allowing for high-quality outputs in both audio-visual formats.",
    "description": "**What's next**\nThis short film, entirely produced with AI, explores, on a deeper level, our relationship with emerging technologies. It reflects on how these tools can empower us to tackle challenges‚Äîif we embrace them responsibly and move beyond the fear of the unfamiliar. Neophobia. It was created and developed in less than a month using AI tools from FREEPIK and FAL. The challenges were TIME and maintaining CHARACTER CONSISTENCY; without a doubt, I learned a tremendous amount from this project. I feel proud of the outcome and of the message it aims to convey. What I‚Äôve learned is invaluable; my skills have been pushed to their limits, and I‚Äôm extremely satisfied with the outcome. The next step is hoping the audience understands that AI and emerging technologies are not a threat. No tool can function w\n\n",
    "prize": "Winner Film ‚Äì 2nd Place",
    "techStack": "capcut, elevenlabs, fal, freepik, hailuo, kling, minimax",
    "github": null,
    "youtube": null,
    "demo": null,
    "team": null,
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/neo-z6c4ur"
  },
  {
    "title": "FixItXR",
    "summary": "TECHNICAL HIGHLIGHTS: The project features notable implementations such as hand tracking for intuitive interaction, the MetaSDK for augmented reality capabilities, and OpenAI for intelligent assistance. Additionally, the use of computer vision techniques from Roboflow allows the application to recognize tools and parts, making the repair process more efficient and user-friendly.",
    "description": "**What it does**\nFixItXR automatically labels the components inside the engine bay and overlays clear, labels on each part. It then displays step-by-step tutorials showing the user exactly how to perform maintenance tasks or repairs. Using the Meta Quest 3‚Äôs cameras and spatial tracking, the system aligns instructions with real components, making each repair easy to follow.\n\n**Inspiration**\nFixItXR was inspired by the idea that car repairs should feel intuitive rather than confusing. We wanted to replace paper manuals and guesswork with clear, mixed-reality guidance. Using the Meta Quest 3‚Äôs cameras and spatial tracking, we envisioned an assistant that overlays labels and instructions directly onto the real engine bay, helping anyone perform repairs with confidence and clarity.\n\n**How we built it**\nWe built FixItXR using a combination of mixed reality tools, AI models, and spatial computing. Unity, together with the Meta Quest 3 passthrough and MR SDK, handled real-time spatial tracking, scene understanding, and anchoring virtual labels onto real engine components. Using 169 photos, we trained a Roboflow computer vision model capable of detecting key parts inside the engine bay. OpenAI powered the adaptive step-by-step guidance, reasoning, and TTS instructions that respond to the user‚Äôs actions. By combining these systems, we created a seamless workflow that scans the environment, identifies components, and delivers clear, interactive repair tutorials.\n\n**Challenges**\nOne of the biggest challenges was getting object detection to work reliably in 3D space. Existing pre-trained models worked in 2D images but failed when used in mixed reality, so we trained our own Roboflow model from 169 photos to recognize engine components more accurately. Another challenge was integrating multiple systems‚Äîcomputer vision, Unity MR interactions, UI flow, and AI-driven tutorials‚Äîinto one smooth pipeline. We also had to carefully coordinate timing between detection events, spatial anchoring, and step-by-step instructions to ensure the experience felt stable and responsive inside the headset.\n\n**Accomplishments**\nWe're proud that our custom-trained model consistently labels engine components correctly with no noticeable misidentification, even in a real mixed-reality environment. We successfully built clear, structured tutorials that guide users through each step, supported by smooth text-to-speech instructions that make the experience accessible and hands-free. Bringing all these elements together‚Äîaccurate detection, stable MR anchoring, and an intuitive AI-driven tutorial system‚Äîfelt like a major milestone for our team.\n\n**What we learned**\nWe learned a lot about training and refining computer vision models, especially how much data and iteration it takes to get reliable results. We also gained experience debugging complex MR interactions and solving technical challenges quickly as a team. Most importantly, we built strong friendships and learned how to collaborate effectively under pressure, bringing together different skills to create something meaningful in a very short time.\n\n**What's next**\nWe plan to expand FixItXR to cover more car systems beyond the engine bay‚Äîincluding brakes, suspension, electrical components, and interior repairs. We also plan to add real-time error tracking to detect and correct user mistakes before they cause damage, preventing issues like loosening the wrong bolt or skipping safety steps.\nAdditionally, we want to add multilingual support to make automotive repair accessible globally, delivering instructions in users' native languages. Our vision is to build a comprehensive platform that empowers anyone to confidently maintain and repair their vehicle, regardless of experience level.\n\n",
    "prize": "Winner AI with Camera Access by Meta 1st Place",
    "techStack": "handtracking, metasdk, openai, passthrough, pca, roboflow, unity",
    "github": "https://github.com/sarahimdad/MetaPCARoboflow/tree/add-tutorial-1",
    "youtube": "https://www.youtube.com/watch?v=8-F2Ac9t3I0",
    "demo": null,
    "team": "Jesus Hernandez, Chris KAMAL YOUSSEF",
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/fixitxr"
  },
  {
    "title": "Morning buddy",
    "summary": "TECHNICAL HIGHLIGHTS: Morning Buddy was built using C#, Unity, and NVIDIA technologies, showcasing strong performance in real-time hand tracking. The use of Meta's hand tracking solutions allowed for precise gesture recognition and interaction, enhancing the user experience and setting a high bar for implementation standards.",
    "description": "**What it does**\nIn this mixed reality experience, you begin by settling into your usual morning routine. You place your coffee cup in front of you, and wherever you set it on the table, a small Buddy emerges beside it, eager for a sip, but only once he reaches the top of the mug. Your goal is to complete a sequence of gestural tasks. They may look simple but performing them accurately and consistently within a set time limit is surprisingly challenging. When you succeed, the little Buddy finally earns his sip of coffee, and you get your well-deserved first sip too.\n\n**Inspiration**\nOne part of what inspired this group of Hackers, who have known each other for a while, was a topic that popped up quite often: Forgetfulness.\nThere's a common subtle sense of fear of the brain aging, maybe sooner than it should. One of the team members had a genuine freak-out about early memory issues the long-term cognitive health, becoming hyper-aware of it. They came across this video while doom scrolling on Instagram which sparked the research from there: 3 Brain Activation Exercises for Beginners\nAnother part of the inspiration came from noticing how easy it is to rely on AI in daily life for things that used to challenge our minds. Stopping to push our own thinking stops the support of neuroplasticity, what our brain needs to stay sharp. \n\"Morning Buddy\" came from this reminder of w\n\n**How we built it**\nFor this application's logic, we utilized: Unity + Meta XR SDK for hand tracking, and scene understanding +Hand gesture classifiers for sequences like peace sign, palm open, rock on, and index up. For this application's narrative virtual layer: We designed the character \"Buddy\", he's brainy fella.\nModeled \"Buddy\" using hyper3d\n-Rigged and animated \"Buddy\" using Mixamo\nWe modeled on Maya the stairs that \"Buddy\" uses to climb us the coffee cup.\nWe made timeline sequences in unity for the intro/outro. \nWe made voiceovers using elevenlabs\n\n**Challenges**\nDuring the hackathon, we faced the challenge of implementing cutting-edge technology with components still in beta (Meta AIO SDK), with the incompatibility issues between packages and versions that this entails. For some of us it was the first time doing a hackathon so time was running by so quickly, we had to keep the scope of the project within clear limits and not get carried away by the chaos of this type of environment, analyzing and prioritizing all the situations we encountered, such as wasting too much time implementing or improving components that do not have a primary impact on the experience, or building stable checkpoints to which we could return in case of a major problem.\n\n**Accomplishments**\nThe members of this team for years have been primarily focused on building VR experiences with a specific SDK XROAM so we‚Äôre quite proud to create a mixed reality experience in just two days using building blocks we hadn‚Äôt worked with extensively and not as familiar with the recent updates.\nThis was the first hackathon for 4 people out of 5 in this team, so being extra agile was a great exercise. A lot of creative solutions came from this while keeping focus on the main goal, the core mechanic.\n\n**What's next**\nInclude context-aware behavior:\nTea/Coffee/Milk/Juice, etc..... + Text-To-Speech for the character (elevenlabs)\nBigger/ wider library of gestural tasks. \nPersonalization: Adaptive to your progress with time / your weak points.\nAdd difficulty levels: Play with time frame\n\n",
    "prize": "Winner Best Implementation of Hand Tracking by Meta 1st Place",
    "techStack": "c#, meta, nvidia, unity",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=ZnqLUPS5Eo4",
    "demo": null,
    "team": "Hoda Rawas, Alex Gavalda, David Sillero, Katrina Gutierrez",
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/morning-buddy-3y2uwt"
  },
  {
    "title": "GR Cup Analytics",
    "summary": "GR Cup Analytics is a data-driven platform designed to analyze and visualize performance metrics related to the GR Cup, likely focusing on sports analytics. The project aims to provide insights into player and team statistics, trends, and overall performance, enabling fans and analysts to better understand the competition.",
    "description": "**What it does**\nThe app analyzes race 1 data for Barber Motorsports Park,Circuit of the Americas,Indianapolis, Sonoma race tracks.  It provides driver insights, pre race predictions and post event analysis.  The user can select a race and get the insights.  It also provides the ability for the user to select a driver and see how well they performed compared to the average.\n\n**Inspiration**\nThe goal was to analyze GR Cup Data using race data that was provided.  There were multiple races with different races data information\n\n**How we built it**\nIt was built using python and streamlit.  Python data analysis libraries were used (python, dumpy, scikit-learn, plotly).  Streamlit was used to build a data web app which showcases the insights in a simple to understand format.\n\n**Challenges**\nThe main challenge was understanding the data as there was no main data dictionary.  Another challenge was inconsistent naming and columns information across races.  The file names were inconsistent as well as some columns were not present in some of the race data.  To tackle this the information was narrowed to Race 1 information for Barber Motorsports Park,Circuit of the Americas,Indianapolis, Sonoma race tracks.\n\n**What we learned**\nFor me it was my first time working with race data, so it was a fun learning experience.\n\n**What's next**\nIn terms of next steps it is to improve Improved Data: Work to ensure data is consistent across GR Cup races\nExperiment with more models: Use different modelling approaches\n-Multi-Race Comparison: Improve the application to provide the ability to analyze results and metrics across races\n\n",
    "prize": "Winner Best of Wildcard",
    "techStack": "html5, python, streamlit",
    "github": "https://github.com/edimaudo/grcup-analytics",
    "youtube": "https://www.youtube.com/watch?v=K3ZWtno3Vfs",
    "demo": "https://blank-app-qd7rpm31xpn.streamlit.app/",
    "team": "Ed U",
    "date": "2025-11-19",
    "projectUrl": "https://devpost.com/software/gr-cup-analytics"
  },
  {
    "title": "Polkadot Analytics hub",
    "summary": "Polkadot Analytics Hub is a comprehensive platform designed to provide users with in-depth insights and analytics related to the Polkadot ecosystem. By leveraging advanced data processing and visualization techniques, the project aims to facilitate better decision-making for developers and investors involved in the Polkadot network.",
    "description": "**What it does**\nPolkadot Analytics Hub provides a unified dashboard with:\nReal-time parachain metrics (activity, TVL, transactions)\nCross-chain insights and comparisons\nHistorical charts (7‚Äì30 day trends)\nInteractive visualizations and clean UI\nWallet connection with Polkadot.js\nOptional AI features: forecasting + anomaly detection\nIt helps developers, analysts, and users understand network health in one place.\n\n**Inspiration**\nThe Polkadot ecosystem is growing fast, with many parachains and cross-chain activities happening at the same time. But there is no simple way to see everything in one place. Most tools focus on a single chain or provide only raw metrics. We wanted to build a central analytics hub that shows clear, real-time insights for the entire Polkadot network.\n\n**How we built it**\nBackend: Node.js + Express for API endpoints, caching, validation, and data structuring\nFrontend: Next.js + Tailwind + Chart.js for visual charts and smooth UI\nData: Polkadot RPC / Substrate endpoints\nAI Layer (optional): Python + FastAPI for prediction models and anomaly detection\nTools: GitHub, and Postman for testing\nThe system is modular so features can be added or replaced easily.\n\n**Challenges**\nHandling RPC limits and inconsistent chain responses\nDesigning endpoints that stay fast even with large data\nChart performance with high-frequency updates\nCORS issues during dev and deployment\nKeeping backend and frontend synchronized while developing fast\n\n**Accomplishments**\nA working, full-stack analytics system from scratch\nClean UI with responsive and interactive charts\nA structured backend ready for real Polkadot metrics\nA working AI prediction module\nStable architecture that can scale\nClear documentation and easy setup\n\n**What we learned**\nHow to structure production-style APIs\nHandling live blockchain data and caching\nAdvanced React + Next.js concepts\nHow to optimize charts for performance\nHow to integrate Python/AI services into a JS system\nTeam coordination, testing, and deployment workflows\n\n**What's next**\nFull real-time data syncing with all major parachains\nUser accounts + personal saved dashboards\nAlerts and notifications for unusual network activity\nCross-chain flow diagrams (visualizing asset movement)\nMobile and tablet versions\nIntegration with more ecosystem tools (XCMP, bridges)\nLaunching a public version of the platform for community use\n\n",
    "prize": "Winner Community Vote",
    "techStack": "ai:, api, control:, devops, environment:, gemini, git, google, manager:, npm, numpy, package, pandas, version",
    "github": "https://github.com/Mr-mpange/polkadotblockchain.git",
    "youtube": "https://www.youtube.com/watch?v=ttCsqy98onA",
    "demo": null,
    "team": "I worked as a project manager",
    "date": "2025-11-17",
    "projectUrl": "https://devpost.com/software/polkadot-analytics-hub"
  },
  {
    "title": "[B25] Archer Defense",
    "summary": "Archer Defense is an engaging interactive experience built in Horizon Worlds, where players assume the role of archers tasked with defending against waves of enemies. Utilizing immersive gameplay mechanics, the project combines strategy and action to create a captivating environment that encourages teamwork and skillful archery.",
    "description": "12345    \n\n\n\n      \n  üß† Inspiration\n\nWe wanted to prove that Horizon‚Äôs new portrait mode can deliver true mobile-first, hyper-casual gameplay - fast, simple, addictive, and built entirely without the default avatar system.\nArcher Defense explores how portrait-first action can feel when everything is designed specifically for vertical play.\n\n\n\nüõ†Ô∏è What It Does\n\nArcher Defense is an endless vertical action game where players fight rising waves of enemies, push for new high scores, and join forces for chaotic co-op runs.\nThe experience is built to feel immediate, readable, and satisfying on a mobile screen.\n\n\n\nüîß How We Built It\n\nWe built every system from scratch, including the camera, input interactions, and a mobile-first UI.\nAll art, effects, and audio were tuned specifically for vertical play, ensuring the game feels like a native mobile title rather than a traditional Horizon world.  \n\n\n\nüöß Challenges\n\nWhile building a single-player version would have been straightforward, creating a game that feels equally good in solo and multiplayer became our toughest challenge.\nWave pacing, sync, enemy logic, and readability in portrait mode required continuous iteration.\n\n\n\nüèÜ Accomplishments\n\n\n‚úÖ Fully custom gameplay systems\n‚úÖ Smooth solo + co-op modes\n‚úÖ Fast, polished hyper-casual loop\n‚úÖ A Horizon world that feels like a native mobile game\n\n\n\n\nüìö What We Learned\n\nPortrait mode demands rethinking the fundamentals - pacing, UI, and camera framing all behave differently in vertical orientation.\nWe also discovered how much custom-built systems define the feel and polish of a Horizon world.\n\n\n\nüöÄ What‚Äôs Next for Archer Defense\n\n\nüí• Power-ups\nüèπ Weapon skins\nüëæ New enemy types\nüè∞ Alternate layouts\nü§ù Expanded multiplayer modes\n\n\n\n\n        \n    Built With\n\n    horizontypescript\n  \n\n        \n    Try it out\n\n    \n        \n  \n  horizon.meta.com",
    "prize": "Winner Horizon Worlds Category by Meta - First Prize",
    "techStack": "horizon, typescript",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=X6Id_wBj8sg",
    "demo": "https://horizon.meta.com/world/803648796175840/",
    "team": null,
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/archer-defense"
  },
  {
    "title": "xgotop: Realtime Go Runtime Visualizer",
    "summary": "xgotop is a real-time visualizer for Go runtimes that leverages eBPF (Extended Berkeley Packet Filter) technology to monitor and analyze the performance of Go applications. By providing in-depth insights into runtime behavior, it enables developers to optimize their applications more effectively.",
    "description": "**What it does**\nxgotop attaches several uprobe's to the specified program which is written in Go, and traces the goroutine state changes and memory allocations in detail with sub-microsecond delay.\n\n**Inspiration**\nI've been learning about, experimenting with, and writing about eBPF for a while. While working on a new project on tracing HTTP requests in HTTP servers written in Go using eBPF and bpftrace, I've needed to trace specific goroutines in realtime. That's when I've noticed that I should develop a realtime Go runtime visualizer with a CLI and a web UI to trace goroutine lifecycle and memory allocation events.\n\n**How we built it**\nI've built this project by first writing a bpftrace script to prototype the idea to see if it's doable, and then started writing the real implementation using C for the eBPF program, Go with the cilium/ebpf package for the userspace program, Go again for storage/API layer, Python for runtime metrics plotting and test validations, and React for web UI. You can see the overall system design in the illustration below.\n\n**Challenges**\nThe initial challenge was that extracting goroutine information from a running Go program is not a trivial task. Memory layout of internal runtime structures depends on the architecture of the host machine and the Go version of the target program. I've read the Go ABI to find out that I needed to use the r28 register to get the g runtime struct of the current goroutine for example. I've also read the Go source code for Go 1.25 to learn the memory offsets of g struct's fields that I need, enumerated values of native Go runtime types, internal values of goroutine states, runtime functions that I can hook, and so on.\nAfter reading the source code, I've bumped into the issue of not being able to use uretprobes on Go runtime functions, as the return probe was overriding the return PC of the run\n\n**Accomplishments**\nI'm proud of being able to build a low-level project from start to finish by first doing the research by reading the Go source code and docs, then prototyping the idea with bpftrace, implementing the production version using C and Go, developing a testing suite around it, and finally building a web UI (with the help of AI) for the project. At the end of the day, I have a tool that I needed and also might be useful for the community.\n\n**What we learned**\nI've learned that building tooling products which support multiple versions of the programming language and OS architectures is not a trivial task. That requires a lot of research and testing to validate the correctness of the implementation. I've also learned more about how goroutines work and allocate memory in Go runtime.\n\n**What's next**\nThere are still many things to fix and optimize and introduce as new features to the project. Here is an incomplete list of my ideas about the next steps for xgotop: Add support for amd64 architecture\nStore events in ClickHouse, which will be storage-efficient and fast as a columnar metrics DB is the perfect fit for this kind of data we generate\nOffline replay of sessions\nAdd support for more Go lifecycle events\nAdd support to be able to see which function is being executed in the goroutine (parsing the function name in runtime)\nAdd support for listing the stack trace of the goroutine at the time of the event\nOptimize the way that it's reading Go runtime objects (g struct has lots of padding now, which is a bad practice)\n\n",
    "prize": "Winner Using eBPF",
    "techStack": "c, ebpf, go, python",
    "github": "https://github.com/ozansz/xgotop",
    "youtube": "https://www.youtube.com/watch?v=Xr3l-eGaY7c",
    "demo": null,
    "team": "Ozan Sazak",
    "date": "2025-11-28",
    "projectUrl": "https://devpost.com/software/xgotop-go-runtime-observer"
  },
  {
    "title": "[B03] XR Latte Art: Pour decisions? Never again.",
    "summary": "XR Latte Art is an immersive mixed reality application designed to enhance the art of coffee-making by allowing users to create intricate latte designs through virtual guidance. By leveraging augmented and virtual reality, it provides interactive tutorials and real-time feedback, making latte art accessible to both beginners and seasoned baristas.",
    "description": "**What it does**\nXR Latte Art leverages mixed reality hand tracking to solve a surprisingly complex motor learning problem: latte art. Traditional learning requires expensive repetition and immediate feedback is impossible‚Äîyou only see results after the pour is complete. Our solution overlays 3D pour patterns directly into the user's physical space, tracks pitcher movement in real-time using hand tracking, and provides instant biomechanical feedback. Users practice the precise wrist rotations, pour heights, and movement speeds required for specific patterns (like heart, rosetta, tulip, swan, etc.), with scoring based on trajectory matching and timing accuracy. Once you are done with practicing, you can do the real deal with real milk and coffee. The result: accelerated skill acquisition through deliberate\n\n**Inspiration**\nSince I drink 2 coffees per day I only had limited chance to learn latte art and with that my first perfect latte art took 4 months of daily 1-2 pours. I want to learn quicker other latte arts and I was not willing to waste milk.\n\n**How we built it**\nWe began with a focused ideation phase, prioritizing a strict MVP to avoid scope creep and ensure we could deliver a complete, high-quality experience within the hackathon timeframe. Our team split into two parallel workstreams:\n-Three team members developed the hand-tracking movement recording system along with the replay and evaluation engine.\n-Two team members focused on the user-facing experience, UI/UX flow, and overall ambience.\nWe leveraged Meta‚Äôs Hand Tracking SDK to capture high-fidelity hand joint data. To record a real barista‚Äôs movements, we generated a JSON file containing the full frame-by-frame positions and rotations of both hands throughout the demonstration. This JSON serves as the ‚Äúgold standard‚Äù animation sequence: during training sessions, we replay this movement for t\n\n**Challenges**\n-Being able to find the best technique to follow the movement of the accessories in hand to be used.\n-Handtracking of the MetaSDK is not accurate enough to describe the precise movement of the barista with the current technology (fallback we used: use controllers with virtual objects overlayed for recording of the training movements)\n-Some errors with Meta SDK (since we were using the newest version of Meta SDK and Unity there were some incompatibilites and errors)\n\n**Accomplishments**\n-Being able to to iterate from the initial idea through extraordinary individual improvement points\n-Being able to record, track, measure and playback hand movements with accuracy\n-Being able to provide pleasant experience for endusers in this short timeframe with ambience\n-Being able to test the concept in real environment with actual professionals on the field (2 baristas in coffee shops) and apply the feedback to the product.\n-Being able to do the movement recording for use from professional to ensure that the data is high quality.\n\n**What we learned**\n-How to mix real objects with the MR space and interact with them in a way that we can also interact with the application at the same time.\n-Good UI designer is key and we were really blissful with ours\n-You can calculate scores based on fitness for data analysis\n-Macbook is faster at building\n-Collaborating with multiple expertise\n\n**What's next**\nPhase 1: Expand the Pattern Library\nWe'll add advanced latte art patterns including swan, phoenix, dragon, and competitive-level designs. Users can unlock progressively harder patterns as their precision scores improve, creating a skill tree from beginner hearts to championship-level pours.\nPhase 2: Community Pattern Sharing\nEnable users to record their own signature patterns and share them. A professional barista in Tokyo can upload their award-winning rosetta technique, and enthusiasts worldwide can learn it exactly as performed.\nPhase 3: The Skill Marketplace - Beyond Coffee\nOur core technology‚Äîhand motion recording with precision scoring‚Äîapplies to any craft requiring muscle memory. We're building a marketplace where experts teach and learners master:\nCalligraphy & Hand Lettering - Bru\n\n",
    "prize": "Winner Mixed & Virtual Reality Category by Meta 1st Place",
    "techStack": "chatgpt, elevenlabs, metasdk, unity",
    "github": "https://github.com/danieloquelis/LatteArtXR",
    "youtube": "https://www.youtube.com/watch?v=mo_t3zV5lHk",
    "demo": null,
    "team": "Ignacio Arqueros, Juan Hernandez Martin, Daniel Oquelis",
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/b03-latte-art-pour-decisions-never-again"
  },
  {
    "title": "Context Cards",
    "summary": "Context Cards is an innovative project that leverages face recognition technology to create personalized context-aware information cards. By identifying individuals through facial recognition, the application delivers relevant data and insights tailored to the user‚Äôs context, enhancing interaction experiences in various settings.",
    "description": "**What it does**\nWe're developing a digital memory assistant to help you remember people in social situations without being creepy or invasive. This is how it works in human terms: 1) Our Simple, Ethical Solution Instead of recording conversations in secret, like some apps, ContextCards uses a mutual consent approach: both parties have to actively agree to connect, by scanning each other's QR codes, in a sort of digital handshake. And Most Important of all, No data collection on the sly; no surreptitious recording. 2) How It Works Day-to-Day *You meet a new person at a conference or networking event *You both open ContextCards and scan each other's QR codes *You jot down a quick note about them: \"Sarah - works on AI safety, loves hiking\" *Next time you see Sarah, your phone's camera recognizes her and flas\n\n**Inspiration**\nLast time when I was on event, I made some fantastic connections but next day, I couldn't recall them. This was embarrassing but I asked again and when they told me about context of our last meeting, I remembered everything! this brought the spark for this project.\nAnd I don't think its problem of only mine, I feel like most of us have went through this issue and it is reported that \"85% of professionals admit to forgetting critical details about people they've met.\"\n\n**Challenges**\n1) Face recognition model: We ran into the typical problems of hackathon about model training but for the current MVP, we decided not to get involved into training one instead, we decided on using face_recognition library from python for this prototype. 2) No IoT: This project is actually meant to be IoT project with camera integrated in glasses but due to limitation, we decided on using web version for demo and if we get enough reach, we could scale this project.\n\n**Accomplishments**\nAt the end of the day, we were able to set our skills together to create something that could bring a change, that change for us is solving networking and social connection problems.\n\n**What we learned**\nI personally learned many things and as a team, we learned about project designing and other basics as its our first online hackathon which is really different from offline ones we attended.\n\n**What's next**\nFor future, we can add IoT in this project to 100x the concept. Additionally we can train our own model with custom dataset and all for even better accuracy.\n\n",
    "prize": "Winner Innovative minds",
    "techStack": "css3, face-recognition, flask, html5, javascript, numpy, python, sqlite, werkzeug",
    "github": "https://github.com/sakshyamadhikari01/meoww",
    "youtube": null,
    "demo": null,
    "team": "AKITO009",
    "date": "2025-11-17",
    "projectUrl": "https://devpost.com/software/polypharma"
  },
  {
    "title": "NeuroSpring ‚Äì Real-Time Cognitive Load Optimizer",
    "summary": "NeuroSpring is a real-time cognitive load optimizer that leverages EEG (electroencephalography) data to assess and adjust cognitive load during tasks. By analyzing brain activity, the system aims to enhance productivity and well-being by dynamically optimizing work conditions based on individual cognitive states.",
    "description": "**What it does**\nNeuroSpring is an AI-powered platform that monitors cognitive load in real-time using EEG and behavioral data. It detects fatigue, stress, and focus lapses, and provides personalized interventions such as micro-break suggestions, mindfulness exercises, and environmental adjustments. It helps optimize productivity, prevent mental fatigue, and improve performance in everyday tasks and professional work.\n\n**Inspiration**\nEver felt completely exhausted after hours of work, yet unable to focus, and wondered why your brain just won‚Äôt cooperate? I noticed that people in high-pressure environments‚Äîwhether at work, studying, or even during daily tasks‚Äîstruggle with mental fatigue, lapses in focus, and stress, leading to mistakes, lost time, and frustration. There was no easy, real-time way to understand what the brain was experiencing or to intervene before it was too late. NeuroSpring was born to solve this: a system that listens to your brain in real-time and helps you stay sharp, focused, and mentally energized throughout the day.\n\n**How we built it**\nEEG Signal Acquisition: 14-channel EEG for real-time brainwave data\nAI Model: Hybrid CNN-Transformer for cognitive state classification\nMulti-modal Integration: EEG + eye tracking + heart rate + keystroke dynamics + environment sensors\nExplainable AI: SHAP and LRP for real-time transparency of model decisions\nInterventions: Automated, personalized suggestions triggered by cognitive state\nDashboard & API: Real-time metrics visualization, logging, and analytics with React, FastAPI, WebSockets\nDeployment: Docker-compose ready, privacy-compliant, scalable to enterprise level\n\n**Challenges**\nReal-time EEG processing with low latency while maintaining accuracy\nIntegrating multiple modalities (behavioral, physiological, environmental) in a unified model\nDesigning interventions that are effective, personalized, and non-intrusive\nEnsuring data privacy and security while collecting sensitive neurophysiological data\nMaking a production-ready dashboard with live AI explanations\n\n**Accomplishments**\nSuccessfully implemented real-time EEG cognitive load monitoring with hybrid deep learning\nDeveloped a multi-modal integration system combining EEG, heart rate, eye tracking, and behavioral data\nBuilt Explainable AI features so users understand why interventions are suggested\nDesigned personalized interventions that increase focus duration by 80% and reduce mental fatigue by 60%\nCreated a production-ready, scalable platform ready for hackathon demo or real-world deployment\n\n**What we learned**\nCombining neuroscience research with AI enables tangible improvements in cognitive performance\nReal-time, explainable interventions are critical for user trust and effectiveness\nMulti-modal data fusion significantly improves prediction accuracy over EEG alone\nScalable system design, privacy, and real-time processing are achievable in neurotechnology applications\nUser experience and UI design are as important as technical performance\n\n**What's next**\nAdd team-based and institutional analytics for workplaces\nIntegrate sleep and circadian rhythm optimization for 24/7 cognitive enhancement\nExplore AR/VR immersive environments to further improve focus\nDevelop predictive career guidance using cognitive load trends\nExpand cross-platform mobile apps for wider accessibility\n\n",
    "prize": null,
    "techStack": "cnn, docker, eeg, fastapi, onnx, postgresql, python, react, redis, transformer, websocket",
    "github": "https://github.com/senushidinara/NeuroSpring",
    "youtube": "https://www.youtube.com/watch?v=GaQwzHrI2dE",
    "demo": "https://691b246996bab2377e6c740b--incandescent-squirrel-caf8e0.netlify.app/#metrics",
    "team": null,
    "date": "2025-11-17",
    "projectUrl": "https://devpost.com/software/neurospring-real-time-cognitive-load-optimizer"
  },
  {
    "title": "PathLearn",
    "summary": "TECHNICAL HIGHLIGHTS: The project employs a sophisticated stack including JavaScript, Node.js, and React for its frontend, while utilizing Python, PyTorch, and sentence-transformers for backend machine learning processes. This combination enables robust data processing and user interaction, making it both functional and responsive.",
    "description": "**What it does**\nPathLearn is an adaptive AI tutor that builds a completely personalized, career-aligned curriculum for every student. It models each student's thinking using an RNN-based memory system, tracks mastery over time, and learns the student‚Äôs learning style through a Deep Reinforcement Learning teacher agent that continuously adjusts difficulty, pacing, and concept order. All explanations, examples, and practice problems are generated through a RAG pipeline powered by katanemo/Arch-Router-1.5B, ensuring every step is grounded, accurate, and tailored.\nStudents start by choosing both a course (Physics Honors, Algebra 2, Biology, Chemistry, etc.) and a future career (doctor, astrophysicist, biomedical researcher, engineer‚Ä¶). PathLearn fuses these two choices to build a dynamic curriculum that evolv\n\n**Inspiration**\nI‚Äôve always noticed how students struggle with learning independently‚Äîespecially at home. Time management, stress, gaps in understanding, and the lack of a truly personalized tutor make studying harder than it needs to be. Existing tools like Khan Academy and AI chatbots help, but they don‚Äôt adapt to you, your pace, or your goals. I wanted to build something that understands each student as an individual learner and shapes a learning path around who they are and who they want to become. That idea became PathLearn.\n\n**How we built it**\nI combined three core machine learning systems: RNN Learning Memory Model Models the student‚Äôs pace, retention, misconceptions, and improvement over time. DRL Curriculum Orchestrator A reinforcement-learning agent selects the next topic, difficulty, explanation type, and career-aligned examples based on ongoing performance. RAG + katanemo/Arch-Router-1.5B Retrieves curated textbook content, prior attempts, and example templates to generate personalized explanations, steps, and practice problems. A backend pipeline integrates these components, while the frontend adapts to each student with mastery maps, learning diagnostics, and personalized practice flows.\n\n**Challenges**\nDesigning a reward function that balances difficulty, mastery, and frustration.\nGetting the RNN to meaningfully predict retention curves based on sparse data.\nIntegrating RAG with Arch-Router-1.5B in a way that consistently produced career-aligned explanations.\nEnsuring the DRL agent didn‚Äôt overfit to a single learning pattern early on.\nMaking the system feel ‚Äúhuman‚Äù instead of just algorithmic.\n\n**Accomplishments**\nBuilt a genuinely adaptive learning system that goes far beyond Q&A chatbots.\nGot the DRL agent to meaningfully personalize topic sequencing.\nCreated a career-driven learning experience that feels truly unique to each student.\nIntegrated Arch-Router-1.5B into a full RAG pipeline for high-quality explanations.\nDeveloped a model that adjusts to student weaknesses in real time.\n\n**What we learned**\nI learned how powerful multi-model AI systems can be when each component plays a specific role. DRL and RNNs together can create deeply personalized learning dynamics that LLMs alone simply cannot achieve. I also gained experience building RAG pipelines, designing reward signals, and engineering scalable, adaptive education tools.\n\n**What's next**\nAdd more courses and specialized career tracks.\nTrain a larger student-behavior model using anonymized interaction data.\nIntegrate more modalities like drawings, graphs, and math handwriting.\nDeploy PathLearn as a full online learning platform.\nBuild student communities with AI-guided peer collaboration.\nExpand to college-level and professional certification tracks.\n\n",
    "prize": "Winner 2nd Overall",
    "techStack": "javascript, katanemo, node.js, python, pytorch, react, sentence-transformers, tailwindcss",
    "github": "https://github.com/aaaravM/PathLearn",
    "youtube": "https://www.youtube.com/watch?v=M4-9kVrcVt8",
    "demo": null,
    "team": "Aarav Muttanapalli",
    "date": "2025-11-17",
    "projectUrl": "https://devpost.com/software/pathlearn"
  },
  {
    "title": "Launchpad",
    "summary": "TECHNICAL HIGHLIGHTS: Built with Next.js, Launchpad leverages server-side rendering for improved performance and SEO. Notable technical implementations may include dynamic routing for seamless navigation, API integration for real-time data access, and responsive design to ensure usability across devices.",
    "description": "**What it does**\nLaunchpad uses your profile data (user entered) to score your profile out of 10 based on your essays, academics, awards and extracurriculars. It then uses gpt-4o-mini to assess your strengths and weaknesses to present you with a tailor made strategy for college applications.\nLaunchpad also has a (currently small) database of universities/colleges as well as upcoming opportunities (competitions, scholarships). Clicking on a college will presented you with AI generated insights about that college and how your 'fit' matches with what they are looking for and whether you are a competitive applicant or not by rating your fit and chances of admission out of 10 based on SEVERAL FACTORS. This is achieved using a REALLY LONG prompt that goes into in depth detail about admission requirements and nua\n\n**Inspiration**\nWhat Inspired me to build launchpad was that there were like a bajillion application portals for different universities and colleges and as a busy high-schooler, it took me 2 years to wrap my head around most of them. Thats why I  built Launchpad! it consolidates all that you need to know about college applications around the globe and how to make your application better.\n\n**How we built it**\nIt was built using Next.js and Next API routes along with Mongo DB and the open AI API (using gpt 4o mini and Tailwind-CSS for styling and AOS for animations. \nAI disclosure: Github copilot was used to assist in development and testing, and the Open AI API was used for generating the analysis and insights for user profiles. I mentioned prompts for the gpt API before which serve as the backbone for Launchpad so heres the entire prompt (its only a bit long...): const prompt = `You are a college admissions expert. Analyze this student's fit for ${college.name}.\n\nCRITICAL INSTRUCTIONS: \n\n1. ADMISSION CRITERIA - Identify what ${college.name} actually uses:\n   - Canadian universities (Alberta, UofT, UBC, McGill): HIGH SCHOOL GRADES only, NOT SAT/ACT\n   - UK universities: A-Levels, IB, or equival\n\n**Challenges**\nGathering info on colleges, ensuring the model doesn't hallucinate new info , persisting user profile insights by adjusting the models temperature parameter (to ensure consistent outputs)\n\n**What's next**\nWe'll continue expanding our database to ensure that it can truly evolve into an all in one platform for college applications.\n\n",
    "prize": null,
    "techStack": "next.js",
    "github": "https://github.com/ARTariqDev/Launchpad",
    "youtube": "https://www.youtube.com/watch?v=oQ47-gXgV2U",
    "demo": "https://launchpad-bice-seven.vercel.app/",
    "team": "Abdul Wasiq Khan, Ahmad Hassan, Muhammad Abubakar, Abdur Rehman Tariq",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/launchpad-7xkjwu"
  },
  {
    "title": "wARkshop",
    "summary": "wARkshop is an interactive workshop platform that utilizes hand tracking technology to create immersive experiences for users. By leveraging Meta's hand tracking capabilities within a Unity environment, it allows participants to engage in activities and learn through intuitive, gesture-based interactions.",
    "description": "123    \n\n\n\n      \n  This project has been built using Meta's All-In-One SDK in Unity. It features Microgestures and Spatial Anchors. The user is greeted with a UI to choose a door type into the Door Frame. When they select select one option, an interactable door appears inside the frame, ready to walk on it. When the user walks into the door, it automatically opens up, and it closes down when it leaves.\n\nThe user is also able to customize the color of said door, with the left hand to control the black levels of it and the right hand to set the color.\n\nwARkshop is a tool that enhances the construction visualization experience for both sellers and clients alike, and the framework built upon it ensures that the product is shown in the best quality possible.\n\n\n\n        \n    Built With\n\n    c#metaunity\n  \n\n        \n    Try it out\n\n    \n        \n  \n  GitHub Repo",
    "prize": "Winner Best Implementation of Hand Tracking by Meta Runner-Up",
    "techStack": "c#, meta, unity",
    "github": "https://github.com/jordialfonsop/wARkshop",
    "youtube": "https://www.youtube.com/watch?v=KZwpt0Wb5mM",
    "demo": null,
    "team": "I Worked in AI, ComfyUI,, Roberto Villar Vega",
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/warkshop"
  },
  {
    "title": "StudySync AI",
    "summary": "StudySync AI is an intelligent study assistant designed to enhance learning experiences through personalized content delivery and emotional analysis. By leveraging advanced AI technologies, the platform adapts lectures and study materials to meet individual student needs while assessing emotional engagement during learning sessions.",
    "description": "**What it does**\nStudySync AI reads your emotions, tracks your focus, and predicts when you'll forget‚Äîadapting in real-time to help you learn smarter. LectureGPT processes uploaded lectures (video/audio/PDF) and generates Cornell notes, flashcards, and quizzes instantly. Focus Sessions track tab-switching in real-time‚Äîwhen you get distracted, positive messages appear (\"Stay focused! You've got this! üéØ\"), achieving 88% average focus scores. MindMirror uses webcam emotion detection to identify confusion and automatically simplifies content with visual aids. Memory Retention Engine predicts exactly when you'll forget using the Ebbinghaus forgetting curve, scheduling reviews at optimal intervals. AI Tutor provides on-demand explanations during study sessions. Students improve quiz scores from 75% to 95%, main\n\n**Inspiration**\nYou're studying quantum mechanics. You switch to check Instagram‚Äîyour focus score drops and a gentle message appears: \"Stay focused! You've got this! üéØ\" You furrow your brow at a complex equation‚Äîyour webcam detects confusion and simplifies the explanation. Three days later, just as you're about to forget, a review notification arrives. That's StudySync AI: the platform that knows when you're distracted, confused, or forgetting‚Äîand helps in real-time.\n\n**How we built it**\nTech Stack: React + TypeScript, Tailwind CSS, Google Gemini API (lecture processing), Face-API.js (emotion detection), Recharts (analytics), and Ebbinghaus algorithms (spaced repetition). Development: We built in 4 phases over 48 hours‚Äîinfrastructure setup, AI feature integration, behavioral tracking implementation, and analytics dashboard polish. Used client-side processing for speed, LocalStorage for persistence, and prompt engineering to generate structured Cornell notes from Gemini.\n\n**Challenges**\nEmotion detection accuracy: Face-API.js misclassified expressions. We added confidence thresholds (75%+) and 3-second smoothing windows, achieving 85% accuracy. API rate limiting: Hit Gemini limits during testing. Implemented exponential backoff, caching, and switched to faster gemini-2.0-flash-exp model. Real-time performance: Face detection + timer + tracking caused lag. Reduced detection from 60fps to 10fps and optimized React renders. Time constraints: At hour 30, we had 5 half-built features. Prioritized ruthlessly‚Äîfinished 2 core features perfectly rather than 5 poorly.\n\n**Accomplishments**\nSuccessfully integrated 3 complex APIs into a cohesive experience. Built real-time emotion detection that actually works (85% accuracy). Created a complete ecosystem‚Äînot just one feature. Implemented scientific algorithms students can understand. Achieved professional UI that looks like a real SaaS product. Most importantly: we built empathy into code. We don't shame students for distractions‚Äîwe encourage them. We don't wait for confusion‚Äîwe detect and help. We don't send random reminders‚Äîwe predict scientifically.\n\n**What we learned**\nTechnical: Emotion detection is accessible with pre-trained models. Prompt engineering is an art requiring 20+ iterations. Performance optimization matters more than perfect features. Domain: Ebbinghaus curve actually works (60% better retention). Positive reinforcement beats punishment. Students don't know when they're confused until too late. Hackathon: Build for the demo. Solve real problems judges relate to. Differentiation is key‚Äîemotion detection is our unfair advantage.\n\n**What's next**\nImmediate: Beta program with 50 students, move to secure backend, add offline mode. V2.0 (Q1 2025): Mobile apps, group study features, live lecture recording Chrome extension, LMS integration (Canvas/Blackboard). V3.0 (Q3 2025): Enhanced emotion model trained on 10K+ expressions, voice-based AI tutor, predictive performance analytics, AR/VR study environments. Enterprise: University licensing at $5/student/year, API for third-party integrations, academic validation through research partnerships. Vision: Transform learning for 100M+ students worldwide by making education adapt to them‚Äînot the other way around.\n\n",
    "prize": "Winner Participation Prize; Winner Second Place",
    "techStack": "claude, ebbinghaus, face-api.js-(emotion-detection), google-ai-studio, google-gemini-api-(lecture-processing), html5, javascript, npm, react-+-typescript, recharts-(analytics), tailwind-css",
    "github": "https://github.com/srikarkarri/StudySync-AI/tree/main",
    "youtube": "https://www.youtube.com/watch?v=YRp7QXfoaOs",
    "demo": null,
    "team": "Srikar Karri",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/studysync-ai-v86jts"
  },
  {
    "title": "ChainLink Portfolio - Polkadot Cross-Chain Activity Tracker",
    "summary": "ChainLink Portfolio is a cross-chain activity tracker designed for the Polkadot ecosystem, allowing users to monitor and manage their assets across multiple blockchain networks. By integrating various data sources and visualization tools, it aims to provide a comprehensive overview of users' portfolio performance and activity in real-time.",
    "description": "**What it does**\nChainLink Portfolio is an AI-enhanced, privacy-first dashboard that unifies your Polkadot ecosystem presence: Multi-Chain Aggregation: Connects to 4 parachains (Polkadot, Astar, Moonbeam, Paseo Asset Hub) and displays all balances in real-time with USD values\nAI Portfolio Advisor: Powered by Groq's Llama 3.1, it analyzes your holdings and provides personalized recommendations on staking, allocation, and cross-chain opportunities\nPortfolio Health Score: Calculates a 0-100 score based on diversification, risk balance, chain activity, and portfolio size\nReal XCM Transfers: Execute actual cross-chain asset transfers using Polkadot's XCM protocol‚Äîmove tokens between parachains with one click\nBeautiful Visualizations: Interactive pie charts, chain cards, and responsive design that works on any d\n\n**Inspiration**\nAs a Web3 learner exploring Polkadot, I faced a frustrating problem: my crypto identity was scattered across multiple parachains. Checking my Astar balance meant opening one explorer, Moonbeam another, Polkadot yet another. I realized this fragmented experience was preventing mainstream adoption‚Äîusers shouldn't need to be blockchain experts to see their assets. Polkadot's unique multi-chain architecture is powerful, but the user experience needed to catch up. I wanted to build something that made cross-chain interaction as simple as checking your bank account‚Äîone dashboard, complete visibility, and actual cross-chain actions.\n\n**How we built it**\nTech Stack: Next.js 14, TypeScript, Tailwind CSS, Polkadot.js API, XCM Protocol, Groq AI, Recharts\n\n**Challenges**\nSSR vs Browser APIs: Next.js server-side rendering conflicted with Polkadot.js extension (browser-only). Solved with dynamic imports and typeof window checks.\nXCM Address Encoding: Spent hours debugging \"Expected 32 bytes, found 48 bytes\" error. Learned SS58 addresses need decodeAddress() to convert to raw 32-byte account IDs for XCM messages.\nTestnet Token Availability: Getting testnet tokens was harder than expected. Faucets were deprecated, rate-limited, or broken. Had to find alternative faucets and coordinate multi-chain testing.\nAI Data Formatting: Initially, AI recommendations were generic. Had to refine prompts extensively to ensure the AI understood users already own these tokens and should focus on allocation/staking strategies, not purchase recommendations.\nZero Balance Visualiz\n\n**Accomplishments**\nReal XCM Transfers: Not a mock or demo‚Äîactual on-chain cross-chain asset transfers using Polkadot's XCM protocol\nAI That Understands Polkadot: Created an AI advisor that provides genuinely useful, ecosystem-specific recommendations about staking, XCM opportunities, and parachain strategies\nComplete Portfolio Analytics: Built a comprehensive health scoring system that goes beyond simple balance display\nWeb2 UX in Web3: Achieved a polished, responsive interface that feels modern and accessible to non-technical users\nPrivacy-First Architecture: No backend, no data collection‚Äîeverything client-side while still being feature-rich\nPerformance: Parallel queries load 4 chains simultaneously in ~2-3 seconds with graceful error handling\nFully Responsive: Works beautifully on desktop, tablet, and mob\n\n",
    "prize": "Winner Certified Polkadot Tinkerer",
    "techStack": "3.1, 8b, ai:, api, apis:, asset, astar, chain, chains:, coingecko, data), extension, frontend:-next.js-14, groq, instant, llama, moonbeam, network, paseo, polkadot, polkadot.js, pricing, protocol, recharts, relay, tailwind-css-blockchain:-polkadot.js-api, target, testnet), typescript, visualization:, wallet:, with, xcm",
    "github": "https://github.com/satishtamilan/polkadot-portfolio-ai",
    "youtube": "https://www.youtube.com/watch?v=aFCO6nQfPT4",
    "demo": null,
    "team": "Satish Kumar Anandhan",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/chainlink-portfolio-polkadot-cross-chain-activity-tracker"
  },
  {
    "title": "ConsilAI",
    "summary": "TECHNICAL HIGHLIGHTS: Notable technical implementations include the use of Azure for cloud services, BeautifulSoup4 for web scraping, and Playwright for automated testing. The project also employs a modern stack with Next.js and React for the frontend, ensuring a responsive and dynamic user experience, while leveraging Supabase for backend development and database management.",
    "description": "**What it does**\nConsilAI is an intelligent classroom management platform that combines student profiling, automated educational research, and AI-powered plan generation to help teachers create personalized learning strategies for every student. Core Features: Comprehensive Student Profiles:  Teachers create detailed profiles for each student including their issues (behavioral or academic challenges), strengths, goals, and contextual notes\nInteractive Drag-and-Drop Seating Chart:  Visual classroom layout simulator where teachers can arrange students and reason about optimal seating arrangements\nAutomated Research Scraping:  The system automatically searches and scrapes real educational research from across the web using Playwright and BeautifulSoup\nAI-Powered Learning Plans:  Azure Phi generates structured\n\n**Inspiration**\nThe inspiration for ConsilAI came from recognizing the overwhelming challenge teachers face in managing diverse classrooms where every student has unique needs, learning styles, and behavioral patterns. We've seen teachers struggle to provide individualized attention when they're responsible for 25+ students, each requiring different approaches. Furthermore, special education teachers and those working with neurodiverse students face even greater challenges in developing personalized intervention strategies quickly enough to make a real difference fast. We realized that while teachers have incredible expertise and dedication, they often lack the time to research and implement evidence-based strategies for each student's specific situation. ConsilAI was born from the vision of giving teache\n\n**How we built it**\nFrontend Architecture: Built with Next.js 14 using the modern App Router for optimal performance and routing\nReact 18 for component-based UI with server and client components\nTailwind CSS for rapid, responsive styling with utility-first approach\nImplemented drag-and-drop seating chart for intuitive classroom layout management\nCreated clean, accessible interfaces for student profile creation and viewing\nDesigned the Plans page UI ready for AI integration\nDeveloped a light and dark mode for the user's personal choice Backend & Database: Supabase for authentication, database, and real-time capabilities\nStructured data models for students, profiles, and generated plans\nSecure environment variable management for API keys AI Pipeline (packages/ai): TypeScript for type-safe AI integration code\nAz\n\n**Challenges**\nIntegrating Azure Phi: The AI doesn't always return perfectly formatted JSON. We spent a lot of time tweaking prompts and adding error handling (type validation, fallback mechanisms) to parse responses reliably. Web Scraping Development: Every educational website is structured differently. We had to make our scraper smart enough to extract actual content while filtering out menus, ads, and irrelevant content. Timing was tricky too since some pages load content dynamically, meaning we needed Playwright to wait for the right elements. Monorepo Coordination: We had TypeScript handling the frontend and AI calls, while Python did the scraping. Making sure they talked to each other smoothly took some planning, especially around passing data back and forth. Furthermore, there was the coordination\n\n**Accomplishments**\nEnd-to-End AI Pipeline:  Successfully built a complete pipeline from: student profile ‚Üí keyword extraction ‚Üí web scraping ‚Üí AI generation ‚Üí structured learning plan, all working together seamlessly and in real-time\nReal Research Integration:  Unlike generic chatbot AIs and their responses, our system pulls from actual educational research and search results on the web, making recommendations based on the best practices currently used\nMonorepo Architecture:  Our monorepo structure kept things organized even while rushing. The AI package, scraper, and frontend all have clear boundaries, making it way easier to debug and add features\nIntelligent Web Scraping:  We got web scraping working reliably across different educational sites, which is notoriously finicky. The fact that we can load pages\n\n**What we learned**\nTechnical Skills: Prompt engineering is an art, since getting Azure Phi to consistently return usable JSON took way more tweaking than expected\nNext.js 14's App Router is powerful but has a learning curve (less if coming from older React patterns, but still quite some)\nPlaywright is amazing for scraping modern websites that load content dynamically\nManaging a project with TypeScript and Python packages requires clear interfaces and good documentation\nSupabase makes auth and database stuff way easier than using your own About the Problem Space: Teachers deal with way more privacy regulations (like FERPA) than we initially thought\nActionable, specific advice matters more than research summaries\nTeachers want to see where information came from since trust is huge (especially in this subject m\n\n**What's next**\nMake It More Useful: Let teachers track which plans they've tried and how they're working\nLet teachers edit the AI-generated plans before saving them\nMake it work extremely well on tablets since teachers use those in class Improve the AI: Pull from more specialized education databases, not just general web searches\nAdd a feedback system so teachers can rate plan effectiveness and improve future suggestions\nBuild different strategies for different needs (ADHD, autism, gifted students, etc.) Make It Faster: Scrape multiple pages at once instead of one-by-one\nCache research we've already scraped so similar students don't trigger repeated searches\nShow AI responses as they're being generated instead of waiting for the whole thing Miscellaneous: Add better error handling when scraping fails or\n\n",
    "prize": "Winner Tour of Microsoft Reston; Winner Techno Basket",
    "techStack": "azure, beautifulsoup4, chromium, css, html, javascript, next.js, node.js, phi-4, playwright, python, react, supabase, tailwind, typescript",
    "github": "https://github.com/cchamb26/consilai",
    "youtube": "https://www.youtube.com/watch?v=jGOfytp5zUI",
    "demo": null,
    "team": null,
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/consilai"
  },
  {
    "title": "TaskTamer",
    "summary": "TECHNICAL HIGHLIGHTS: The project utilized a combination of base44 for backend architecture, Python for server-side logic, and React for creating a dynamic front-end experience. This stack allowed for a responsive user experience and efficient data handling.",
    "description": "**What it does**\nTaskTamer is a world alive with possibilities. It automatically creates a daily calendar for you based on the onboarding questions, integrating all your tasks, commitments, and personal preferences. It features duel mode, where milestones in your tasks translate into exciting real-time zombie battles; a thrilling way to stay accountable. Your city map grows and evolves as you complete tasks, turning everyday productivity into a personal adventure. Each zombie companion and mutation card represents your progress and health: tasks left unfinished become zombies that challenge your focus, while curing and advancing them reflects tangible growth. Analytics provide clear feedback on your progress, showing statistics and patterns in a playful, motivating way. Different themes let you personalise\n\n**Inspiration**\nI built TaskTamer out of a very personal frustration. Like many students, I have a to-do list that grows longer by the day, yet the apps I‚Äôve tried are dull, static, and uninspiring. They never change, never adapt, and quickly become forgotten. I wanted something different. I wanted an app that breathes life into productivity, that moves with me, that actually understands how students like me think and feel. TaskTamer is born from the Eisenhower Matrix, but taken a step further: it updates in real time based on deadlines and importance, nudging the most pressing tasks to the forefront. I know what it is like to struggle with motivation and stress, so I designed the app to create a genuine reward and gratification cycle. Each task completed earns you XP, powers up your zombie companion, and\n\n**How we built it**\nWe built TaskTamer using Base44, with all prompts and AI behaviours engineered by myself. The development process involved extensive market research, where 86% of students expressed excitement for an app like this. We debated multiple themes; basic trees, pets, zombies and landed on zombies because it is unique, fun, and appeals directly to teens‚Äô love of playful challenge. I poured thought into every detail, from calendar logic to duel mechanics, ensuring the app isn‚Äôt just functional, but immersive and enjoyable.\n\n**Challenges**\nConceptualising TaskTamer in its entirety was a challenge. There were so many ambitious ideas I wanted to include, from duels to city maps to dynamic calendars. Making duel mode feel like a real-time, live battle with milestones was particularly tricky. Animations and graphics for the zombie theme were also a challenge; I needed them to feel professional, engaging, and non-tacky, which required iteration and refinement.\n\n**Accomplishments**\nI am incredibly proud of the variety of themes, which allow the app to feel fresh and personal for any student. The automatic calendar is one of my favourite achievements, as planning my day is something I personally struggle with. I adore the zombie theme, which is quirky, fun, and entirely unique, as well as the random health alerts that gently remind students to breathe, stretch, or reflect; they are small, delightful touches that make the app feel alive. I also love how the Base Camp background changes by time of day, and the overall concept is something I have thought about for years. This hackathon gave me the motivation to finally bring it to life, and I am thrilled with how it turned out.\n\n**What we learned**\nThrough building TaskTamer, I learned how to turn a concrete vision into reality. I discovered how to expand ideas thoughtfully, balancing ambition with usability, and how to craft a product that truly delivers what students need. I now understand the value of iterative design, research, and making something both functional and magical.\n\n**What's next**\nLooking ahead, there are a few exciting areas I want to expand. I hope to enhance gamification in duels, adding more levels, animations, and city map interactions. There are also ideas to introduce new rewards, skills, and evolving challenges. Overall, I am so impressed with what the app has become; it is playful, supportive, motivating, and uniquely designed for students like me, and I cannot wait to see how it continues to grow.\n\n",
    "prize": null,
    "techStack": "base44, python, react",
    "github": "https://github.com/superiorlati/TaskTamer",
    "youtube": "https://www.youtube.com/watch?v=A_nyFxvKhiQ",
    "demo": "https://app-3f3fb448-3a43-49df-b3c0-5abf8ba35258.base44.app/",
    "team": "Ananya Gulati",
    "date": "2025-11-21",
    "projectUrl": "https://devpost.com/software/tasktamer-hetu2y"
  },
  {
    "title": "SafeWalk+",
    "summary": "TECHNICAL HIGHLIGHTS: The project was built using a robust technology stack that includes HTML, CSS, JavaScript, and React, allowing for a responsive and user-friendly interface. Key technical implementations may have included real-time geolocation tracking, user authentication systems, and integration with messaging services to facilitate communication between users and their safety networks.",
    "description": "**What it does**\nSafeWalk+ calculates routes for walking or cycling by balancing safety and efficiency. It uses a weighted safety score that accounts for infrastructure (sidewalks, bike lanes), street lighting, nearby amenities, crime levels, and disruptions like construction. Users can choose between the safest route or the fastest route, select a departure time, and view an interactive map with turn-by-turn guidance, highlighting streets according to their safety.\n\n**Inspiration**\nSafeWalk+ was inspired by the need for safer urban navigation for pedestrians and cyclists. In many cities, traditional navigation apps prioritize speed over safety, which can put users at risk in areas with poor lighting, high traffic, or elevated crime. We wanted to create an app that empowers people to confidently navigate their city by prioritizing safety without sacrificing convenience.\n\n**How we built it**\nWe built SafeWalk+ using a React frontend with Leaflet for interactive maps and a Node.js/Express backend for route calculations. Street and safety data are loaded from CSV files and supplemented with mock data for demonstration purposes. The backend constructs a graph of intersections and street segments, calculates weighted safety scores, and uses Dijkstra‚Äôs algorithm to determine the safest and fastest paths. The frontend displays routes with color-coding, allows users to input start/end locations, select departure times, and view upcoming turns and safety alerts.\n\n**Challenges**\nOne major challenge was ensuring the safest and fastest routes were distinct‚Äîinitially they were identical because we hadn‚Äôt separated distance and safety scoring in the algorithm. Integrating CSV data with mock data for streets also required careful handling to prevent missing or incomplete segments. Lastly, designing a clean, intuitive UI that could display multiple routes, turn directions, and alerts without clutter was tricky, especially for a hackathon timeframe.\n\n**Accomplishments**\nWe are proud of building a fully functional MVP that can calculate and display safe and fast routes on a map, even with limited data. The app dynamically adjusts route selection based on time of day, and our safety scoring formula accounts for multiple real-world factors. We also implemented an Apple-like interface for journey mode with turn-by-turn guidance and safety notifications, providing a professional, polished feel in a short time.\n\n**What we learned**\nWe learned how to integrate geospatial data from CSVs into a graph structure for routing, and how to apply weighted scoring to balance multiple factors like safety and distance. We also gained experience using React Leaflet for interactive map visualization and handling frontend-backend communication in real time. Finally, we improved our understanding of UI design principles for mobile-friendly, intuitive, and aesthetically pleasing applications.\n\n**What's next**\nNext, we plan to refine the backend algorithm to include real-time traffic data, dynamic crime alerts, and better time-of-day adjustments. We also want to enhance the frontend by allowing users to save favorite routes, integrate notifications for upcoming hazards, and expand to additional cities using live data feeds. Our ultimate goal is to make SafeWalk+ a go-to app for safe, confident urban navigation for pedestrians and cyclists everywhere.\n\n",
    "prize": "Winner Best Hack for Social Good",
    "techStack": "css, html, javascript, react",
    "github": "https://github.com/jashanbains-24/HackCampsProject",
    "youtube": "https://www.youtube.com/watch?v=WUHdUjCq7TA",
    "demo": null,
    "team": "Jashan Bains, Priyansh Bahri, Navi Sharma",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/safewalk-bneql3"
  },
  {
    "title": "Mela Pay",
    "summary": "TECHNICAL HIGHLIGHTS: The project utilizes a robust tech stack consisting of express.js, JavaScript, MongoDB, Next.js, Node.js, and Polkadot.js. This combination allows for a scalable and efficient architecture, providing real-time transaction capabilities and a user-friendly experience. The integration with Polkadot.js indicates a commitment to leveraging blockchain interoperability for enhanced security and functionality.",
    "description": "Cart page\n    \n\n              \n  \n\n\n\n            \n        \n          \n  \n\n        \n    \n      Landing Page\n    \n\n        \n          \n  \n\n        \n    \n      Courses Page\n    \n\n        \n          \n  \n\n        \n    \n      Cart page\n    \n\n              \n  \n\n\n\n            \n        \n          \n  \n\n        \n    \n      Landing Page\n    \n\n        \n          \n  \n\n        \n    \n      Courses Page\n    \n\n        \n          \n  \n\n        \n    \n      Cart page\n    \n1234    \n\n\n\n      \n  Mela Pay Project Story\n\nInspiration\n\nAccess to high-quality education is still limited in many parts of the world due to payment barriers. Many international platforms support only global payment methods like credit cards or PayPal, which many students in developing regions don't have.\n\nAt the same time, the Polkadot ecosystem provides fast, secure, low-fee transactions and a vision of global interoperability. That sparked a question:\n\nWhat if we could use Polkadot to pay for digital education anywhere in the world?\n\nMela Pay was born from this idea enabling students to buy courses (starting with edX) using DOT, directly from their Polkadot wallets, instantly and without barriers.\n\nWhat it does\n\nMela Pay is a Web3 payment gateway that converts DOT from the Polkadot network into USD for purchasing online courses.\n\nCore features\n\n\nConnect any Polkadot wallet (Polkadot.js extension, Nova integration planned)\nPurchase courses using DOT\nAutomatic currency conversion (DOT to USD)\nSecure on-chain transactions using Polkadot API\nReal-time transaction confirmation\nClean, simple checkout experience for education platforms\n\n\nIn short: Mela Pay allows students to buy courses with cryptocurrency ‚Äî easily, safely, and globally.\n\nHow we built it\n\nWe built Mela Pay using a modern Web3 stack.\n\nFrontend\n\n\nNext.js + React\nTailwind CSS for fast, responsive UI\nPolkadot.js extension-dapp for wallet integration\nReact Context API for global state management\n\n\nBackend\n\n\nNode.js / Express\nPolkadot.js API for signing and sending transactions\nCurrency conversion service for live DOT/USD pricing\n\n\nBlockchain\n\n\nPolkadot RPC (wss://rpc.polkadot.io) for chain connection\nweb3Enable, web3Accounts, web3FromAddress for wallet access\napi.tx.balances.transfer()` for secure on-chain transfers\n\n\nMathematical Conversion\n\nDOT to USD:\n\n[ \\text{DOT_required} = \\frac{\\text{USD_Price}}{\\text{DOT_USD_Rate}} ]\n\nDOT to plancks:\n\n[\\text{plancks} = \\text{DOT} \\times 10^{12}]\n\nThese formulas ensure accurate and consistent payment amounts.\n\nChallenges we ran into\n\n\nWallet compatibility\n\n\nNova Wallet does not directly sync with Polkadot.js extension. We had to design for:\n\n\nDesktop extension flow\nMobile wallet \n\n\n\nNext.js SSR vs CSR issues\n\n\nPolkadot.js relies on the window object, which does not exist in server-side rendering. This produced:\nReferenceError: window is not defined\n\nWe solved this by:\n\n\nLazy-loading Polkadot wallet functions\nEnsuring wallet interactions run only on the client side\n\n\n\nTransaction signing errors\n\n\nWe ha",
    "prize": "Winner Roots Community Meetup Prize",
    "techStack": "express.js, javascript, mongodb, next.js, node.js, polkadot.js",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=MhBIjCCSxVc",
    "demo": "https://mela-polkadot-project-gzok.vercel.app/",
    "team": null,
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/mela-pay"
  },
  {
    "title": "RENEW'D",
    "summary": "TECHNICAL HIGHLIGHTS: RENEW'D was developed using JavaScript and React, ensuring a responsive and dynamic user interface. The use of SQLite for data management suggests efficient handling of user data, while Tailwind CSS likely provided a modern and visually appealing design.",
    "description": "**What it does**\nRenew'd reimagines thrifting as an effortless, personalized experience. Users swipe through items matched to their style, sizing, and location‚Äîno cash, no shipping costs, just direct trades. Our matching algorithm pairs people based on what they're looking for and what they have to offer, keeping fashion circular while reducing waste and expanding wardrobes. Like something? Match with the owner, chat in-app, and arrange a trade. A world of preloved items now at your fingertips. What makes us unique: Matching algorithm: dating-app meets marketplace\nCash free: item-for-item trading, no shipping costs\nSmart results: pairing users based on sizing, style and location\n\n**Inspiration**\nFast fashion is killing the planet. Millions of tons of textiles hit landfills every year while perfectly good clothes sit unworn in closets. Buying new is expensive, and existing thrift platforms are overwhelming catalogs with zero personalization. We saw an opportunity to turn your closet into someone else's favorite piece of clothing, and make sustainable fashion actually feel good. Why should finding your next favorite piece be harder than finding your next date?\n\n**How we built it**\nWe started by mapping complete user flows from sign-up through listing creation, swiping, matching, and trade completion. The database architecture uses relational structures connecting users, listings, likes, matches, and messages with optimized queries for real-time feed generation. The frontend centers on gesture-based swipe mechanics with smooth card animations and an intuitive, accessible UI with readable layouts and size-inclusive tags. The backend implements a weighted scoring system that normalizes location distance, size compatibility, and style preferences‚Äîboth explicit (tags, requirements) and implicit (swipe history, interaction patterns). We designed the system around mutual matching rather than buyer-seller dynamics, which fundamentally changed how we approached transaction f\n\n**Challenges**\nBalancing algorithm feature weights across heterogeneous data types was complex‚Äîlocation needs distance-based scoring, style requires categorical matching, sizing demands precision. We had to optimize database queries joining multiple tables while filtering already-seen items and maintaining performance. The biggest challenge was rethinking marketplace dynamics for trades instead of purchases, which meant designing for mutual interest and equal exchange rather than one-directional transactions. Time constraints forced us to prioritize the core matching pipeline over features like trade histories, reputation systems, and community tabs.\n\n**Accomplishments**\nWe successfully translated dating app engagement mechanics into a functional trading marketplace that solves real environmental problems. The database schema we built scales efficiently despite complex many-to-many relationships across users, items, and interactions. Our matching algorithm framework uses collaborative filtering techniques that improve recommendations over time as it learns from user behavior. We created comprehensive user flows covering every edge case and state transition. Most importantly, we proved that making sustainability effortless and fun can drive meaningful behavioral change without sacrificing functionality.\n\n**What we learned**\nWe gained hands-on experience with recommendation systems, feature engineering, and multi-dimensional scoring algorithms. The project reinforced how critical database indexing and query optimization are for apps with complex relational data. We learned to adapt UX patterns between domains while preserving their psychological effectiveness‚Äîchanneling the dopamine hit of swiping toward sustainable behavior. Building for trades rather than sales taught us to think differently about incentive structures, mutual value creation, and transaction completion. We also learned that accessible design and inclusive features (like size-inclusive tags) aren't just nice-to-haves‚Äîthey're essential for building platforms that serve diverse communities.\n\n**What's next**\nWe're completing the MVP and launching a beta to gather real user data for algorithm refinement. Next comes machine learning integrationÔºõcollaborative filtering for better recommendations and computer vision for automated item tagging and style recognition. We're building features like profile viewing, bundling multiple items in trades, like history tracking, and community tabs. The technical roadmap includes horizontal scaling with caching layers, materialized views for feed generation, and message queues for real-time notifications. We'll implement trust and safety features including reputation scoring and trade verification. Long-term, this matching framework could extend beyond fashion to furniture, electronics, books‚Äîany marketplace where discovery beats search and sustainability matt\n\n",
    "prize": "Winner Finalist",
    "techStack": "javascript, react, sqlite, tailwind",
    "github": "https://github.com/enyazvn/HackCamp",
    "youtube": "https://www.youtube.com/watch?v=VqgQut_r_Zo",
    "demo": "https://docs.google.com/presentation/d/13whQi02Gx5GyU1AaskrtMNUMuHkk-tTvtzTM8AFgTeE/edit?usp=sharing",
    "team": "Enya Zeng, Arseniy Dolgov, Kiana Faden, Kylie Seto",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/renew-d"
  },
  {
    "title": "NeuroAccess: AI-Powered Cognitive Equity Platform",
    "summary": "NeuroAccess is an AI-powered platform designed to enhance cognitive equity by leveraging real-time EEG signal processing. The project aims to provide personalized insights into cognitive health and accessibility, enabling users to better understand and manage their cognitive capabilities.",
    "description": "123456    \n\n\n\n      \n  üåü Inspiration\n\n\"While teaching students, I witnessed brilliant minds being left behind due to undiagnosed learning barriers and cognitive stress. Current neurotechnology remains locked in expensive clinics, inaccessible to 80% of the global population. My research on gamma band EEG and low-cost neurotech showed me we can do better ‚Äî we can bring cognitive assessment and support to everyone.\"\n\n\n\nüí° What it Does\n\nNeuroAccess is an offline-first AI platform that transforms affordable EEG headsets ($20-50) into cognitive accessibility tools. It:\n\n‚úÖ Detects learning barriers in real-time through EEG gamma band analysis and behavioral monitoring\n‚úÖ Adapts educational content based on cognitive state (attention, stress, engagement)\n‚úÖ Provides teachers with XAI-powered insights about student cognitive needs\n‚úÖ Works completely offline for rural/remote communities with limited internet  \n\n\n\nüõ† How We Built It\n\nArchitecture:\n\n\nFrontend: React + Flutter mobile app\nAI Backend: Python + TensorFlow Lite for edge deployment\nEEG Processing: MNE-Python + custom gamma band analysis pipeline\nML Models: Hybrid CNN-Transformer architecture (adapted from my NeuroGuard research)\nXAI: SHAP/LRP for interpretable cognitive insights\nHardware: Low-cost NeuroSky MindWave integration\n\n\nTechnical Innovations:\n\n‚úÖ Real-time gamma power modulation tracking (based on published research)\n‚úÖ Multimodal data fusion (EEG + webcam-based eye tracking)\n‚úÖ Federated learning capability for privacy preservation\n‚úÖ Offline-first design with 50MB footprint  \n\n\n\n‚ö† Challenges We Ran Into\n\n\nProcessing constraints on low-cost devices required optimizing CNN-Transformer model by 60%\nEEG artifact removal in noisy classroom environments demanded innovative ICA filtering\nReal-time gamma analysis at edge devices required rewriting MNE-Python pipeline in C++\nCultural adaptation of cognitive benchmarks for diverse communities\n\n\n\n\nüèÜ Accomplishments We're Proud Of\n\n‚úÖ Achieved 89.3% accuracy in detecting attention states vs. clinical gold standard\n‚úÖ Reduced cost from $1000+ to $23/student for cognitive assessment\n‚úÖ Built fully functional prototype in 6 days despite hardware limitations\n‚úÖ Designed culturally-adaptive cognitive benchmarks for multiple regional variants  \n\n\n\nüìö What We Learned\n\n\nGamma band modulation is a reliable indicator of cognitive engagement across populations\nTeachers in low-resource settings desperately need objective cognitive data but lack tools\nOffline AI deployment requires different architectural thinking than cloud-based systems\nExplainable AI isn't optional ‚Äî teachers need to understand why a student is struggling to help effectively\n\n\n\n\nüöÄ What's Next for NeuroAccess\n\n‚úÖ Clinical validation partnership with national hospitals (already in discussion)\n‚úÖ Pilot deployment in 3 rural schools (Q1 2026)\n‚úÖ Integration with government education platforms in developing nations\n‚úÖ Research publication on novel gamma band cognitive engagement metrics  \n\n\n\n        \n    B",
    "prize": null,
    "techStack": "edge-computing, eeg-signal-processing, explainable-ai, figma, firebase, flutter, mne-python, node.js, python, pytorch, react, tensorflow",
    "github": "https://github.com/senushidinara/neuroaccess-",
    "youtube": "https://www.youtube.com/watch?v=4DL3CPq0sgw",
    "demo": "https://691a4697edf05dc357139391--guileless-dieffenbachia-ab248d.netlify.app/",
    "team": null,
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/neuroaccess-ai-powered-cognitive-equity-platform"
  },
  {
    "title": "Big Boy With A Fez",
    "summary": "\"Big Boy With A Fez\" is an interactive installation that combines physical and digital elements, allowing users to engage with an animated figure equipped with a fez. Utilizing sensors and programming, the project creates a playful and immersive experience, where the character responds to user interactions in real-time.",
    "description": "**What it does**\nHe's a big boy that wears a fez! He can speak and move his head, and his mouth moves with it! He's controlled by a remote. He's powered by an Arduino Mega 2560, plus a servo, an ST7735 TFT display, and an IR receiver!\n\n**Inspiration**\nI was inspired by Doctor Who, specifically the eleventh doctor, who wore a fez. His suit was inspired by Kingpin from Spider-Man: Into the Spiderverse. I was also inspired by the cardboard boxes.\n\n**How we built it**\nI built it using Arduino code to control everything except sound, which is controlled by a Python script over serial on my MacBook.\n\n**Challenges**\nI didn't have very much time to make him! I had probably 6 hours total. He's more complicated than you would think.\n\n**Accomplishments**\nThere was a point I didn't think I'd have a project because I was so behind. But I have a project! Huzzah!\n\n**What we learned**\nI've learned to have an idea next time going into this, and to use my time wisely!\n\n**What's next**\nHe's going to continue to be a big boy wearing a fez. The Python file was completely generated with AI (ChatGPT). The Arduino code was not and is released under the GNU GPLv3.\n\n",
    "prize": "Winner Interactive Media Arts/Entertainment",
    "techStack": "arduino, cardboard, display, ir, python, servo, tft",
    "github": "https://github.com/danieliscrazy/BigBoyWithAFez",
    "youtube": "https://www.youtube.com/watch?v=oP9MieDAlJU",
    "demo": null,
    "team": "Daniel Davidson",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/big-boy-with-a-fez"
  },
  {
    "title": "Zeural",
    "summary": "TECHNICAL HIGHLIGHTS: Zeural utilizes a robust tech stack including CSS, HTML, and JavaScript for front-end development, while leveraging Firebase for real-time database and authentication functionalities. The use of GitHub workflows indicates a commitment to continuous integration and deployment, which enhances the project‚Äôs reliability and scalability.",
    "description": "**What it does**\nZeural is an open-source learning platform that combines: Educational content: AI/ML tutorials, programming lessons, and research summaries.\nFun engagement: Mythology stories to refresh learners between intense topics.\nResearch integration: Cron job fetching the latest arXiv papers, with chatbot explanations.\nAccessibility & customization: Theme options for learners with different needs and upcoming multilingual support.\nCommunity contributions: Anyone can add content or improve the platform.\n\n**Inspiration**\nI noticed that learners often get bored when studying complex topics for long periods. To keep engagement high, I integrated mythology stories in between lessons‚Äîfun, short stories that let learners take mental breaks while staying curious. Additionally, I wanted to make research papers more accessible, since my first experience reading them was challenging. This inspired me to add a chatbot below each research paper to help learners understand the content more effectively.\n\n**How we built it**\nFrontend: React.js, TypeScript, Tailwind CSS\nBackend: GitHub Jekyll and Firebase.\nImplemented Jekyll so that it is easier for others to contribute\nDatabase: MongoDB / PostgreSQL ‚Äî stores content, user progress, and contributions.\nContent Management: Markdown-based with Git versioning for easy contribution.\nSpecial features: Dynamic research paper fetching, chatbot explanations, theme section, and an upcoming Zeural learner community.\n\n**Challenges**\nBalancing learning content with engaging, fun experiences.\nIntegrating research papers dynamically while keeping them easy to understand.\nDesigning a frontend that keeps learners motivated for long periods.\nPlanning for community contributions and a scalable platform.\n\n**Accomplishments**\nSuccessfully integrated mythology stories to maintain learner engagement.\nBuilt a chatbot-assisted research paper reader for better understanding of complex papers.\nCreated a fully open-source platform where contributors can improve content and features.\nDeveloped a frontend that‚Äôs visually engaging and user-friendly.\n-Search bar for better navigation\n\n**What we learned**\n-How to make learning better and make it in a way those to learn can contribute later to the same. How to combine diverse content types into a cohesive platform.\nHandling real-time data fetching and chatbot integration.\nImportance of user engagement and interface design in education.\nThe value of **community-driven, open-s\n\n**What's next**\nImplement multilingual support to reach a global audience.\nLaunch a dedicated Zeural learner community for collaboration and knowledge sharing.\nExpand content across more domains while maintaining engagement and interactivity.\nContinuously improve the chatbot and research integration for easier understanding of papers.\n\n",
    "prize": "Winner Participation Prize; Winner Top 24-85",
    "techStack": "css, firebas, firebase, githubworkflows, html, javascript, jekyll, python",
    "github": "https://github.com/zeural1/zeural1.github.io",
    "youtube": "https://www.youtube.com/watch?v=s4CHMLAAV9Y",
    "demo": "http://zeural1.github.io/",
    "team": null,
    "date": "2025-11-17",
    "projectUrl": "https://devpost.com/software/zeural"
  },
  {
    "title": "PolkaShield",
    "summary": "PolkaShield is a blockchain project built using Rust and Substrate, aimed at enhancing security and privacy in decentralized applications (dApps) within the Polkadot ecosystem. Its primary focus is to provide robust protection mechanisms against vulnerabilities and attacks that dApps may encounter in a multi-chain environment.",
    "description": "**What it does**\nPolkaShield introduces a decentralized, cryptographically verifiable access-control layer for Web2 applications. It includes: A custom Polkadot chain that stores roles, permissions, and audit logs\nA dashboard where admins can create roles and assign permissions\nA public API gateway that lets Web2 apps verify access using a simple call like:\n\n/verify-access?user=<address>&role=editor\n\nA demo ‚ÄúEditor Portal‚Äù that shows how permissions unlock or restrict actions in real time Users own their permissions through wallet signatures, and developers get a reliable, tamper-proof way to manage access.\n\n**Inspiration**\nEvery Web2 application, from school portals to admin dashboards to platforms, relies on a centralized access-control model, often reduced to a single database flag like is_admin = true.\nThat tiny line silently governs permissions, authority, and trust. But because it lives inside a private server, it creates a dangerous single point of failure.\nIf the database goes down, gets corrupted, or is compromised, the entire access-control system collapses. I kept asking myself: What if permissions were user-owned instead of server-owned?\nWhat if they were tamper-proof, verifiable, and portable across applications? When I discovered the Polkadot SDK‚Äôs modular blockchain design, the solution became obvious.\nWe could decentralize access control itself, not as another token or DeFi concept, but as rea\n\n**How we built it**\nPolkaShield is composed of four coordinated components:\n\n**What's next**\nA clean UI for: Connecting wallets\nCreating roles\nAssigning roles\nViewing logs\n\n",
    "prize": "Winner Third prize - All Themes",
    "techStack": "rust, substrate",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=ExB94irX7-o",
    "demo": null,
    "team": "Fred Gitonga",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/polkashield-og8vky"
  },
  {
    "title": "Involvee Times",
    "summary": "TECHNICAL HIGHLIGHTS: The project utilizes a robust tech stack, including exa for backend management, OpenAI for intelligent features, PostgreSQL for reliable data storage, and React combined with TypeScript for a responsive and scalable front-end. The use of tRPC facilitates seamless communication between the frontend and backend, enhancing overall performance.",
    "description": "**What it does**\nInvolvee cuts the friction, finding bills that will directly impact the user, finding events the user could make an impact at, and helping the user voice their opinion.\n\n**Inspiration**\nWe've experienced firsthand how difficult it is to get involved with local community. Whether it is trying to volunteer, learn about bills that will directly impact you, as well as other general state legislation, there's no good way to find information for you specifically.\n\n**How we built it**\nInvolvee Times uses full-stack Typescript, with React for the frontend, and Express for the backend. tRPC is used to communicate between the two, with Exa and GPT-4o for the search, Twilio and Sendgrid for emailing users, and Postgres for the database. OpenStates was used for the state legislation data.\n\n**What's next**\nOur first goal would be to improve the search, to find more options, especially options that are hidden behind pages of search results. We also would like to have more specific filters for users regarding email updates, with the goal to add the ability to filter via tags, categories, etc.\n\n",
    "prize": "Winner Sony WH-1000XM4",
    "techStack": "exa, openai, postgresql, react, trpc, typescript",
    "github": "https://github.com/Bardemic/sous-teach",
    "youtube": "https://www.youtube.com/watch?v=xDhwR7GmhKQ",
    "demo": null,
    "team": "Brandon Pieczka, Luke Patterson, jbaccam Baccam",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/_-tzw4gb"
  },
  {
    "title": "The Voyage",
    "summary": "The Voyage is an immersive web application that utilizes 3D graphics and animations to create an engaging user experience. It leverages modern web technologies to provide a visually captivating journey, potentially showcasing storytelling or educational content in an interactive format.",
    "description": "**What it does**\nAn interact-able 3D website that allows users to explore the planets and collect data by performing tasks. The data collected about planets are stored and can be accessed at any time. The user is in the first person view and is a pilot flying through the void of space.\n\n**Inspiration**\nBrowsing the NASA website and being able to explore the different planets, and there was a 3D model of a planet that I could explore. I wondered if I could make an immersive experience that can allow users to explore space from the comfort of their own home.\n\n**How we built it**\nUsing React three fiber and GSAP to blend 3D and 2D ui elements together. The model was created by hand in blender, and camera controls and lighting were added to it in React. Then the UI is set over the 3D canvas which allows for interactivity.\n\n**Challenges**\nA lot of issues came from dialogues not showing the correct text so this was solved by creating a dialogue system that relied on knowing the current planet and keeping track of the dialogue index.\n\n**Accomplishments**\nThe project overall, adding all the elements together to create that immersive space exploration experience. Being able to replicate a space ship style vibe with the hologram styling and ambience sound effects really made sitting in that cockpit just a bit more realistic.\n\n**What we learned**\nHow to combine 3D and UI elements to create web interactivity, as well as adding sound and animations for a more engaging user experience.\n\n**What's next**\nAdding the rest of the planets, more ways to collect data, and maybe even expanding further than the solar system.\n\n",
    "prize": "Winner Overall Winner",
    "techStack": "drei, gsap, postprocessing, react, tailwindcss, three.js, typescript",
    "github": "https://github.com/HubertYu03/TheVoyage",
    "youtube": "https://www.youtube.com/watch?v=-eux9uhE4J0",
    "demo": "https://the-voyage-seven.vercel.app/",
    "team": "Hubert Yu",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/the-voyage"
  },
  {
    "title": "EqualEyes",
    "summary": "TECHNICAL HIGHLIGHTS: The project was built using Claude, along with standard web technologies such as CSS, HTML, and JavaScript. Notable technical implementations may include dynamic content adjustments based on user feedback, intuitive UI design for ease of navigation, and efficient use of APIs for real-time accessibility enhancements.",
    "description": "**What it does**\nOur Chrome extension allows you adjust certain features, such as contrast and text of the website based on standards inspired by the Web Content Accessibility Guidelines (WCAG). These guidelines aim to define ‚Äúaccessibility‚Äù with tangible, testable design choices to consider diverse needs.\nBecause every website is designed differently, and everyone has unique needs, one solution does not fix all. So, EqualEyes offers different options that can be toggled at the user's discretion: Dark mode\n\n\nDarkens the background and makes the text light, so you get a softer, but higher-contrast view that‚Äôs easier on the eyes.\n\nHigh contrast\n\n\nTurns the background pure black and the text white to maximize contrast.\nThis is a more intense option for people who need really strong color distinction to read c\n\n**Inspiration**\nWe wanted to create something that improves accessibility or inclusivity while considering the context of ourselves and our peers. As students, we are constantly staring at our laptops, and often studying at night in the dark, while our laptop screen continues to blare at us. But beyond our own privileged experiences growing up able-bodied, we wanted to consider the unique but very relevant struggles of others, such as individuals who are visually impaired or are simply more sensitive to poorly designed website displays. That is where our idea was born! We wanted to build a Chrome extension allows users to adjust certain features to enhance visibility, and in turn, accessibility.\n\n**How we built it**\nUsing JavaScript, HTML, CSS, and online tutorials/debugging with Claude. Lots of struggle. Lots of headache.\n\n**Challenges**\nA big challenge we ran into while creating this project is that most of us were using languages we had never coded in before until Learn Day. So, it was kind of stressful and a big learning curve getting accustomed to the syntax and conventions. Additionally, we had minimal experience in web design or development in general. Taking what we learn solely in school/on our own and applying it to a real-world circumstance was... daunting.\nWe had other features (such as using AI to create descriptions to be read by alt-text for images that do not already have it, to be inclusive of visually-impaired users) we wanted to implement that included APIs, but after around 5-6 hours of attempting, it still wasn't working out, so we decided to focus on other features.\n\n**Accomplishments**\nWe are proud of creating a functioning website that is relatively aesthetically pleasing. We had to be very meticulous and persistent in order to link the files we created separately and standardize their styles. Additionally, we are proud of the logo we made as we feel it added a touch of personality and cuteness to the design. Most importantly, we are proud of our developer for making the actual Chrome extension that allows users to toggle different buttons to apply the different effects (the ones that enhance contrast and font). She did not sleep. She had a nosebleed at 7am. She needs to sleep.\n\n**What we learned**\nWe were exposed to new technical skills this weekend such as web design and development. We gained significant experience (compared to our null starting point...) in new, or slightly familiar languages (JavaScript, HTML, CSS). Additionally, it was satisfying to see the application of what we learned at LearnDay (Saturday morning) to BuildNight (just a few hours later!).\n\n**What's next**\nWe want to add more features to enhance visibility and accessibility for an even more diverse pool of needs. This looks like building on the alt-text using AI descriptions idea. We would also like to look at more criteria for the standards given by WCAG, so we can make all websites on the internet equally accessible. The future of EqualEyes also looks like building on current limitations, such as if the background or text is an image or something more complex than a solid color. In the end, we would like for EqualEyes to be used on a larger scale, daily, to improve the lives, eye-health, and access to information (e.g. graphs, other informational images) for all people.\n\n",
    "prize": "Winner Best Pitch",
    "techStack": "claude, css, html, javascript",
    "github": "https://github.com/jjessicatsai/EqualEyes",
    "youtube": "https://www.youtube.com/watch?v=uIzLU9dT590",
    "demo": null,
    "team": "jessica tsai, Angela Li, Sara A, Tara A",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/equaleyes"
  },
  {
    "title": "Tuffr",
    "summary": "TECHNICAL HIGHLIGHTS: The project is built with a robust stack that includes Adafruit for hardware integration, Cloudflare for secure and scalable web infrastructure, and OpenCV for advanced image processing. The use of PlatformIO emphasizes a strong focus on embedded systems development, while React provides a dynamic user interface, ensuring a seamless user experience.",
    "description": "**What it does**\nTuffr lets you choose a workout and sends that information to our AI agent. Using a computer vision model, it tracks your movements and gives you real-time feedback on your form, along with suggestions for improvement. On the left side, you‚Äôll also find a chatbot you can ask for guidance, whether you want to know what to do next or get tips to improve your lift.\n\n**Inspiration**\nWe set out to make scientific training accessible to everyone. No one should ever feel embarrassed or out of place in a gym; everyone deserves the chance to learn, grow, and train with confidence. To solve this, we created Tuffr to help people analyze their lifts and make sure they can train without injury.\n\n**How we built it**\nWe built Tuffr with a React frontend enhanced by smooth animations using Lenis. Our backend is powered by a Python-based AI agent and chatbot. For computer vision, we use OpenCV with YOLO to detect key points and calculate angles that represent the user‚Äôs form. This information is then sent to the frontend, where we display the user‚Äôs form score in real time.\n\n**Challenges**\nWe've had various problems with the ECG electrode configuration and the other software bugs, and we overcame these issues very quickly and were able to create a cohesive product that was able to successfully measure and give data on a person's workout. To go more in-depth, the ECG pads output a very low-strength voltage signal, which we had to amplify using op amps to get appropriate readings.\n\n**Accomplishments**\nOverall our team successfully built the project and was able to give analytics on how the workout went. We increased accuracy for the computer vision model.\n\n**What we learned**\nWe learned a lot about computer vision and how we can apply AI to lifting and healthcare.\n\n**What's next**\nWe want to increase the accuracy even more and allow for more sensitive outputs on the form progression bar.\n\n",
    "prize": "Winner Best Use of Nodal CLI; Winner Best Use of Qualcomm EdgeAI",
    "techStack": "adafruit, cloudflare, opencv, openserver, platformio, react",
    "github": "https://github.com/The-Bettr-Tech/FrontEnd",
    "youtube": "https://www.youtube.com/watch?v=ZHPY_1P-ulU",
    "demo": "https://tuffr.tech/",
    "team": "Will Pelech, Eric Dong",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/tuffr"
  },
  {
    "title": "StationScope",
    "summary": "TECHNICAL HIGHLIGHTS: StationScope employs a diverse tech stack, including CSS for styling, JavaScript and TypeScript for interactive web features, Python and R for data analysis, and integration with Google Maps and MapQuest for geospatial functionalities. The incorporation of US Census Bureau data allows for demographic insights, enhancing the project‚Äôs analytical depth.",
    "description": "**What it does**\nIt is a fully functional dynamic app. You type in the desired address in the first landing screen, and Google Maps API helps you find a valid address. The app identifies the closest station to your input address as well as maybe additional close stations. Behind the scenes, the app pulls demographic summary stats using a U.S. Census Bureau API for all stations on the same lines as the closest station. We use census tracts partially or fully within a 1/4 mile radius of each subway station and the 2019-2023 5 Year ACS. This information is fed into the bot as reliable quantitative data (less prone to the hallucinations large market AI models can have with numbers). You are prompted to click to on a chat button, after which you can see an interactive map to get a good sense of geographical lay\n\n**Inspiration**\nFinding apartments as an intern or new grad moving to NYC is a daunting task. \nI know it. I lived it 4 months ago when I came to the city for an econ consulting role. \nI would have many pages open on Zillow, late nights compiling spreadsheets, and trying my best to avoid unnecessary transfers but failing. I found ChatGPT not very helpful at suggesting me neighborhoods. Since I had little idea of NYC geography, it did not give me a good sense of distance from work without opening up several other tabs of Google maps. (Real experience from Max and speaking with other new grads in NYC) This is why we created StationScope, an interactive chatbot robust against AI hallucinations by pulling reliable data from the U.S. Census Bureau and MTA for quantitative data measures. It is a custom AI with w\n\n**How we built it**\nTo develop the app, we had two main workstreams: the frontend + data modeling in R which Max handled, and the agentic bot and map, which Daren handled. The goal is to educate the new mover to NYC about their options and make a structured recommendation, much like a consultant would. Anyone can understand the inputs and outputs to this app, even though the infrastructure is a complex web of Javascript, R, and API links and endpoints.\n\n**Challenges**\nCoordinating our differing expertises: Daren has more skills in fullstack development, Max has more skills in data science. Smoothly integrating the APIs took some time and a lot of iteration. We slept for 2 hours.\n\n**Accomplishments**\nThis is Max's first hackathon!\n\n**What we learned**\nAlways plan for more time than it seems. Bugs pop up out of nowhere all the time. R backend plus JSON is an unusual combination but it works with a very helpful R library called \"plumber.\" Commit and push often!!!\n\n**What's next**\nMaking the AI model even more robust. It turned out pretty well, we think, having only one day (and long night) to work on it, though. We also want to finalize the report output and potentially host the site for the public to use.\n\n",
    "prize": "Winner Education; Winner Aristotle Challenge: Most Cutting-Edge AI Agent",
    "techStack": "css, geocoding, google-maps, javascript, json, mapquest, python, r, typescript, us-census-bureau",
    "github": "https://github.com/hellomaxlee/hack-nyu",
    "youtube": "https://www.youtube.com/watch?v=hL5gwPF4RK0",
    "demo": null,
    "team": "Maxwell Lee, Daren Hua",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/stationscope"
  },
  {
    "title": "Study Desk Assistant",
    "summary": "The Study Desk Assistant is an innovative project designed to enhance the study environment by monitoring and optimizing conditions for better focus and productivity. Utilizing a combination of sensors and artificial intelligence, the system can provide real-time feedback on variables such as temperature, humidity, and noise levels, assisting users in creating an ideal study atmosphere.",
    "description": "**What it does**\nThe Study Desk assistant utilizes sensor readings (temperature, humidity, light, sound) and data and displays them to a dashboard easily accessible by the user to get a quick look at their environment, also sending phone notifications and emails to the user when there are major changes in the sensor readings to better alert them and giving tips on how their can improve their environment with the end goal of giving insight to the user and making them more productive and focused.\n\n**Inspiration**\nMany students struggle with being focused and productive and unbeknownst to them, it can be tied back to their study environment. Barley any reliable tools have been make for student with the sole purpose of being able to notify students of their study environment with temperature, humidity, light and sound levels, whilst also sending notifications and giving tips on how ot better improve their environment.\n\n**How we built it**\nI began by utilizing sensors, DHT11 for the temperature and humidity, Photoresistor for the light level, and a KY-037 for the sound level. I have connected these sensors to an Arduino Uno R3 and used Serial Communication to take the sensor readings and parse them into a JSON format in C++, which then was accessed by my Raspberry Pi 5. Using Node-Red on the Raspberry Pi and connecting my Arduino Uno to it, Node-Red was able to access the Serial Communication I had built earlier and used the parsed JSON format to extract specific sensor values which were outputted as clean graphs easily accessible by the user. I have also accessed a local server, which allowed me to send notifications to my phone and send emails about major changes in the environment whilst giving me tips.\n\n**Challenges**\nThe one of the biggest challenge I had run into was the creation of the phone notification and email tips system. Learning how to properly access the sensor values in a way that would check them whilst also keeping track of major changes of the sensors over time to give proper tips that were not repetitive made me think outside the box so that I would not get extreme spammage of notifications just from an extremely small change detected.\n\n**Accomplishments**\nThough I had built multiple projects dealing with sensors and graphs, this was the first time I had properly incorporated all of them together into one seamless app. This was also my first time playing around with a notification and email system, and I am proud to say that it has worked much better than I could have ever expected, being able to access a server and give the user insight leverage this project to a whole nother level.\n\n**What's next**\nI would love to add Alexa integration to this project, eliminating the need of the distraction of looking at another app in the middle of your work to get the sensor values of your room. Having an Alexa connected to this would allow someone to simply ask their Alexa for the sensor values and it can give them a quick summary so they can continue their work without being distracted. Notifications would also work much better, as an Alexa would eliminate the need of looking at your phone for update which could lead to another doom scroll session (I am very guilty of this).\n\n",
    "prize": "Winner Top 4-23",
    "techStack": "arduino, ardunio-uno, c++, chatgpt, dht11, javascript, ky-037, node-red, photoresistor, raspberry-pi",
    "github": "https://github.com/LloydTheCoder/Study-Desk-Assistant",
    "youtube": null,
    "demo": null,
    "team": null,
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/iot-desk-assistant"
  },
  {
    "title": "Understandly",
    "summary": "TECHNICAL HIGHLIGHTS: The project utilizes CSS for styling, HTML for structure, and JavaScript for interactivity, enabling dynamic content presentation. Notable implementations may include responsive design for various devices, seamless animations for transitions, and perhaps the use of APIs to pull in relevant data or resources.",
    "description": "The Solution\n\nUnderstandly is a web application designed to evaluate and deepen your understanding of key concepts from each chapter‚Äînot by testing memorization, but by assessing true comprehension. After each lesson, the platform gives you:\n\nA concept-based quiz to ensure you‚Äôve fully understood the material.\n\nA practical project to apply what you‚Äôve learned in a real-world scenario.\n\nThe goal is to help learners build job-ready skills, not just pass exams.\n\nCurrent Progress\n\nI‚Äôve built a simple prototype where users can choose a subject and lesson (currently only computer science). Each topic includes a small, comprehension-based quiz. While the demo uses a simple concept, the long-term aim is to cover complex, university-level topics to help students genuinely understand the material‚Äîsomething I haven‚Äôt seen other platforms do effectively.\n\nWhat Makes It Unique?\n\nMost learning platforms rely heavily on memorization. Understandly focuses on mastery and application. This isn‚Äôt another Khan Academy, Brilliant, or Duolingo. Instead, it aims to challenge learners to think critically, solve problems, and truly internalize concepts through hands-on implementation.\n\nAI Tools Used\n\nGitHub Copilot helped generate the initial structure of the website.\n\nChatGPT supported me in refining the idea, choosing the tech stack, clarifying the user experience, and structuring the web app.\n\nWhat‚Äôs Next for Understandly\n\nMy next steps include:\n\nExpanding the CS curriculum\nAdding a wide range of computer science lessons based on what universities teach‚Äîdata structures and algorithms, operating systems, networks, and more.\n\nIntroducing more subjects\nAfter building a solid foundation in CS, I plan to add physics, mathematics, economics, and other fields.\nTo ensure high-quality content, I will study these topics through books, expert lectures, and academic events. With financial support, I would also collaborate with subject-matter experts or respected professors to create accurate and reliable learning material.\n\nTeam Members\n\nChirculete Mihai\n\n\n\n        \n    Built With\n\n    csshtmljavascript\n  \n\n        \n    Try it out\n\n    \n        \n  \n  GitHub Repo\n\n        \n  \n  chirculetemihai.github.io",
    "prize": null,
    "techStack": "css, html, javascript",
    "github": "https://github.com/ChirculeteMihai/Understandly",
    "youtube": "https://www.youtube.com/watch?v=9rsFjbyaxmY",
    "demo": "https://chirculetemihai.github.io/Understandly/",
    "team": "Chirculete Mihai",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/understandly"
  },
  {
    "title": "BOBUX BARGAINS",
    "summary": "TECHNICAL HIGHLIGHTS: The project was built using Luau, a scripting language for Roblox, and leveraged tools such as OpenRouter for AI functionalities. The integration of AI agents likely facilitated dynamic interactions within the game, enhancing player engagement through personalized shopping experiences.",
    "description": "**What it does**\nBobux bargains simulates a real world economy including supply and demand and financial transactions, on Roblox! There are two kinds of AI agents at play in our project, Buyers and Sellers. Buyers simulate buyers of goods and services in the real world who want to find the best price available for a product in a good and services market. Sellers are businesses that have the product buyers want and aim to earn the most profit out of each transaction. Each AI agent is fully customizable and can use a variety of AI models using the Open Router API. This allows the application to test the reasoning and negotiation capabilities of each model. Bobux Bargains is also scalable, allowing more accurate tests and economic modelling.\n\n**Inspiration**\nWhenever we think about finance software, we think of boring, unintuitive software, with exorbitant licensing costs and little accessibility or ease to the general public to understand. Oftentimes, processes are also hard to visualize. Therefore, we want to combine the power of AI agents and common variety game engines to simulate a market.\n\n**How we built it**\nFirst, our environment of choice is Roblox and Roblox Studio, as the games produced on the platform are highly accessible to anyone with a device. Furthermore, it is an easy platform to quickly prototype and develop 3D environments with player models, making it easy to attach our agents to an NPC. The agents print their conversations to the Roblox game NPC chat. For our actual agents, we are using OpenRouter to allow us to easily pick different LLMs to experiment with and potentially build agents with different personalities. To develop our prompts, our application keeps track of historical negociations that the buyer has conducted and feed them back to the language model, which means the buyer can remember deals it has made and make decisions based on past experience to conduct future dea\n\n**Challenges**\nOur challenges actually mainly lies with building the simulation environment itself\nGetting automated Roblox characters to communicate with each other\nGetting elements of the game to actually interact with each other in the physical space in a way that makes sense\nDealing with race conditions between element rendering and agent / other API calls within the game engine\nVersion control\n\n**Accomplishments**\nFirst and foremost, we are really proud of our concept and design, which also scales to various other AI Agent simulations and tasks. We are also proud to have built a full-stack application within the time constraint and within such a unique environment.\n\n**What we learned**\nA key learning includes how to simulate negotiation. We often intuitively know how to conduct negotiations ourselves at least at a basic level, but trying to instruct the AI agents to conduct sensible negotiations requires a deeper understanding of the mechanics of sale negotiation. Furthermore, we also learned how to build games and work with 3D spaces effectively. For the most part, this our first part ever building in 3D environments, so from collision to movement, we now have a better understanding of 3D game engines.\n\n**What's next**\nWe want to expand BOBUX BARGAINS to have more buyer and seller agents, while also allowing players to negotiate with other NPCs and players. With 10 BILLION agents, we can spend $450 000 PER SECOND in OpenRouter credits and simulate the BEST and MOST ACCURATE economic model of the real-world marketplace there is.\n\n",
    "prize": "Winner Best Funny Haha Hack; Winner Aristotle Challenge: Most Cutting-Edge AI Agent",
    "techStack": "luau, openrouter, roblox, robloxstudio",
    "github": "https://github.com/jennnniferkuang/Bobux-Bargains",
    "youtube": "https://www.youtube.com/watch?v=xkyIFGGAyg0",
    "demo": "https://www.roblox.com/games/140073423730757/Boxub-Bargains",
    "team": "Seller registry API, Jennifer Kuang, Amelia Song, David Hang",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/bobux-bargains"
  },
  {
    "title": "Zenith",
    "summary": "TECHNICAL HIGHLIGHTS: The project was built using a combination of modern web technologies, including CSS for styling, Firebase for backend services, and JavaScript for interactivity, showcasing a solid understanding of full-stack development. The use of Git and GitHub for version control indicates a collaborative approach to coding, enhancing team productivity and project management.",
    "description": "**What it does**\nZenith is mainly a task tracker/to-do list. When it is your first time on the site it will auto-populate with a few wellness focused tasks but you can easily delete those and add your own and assign them points values. You gain points as you complete tasks as you work to build a rocket ship. Once you have enough points to launch you will gain another module in your space station \"Zenith\" that you are able to see by clicking on the space-station button on the navbar. Shooting across the sky in this view are self care comets, if you click on it, a pop-up with a wellness tip will appear. Once you launch, your selected tasks are deselected, you can either delete them if it was a one time task or keep it if it is a reoccurring task (good for weekly assignments or daily wellness goals). Don't wo\n\n**Inspiration**\nBoth of us are on the the satellite development program at UMD (project THEIA) and are very passionate about space. We both also have a heavy work load and do not prioritize our physical and mental wellness as well as we should. We wanted to make a website that would allow us to keep track of all of our tasks while still giving us wellness reminders (for when we inevitably do not add them to our task list) and also incorporate our love for space. Both of us joined this hackathon to find a community of women with similar interests to us, when you find the right community of people it makes it easier to navigate spaces where you are the outlier. Throughout our journeys in coding and with space development, both of us know what it is like to be the only or one of only a few women in a room. W\n\n**How we built it**\nWe are using Firebase for hosting our site since one of our members has used it on a previous project and is familiar with the application. For front-end development, we are using HTML and CSS and JavaScript to add our functionality and interactive elements on the back-end. Since there were two of us working together, we used Git to maintain version control and to allow for simultaneous coding on different branches without merge conflicts.\n\n**Challenges**\nIn the beginning we ran into a lot of merge conflicts because both of us had limited experience working with Git merges before. After some troubleshooting, we were able to get a better idea of how to fix them and generally avoided merge conflicts for the rest of the hackathon. Also, between the two of us one of us does not have a lot of JavaScript experience and the other does not have a lot of experience with any of the coding languages that we used for this project, so there was a learning curve when figuring out how everything needs to be structured in the code.\n\n**Accomplishments**\nFor one member this was their first hack and for the second, it was her second time doing a project but her first time being a main coder for the project. When we first started, we were unsure of how much functionality we would be able to implement. We ended up placing a lot of features into the \"If we have time at the end\" category. We were able to knock out basically that entire category and more. We would keep finding new and better features to add to enhance the project. Since coming here, we were unsure if we would even have something to present. We are very proud to that we finished and are demoing our project.\n\n**What we learned**\nBoth of us built upon our current knowledge of Git. We now are better at debugging merge conflicts and how to avoid them. We also learned how different branches work and how to merge those branches to the main (and double checking to make sure you pull first). We also learned a lot more about web development especially when it comes to dynamic and interactive web applications. Additionally, we learned to incorporate moving elements, including layering a moving interactive background that covers the entire page over other interactive elements on the page.\n\n**What's next**\nFor Zenith we would like to add more variety and customization in the modules that are being added to your space station. It will make it more exciting to see your Space Station grow if there are different ways that it can be built. Additionally, instead of just seeing the outside, being able to actually see the different interiors of the Space Station modules is a future feature we would like to implement. The interior design could also be your choice based on what the module is, to add more customization for the user. We would also like to make it where there is a second section on the home page, specifically for wellness where a user can input as many goals as they want and everyday the list will randomly populate with a few of them. This will make it where it is more exciting to keep u\n\n",
    "prize": "Winner [Technica] Freshly Unearthed: Best Beginner Hack (College)",
    "techStack": "css, firebase, git, github, html, javascript",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=r_CqMXF6ia8",
    "demo": "https://zenith-sky.work/",
    "team": "Madison Lawson, Bri Rosado",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/zenith-kxw084"
  },
  {
    "title": "DocSimplified",
    "summary": "DocSimplified is a web application designed to simplify document management and processing using advanced generative AI and machine learning technologies. It leverages these technologies to enhance user interaction with documents, likely streamlining tasks such as editing, summarization, and organization for improved efficiency.",
    "description": "**What it does**\nDocSimplified is a multilingual AI-powered web application that simplifies government documents. Users upload any fillable PDF, and the app automatically detects form fields, summarizes each section in clear steps, translates the content into their preferred language, and guides them through filling it out. It even includes text-to-speech for users with low vision or reading difficulties. The goal: make complex forms feel intuitive and approachable.\n\n**Inspiration**\nGovernment forms are notoriously confusing ‚Äî even for families who have lived in the U.S. for decades. Three out of four of our team members come from immigrant households, and we‚Äôve all watched our parents struggle with taxes, visa renewals, and federal documentation because of language barriers, dense terminology, and inaccessible formatting. We wanted to build a tool that makes essential government services understandable for everyone, regardless of literacy level or language proficiency.\n\n**How we built it**\nFrontend: React and Tailwind for a clean, accessible UI; pdf-lib for rendering and editing fillable PDFs.\nBackend: Supabase for authentication, user profiles, and document storage.\nAI Processing: OpenAI 4o API for summarization, language translation, and adaptive explanations tuned to the user‚Äôs education level.\nAccessibility: ElevenLabs for high-quality text-to-speech output in multiple languages.\nWe tied all of these components together to create a smooth workflow from upload ‚Üí analysis ‚Üí guided form-filling ‚Üí saving progress.\n\n**Challenges**\nOur initial PDF viewer couldn‚Äôt read embedded form fields, forcing us to switch to a more reliable parsing method.\nOCR-based image uploads were too time-intensive to implement during the hackathon.\nEarly OpenCV bounding-box detection only recognized perfect rectangles, while many real forms used underlines or irregular shapes.\nIntegrating multiple third-party APIs while keeping the interface fast and accessible required lots of debugging and design iteration.\n\n**Accomplishments**\nDelivering a fully functional, end-to-end document simplification and form-filling experience.\nBuilding a genuinely accessible tool with translation, easy summaries, and text-to-speech baked in.\nCreating a polished and intuitive UI despite the technical complexity underneath.\nStaying true to our mission of helping immigrant and low-literacy communities ‚Äî the people who inspired this project in the first place.\n\n",
    "prize": "Winner [Intuit] Most Transformative Hack Leveraging Generative AI/ML",
    "techStack": "dropzone.js, elevenlabs, node.js, openai, pdf-lib, react, supabase, tailwind, vercel, vite",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=UwHuJXa4LVw",
    "demo": "https://www.docsimplified.us/",
    "team": "Bassil Shalaby, Dvij Raicha, William Conner, Shanmukha Pothukuchi",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/docsimplified"
  },
  {
    "title": "StudyBuddy Pro",
    "summary": "TECHNICAL HIGHLIGHTS: The project employs a robust tech stack, including React for dynamic user interfaces and Tailwind for responsive design. Notably, it incorporates libraries such as Recharts for data visualization and html2canvas/jspdf for exporting study materials, showcasing a blend of performance and usability.",
    "description": "**What it does**\nStudyBuddy Pro is a comprehensive study management platform that combines: Assignment Management: Track all assignments with subjects, due dates, estimated hours, and difficulty levels\nAI-Powered Scheduling: (Planned feature) Generate intelligent study schedules that break down large tasks into manageable daily sessions\nPomodoro Timer: Built-in productivity timer with automatic progress tracking linked to specific assignments\nVisual Analytics: Interactive charts showing weekly study hours, subject distribution, and assignment progress\nSmart Notifications: Deadline alerts (3 days, 1 day, same day), daily study reminders, and inactivity nudges\nGamification System: 8 achievement badges to encourage consistent study habits (Early Bird, Night Owl, Perfect Week, etc.)\nExport Functionality: Gener\n\n**Inspiration**\n87% of students procrastinate on assignments, and 90% struggle with time management. As a student myself, I've experienced the overwhelming feeling of juggling multiple deadlines without clear visibility on progress. I wanted to build a solution that not only tracks assignments but actively helps students develop better study habits through intelligent scheduling, real-time feedback, and motivation.\n\n**How we built it**\nTech Stack: Frontend: React 18 + TypeScript for type-safe, component-based UI\nBuild Tool: Vite for lightning-fast development and hot module replacement\nStyling: Tailwind CSS + Radix UI (shadcn/ui) for beautiful, accessible components\nData Visualization: Recharts for interactive charts (weekly hours, subject distribution, progress tracking)\nAnimations: Framer Motion for smooth transitions and engaging interactions\nData Persistence: Browser localStorage for client-side data storage\nNotifications: Browser Notification API for deadline and study reminders\nExport: jsPDF and html2canvas for generating downloadable reports\nDevelopment Platform: Lovable.dev for rapid prototyping and deployment\nHosting: Lovable for fast, reliable deployment Development Process: Research Phase: Analyzed student pai\n\n**Challenges**\n1. Timer-Assignment Integration\nThe biggest challenge was linking Pomodoro timer sessions to specific assignments and automatically updating progress. I had to design a robust data structure that tracked: Session timestamps\nDuration (work vs break sessions)\nAssignment IDs\nCompletion status Solution: Created a session logging system in localStorage with real-time progress calculation based on completed vs estimated hours. 2. Real Study Streak Calculation\nCalculating an accurate \"study streak\" required handling edge cases: What counts as a study day? (At least one completed work session)\nHow to handle missed days gracefully\nTimezone considerations for accurate date comparisons Solution: Implemented a grace period system and date normalization to accurately track consecutive study days. 3. Ch\n\n**Accomplishments**\n‚úÖ Built a fully functional app in 48 hours with 6+ major features\n‚úÖ Real-time progress tracking that eliminates manual data entry\n‚úÖ Beautiful, responsive design that works seamlessly on mobile and desktop\n‚úÖ Data-driven insights with 3 different chart types for comprehensive analytics\n‚úÖ Professional export functionality for sharing progress with parents/teachers\n‚úÖ Gamification system with 8 achievement badges to keep students engaged\n‚úÖ Accessibility-first design with keyboard navigation, ARIA labels, and dark mode\n‚úÖ Zero-backend MVP - entire app runs client-side with localStorage\n‚úÖ Interactive presentation website built with the same tech stack\n‚úÖ Real student validation - tested with peers who confirmed it solves real problems\n\n**What we learned**\nTechnical Skills: Advanced React patterns (custom hooks, context API, prop drilling avoidance)\nTypeScript best practices for type-safe development\nData visualization with Recharts (responsive charts, tooltips, legends)\nBrowser APIs (Notifications, localStorage, date-fns for date manipulation)\nPDF/CSV export implementation from scratch\nFramer Motion for production-quality animations\nTailwind CSS utility-first styling philosophy\nAccessibility best practices (ARIA, keyboard navigation, color contrast) Product Design: The importance of user research (talking to students revealed pain points I hadn't considered)\nSimplicity beats feature bloat (focused on core features that matter most)\nVisual feedback is critical (progress bars, charts, and badges keep users engaged)\nGamification works (achieve\n\n",
    "prize": "Winner People‚Äôs Choice",
    "techStack": "css, date-fns, framer, html2canvas, jspdf, lovable, lucide, motion, radix, react, recharts, router, sonner, tailwind, typescript, ui, vite",
    "github": "https://github.com/linfordlee14/studybuddypro",
    "youtube": "https://www.youtube.com/watch?v=48DeaEq4Ltw",
    "demo": "https://studybuddyprov-presentations.vercel.app/",
    "team": null,
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/studybuddypro"
  },
  {
    "title": "BuckBounty",
    "summary": "BuckBounty is a fintech project designed to leverage advanced AI technologies to enhance financial insights and decision-making for users. By integrating multiple APIs and leveraging machine learning, it aims to provide real-time data analysis, personalized recommendations, and streamlined financial transactions.",
    "description": "**What it does**\nBuckBounty is an AI-powered personal finance assistant that: Maximizes savings through credit card reward optimization (analyzes your spending patterns and recommends the best cards for each category)\nFinds hidden deals via automated coupon scraping from Gmail, Honey, and Rakuten\nBuilds wealth by converting your savings into personalized investment portfolios with 10-year projections\nProvides intelligent insights using a multi-agent AI system (MARK orchestrates specialized agents for coupons, financial news, and market analysis)\nOffers voice-enabled chat with ElevenLabs text-to-speech for hands-free financial management\nIntegrates with Plaid for automatic transaction syncing and real-time spending analysis\nAnalyzes prediction markets through PolyMarket integration with risk assessment and\n\n**Inspiration**\nWe noticed a common problem: people leave thousands of dollars on the table every year through suboptimal credit card usage, missed deals, and lack of financial awareness. Traditional budgeting apps just show you numbers, but they don't actively help you make money. We wanted to build an AI-powered financial copilot that doesn't just track spending‚Äîit turns every transaction into an opportunity to save, optimize, and build wealth.\n\n**How we built it**\nFrontend: Next.js 14 with TypeScript for a modern, responsive interface\nShadCN UI components with custom glass morphism effects\nRecharts for interactive spending visualizations (radar charts, pie charts)\nVanta.js for immersive 3D animated backgrounds\nWeb Speech API + ElevenLabs for voice interactions Backend: FastAPI server with multiple specialized AI agents using MCP (Multi-agent Coordination Protocol)\nGoogle Gemini for natural language processing and embeddings\nRAG (Retrieval-Augmented Generation) with dual vector indexing:\n\n\nFAISS FLAT index for current month (fast exact search)\nHNSW index for historical data (efficient approximate search)\n\nRedis caching layer for sub-50ms response times on repeated queries\nWeb scraping agents (BeautifulSoup + Selenium) for automated coupon aggregation\n\n**Challenges**\nVector search scalability: Initially, semantic search on 10,000+ transactions was slow. We solved this by implementing a dual-index system‚ÄîFLAT for recent data, HNSW for historical‚Äîwith automatic migration.\nLLM response caching: Gemini calls were expensive and slow (~2.5s per request). We built a Redis-based caching layer that reduced identical queries to <50ms, a 50x improvement.\nMerchant name matching: User transactions have messy merchant names (e.g., \"STARBUCKS #12345 SEATTLE WA\"). We implemented fuzzy matching with embeddings to accurately match @Starbucks mentions to actual transaction data.\nReal-time agent coordination: Coordinating multiple AI agents (MARK, BountyHunter1, BountyHunter2) without conflicts required building a custom MCP server with status tracking and task queuing.\nC\n\n**Accomplishments**\n50x performance improvement through intelligent Redis caching\nMulti-agent AI orchestration that actually works in production\nDual-index RAG system handling 10,000+ transactions with sub-second search\nAutomated wealth building that converts credit card savings into actionable investment portfolios with real fund recommendations\nBeautiful UI/UX with glass morphism, 3D backgrounds, and smooth animations\nVoice-enabled interface that makes finance management accessible hands-free\n@ mention feature that provides instant merchant-specific insights\nProduction-ready security with Plaid OAuth and proper API key management\n\n**What we learned**\nRAG architecture: Implementing FAISS with dual indexing taught us the tradeoffs between exact and approximate search. FLAT is perfect for small, current datasets while HNSW scales to millions of vectors.\nLLM optimization: Caching isn't just about speed‚Äîit saves real money. Our Redis layer reduced API costs by 70%.\nMulti-agent systems: Coordination is hard. We learned to design agents with clear responsibilities and avoid circular dependencies.\nFinancial domain knowledge: Credit card reward structures, investment portfolio theory, and prediction markets‚Äîbuilding BuckBounty required deep research into personal finance.\nUX matters: The most powerful AI is useless if the interface is confusing. We iterated heavily on the dashboard and chat experience.\n\n**What's next**\nMachine learning for budget predictions: Train models on transaction history to predict future spending and warn users before they overspend.\nTax optimization agent: Automatically identify tax deductions from transactions (charitable donations, business expenses, medical costs).\nAutomated bill negotiation: Use AI to call service providers (phone, internet, insurance) and negotiate lower rates.\nSocial features: Allow users to compare anonymized spending patterns with peers and compete on savings leaderboards.\nMobile app: React Native app with push notifications for real-time deal alerts and bill reminders.\nOpen banking expansion: Support for international banks beyond Plaid's coverage using Teller, Finicity, and MX.\nCryptocurrency integration: Track crypto portfolios alongside traditional a\n\n",
    "prize": "Winner Fintech; Winner Aristotle Challenge: Most Cutting-Edge AI Agent",
    "techStack": "beautiful-soup, elevenlabs-api, faiss-vector-database, fastapi, google-gemini-api, next.js-14, plaid-api, polymarket-api, python, react-18, recharts, redis, selenium, sentence-transformers, shadcn-ui, stripe, tailwind-css, typescript, vanta.js, web-speech-api, yahoo-finance-api, zustand",
    "github": "https://github.com/NandanHemanth/BuckBounty",
    "youtube": "https://youtu.be/rCsw2WkivC0",
    "demo": "https://youtu.be/rCsw2WkivC0",
    "team": "Nandan Hemanth",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/buckbounty"
  },
  {
    "title": "Saathi",
    "summary": "Saathi is a web application designed to streamline document management and collaboration, leveraging interactive data visualizations and efficient file processing. It integrates various tools to enable users to create, edit, and manage documents seamlessly while providing analytical insights through visual charts.",
    "description": "**What it does**\nAt first glance, Saathi may seem just another chatbot that can generate quizzes. But what if it actually reduces cognitive dependency on AI and can help humans grow? From calculus to code, Saathi is built to guide you, nudge you, and explain to you the fundamental concepts, not solve the problem for you and hamper your learning. It pushes you to apply STEM concepts through quizzes that test active recall, not passive reading. And when you see your progress visualized over time, the momentum builds, making you actually want to keep going.\n\n**Inspiration**\nWith the rise of vibe coding, almost everyone‚Äîincluding me, at some point‚Äîare or have been stuck in this AI hell where the lines of knowing something and actually being able to do it blur. It's like you know the content, the code, the logic, but actually writing it...just feels impossible. Saathi was built not to show you the destination or to drag you to the finish line, but to keep you walking. Something like this could save me‚Äîand probably many others‚Äîa lot of the hours wasted begging LLMs to teach instead of handing over full answers. Automating learning isn't about taking away the mental strain, because that is how you learn.\n\n**How we built it**\nFrontend: Built with Tailwind CSS. The static HTML/CSS layouts were generated using Stitch to speed up UI development, allowing me to focus on the logic. Chart.js was used to make the beautiful and interactive charts. Backend: I chose Django as the Python framework for this project as I am the most comfortable with it. Along with that, libraries like Markdown It, PyMuPdf,  Python-docx, and Functools have been utilized for displaying the LLM's response in a structured manner, reading and extracting text from pdf and .docx files, and making wrapper functions for views where login is required respectively. The authentication is handled using Supabase whereas the user's profile and quizzes are stored in a local Postgres DB. AI: Saathi itself is powered by a Gemini 2.5 flash model with a system\n\n**Challenges**\n*Supabase's RLS policies: *\nIt  prevent unauthenticated users from inserting data into tables like profiles. But new users need a profile row created before they can authenticate. This meant new users couldn't create profiles because they weren't authenticated, but couldn't authenticate without a profile existing.\n-> Instead of using Supabase for cloud storage, I connected Postgres SQL to Django, and made a local database with the tables \"Profiles\" and \"Quizzes\". This allowed for hassle-free row inserts and smooth authorization *Quiz Parsing: *\nNo matter what prompt was used, Gemini did not always follow it and occasionally returned textual response which broke the quiz generation since using json.loads() failed instantly. Manually parsing and structuring into dictionary containing lists c\n\n**Accomplishments**\nBuilding all of this solo: Of course the project making could be way smoother if I had a teammate, but looking back and realizing I did all of this (code, documentation, demo, filling out the Devpost project details, etc.) all by myself actually makes me proud.\n*Participating: * Yes, even participating was a really huge challenge for me as I was so nervous about this hackathon, people seemed way more experienced and skilled, a lot of people had a clear idea and strong team and initially it was all just so intimidating, especially participating solo, but I am glad that I did it:)\n** Deployement: ** It was my first time deploying a web app and I had no idea what I was doing, but with a little bit of Youtube tutorials and asking Claude to explain me the bits and pieces I didn't get, I DID IT.\n\n**What we learned**\n*Find a team, with the same timezone: * Time zone, no matter what you do, always is a bit of a barrier sometimes when it comes to meetings and ideation, regardless a team helps a lot. Not only with coding but also with ideating and the emotional support that comes along with one.\n*Use Flask: * As much as I love Django and am really comfortable with it, it is such a pain to set it up sometimes, of course the database management and authentication gets really easy, but in a hackathon where you have to build and present in 48 hours, Django might not always be the best choice. \n*Write a script for the demo: * I thought I could just record the demo video without any script or prep and say what comes to mind, then I ended up re recording like 15 times and realized why I need one. \n*How to deploy\n\n**What's next**\nSpaced repetition, flashcard generation, automatic resource finder, and integration with FocusAI, another EdTech project I have that can go well with Saathi. For more information, make sure to read the documentation\n\n",
    "prize": "Winner Best Documentation (Sponsored by Gitbook)",
    "techStack": "chart.js, django, gemini-api, javascript, postgresql, pymupdf, python, python-docx, supabase, tailwind",
    "github": "https://github.com/Aruniaaa/CSG-Hackathon",
    "youtube": "https://www.youtube.com/watch?v=D3-EYI0bZ5o",
    "demo": "https://saathi-production-1513.up.railway.app/",
    "team": "Charu Singhania",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/saathi-f291s5"
  },
  {
    "title": "InvestyBesty",
    "summary": "InvestyBesty is a financial technology application designed to empower users with investment knowledge and tools. By integrating various APIs, it provides real-time financial data, analytics, and educational resources, aiming to enhance users' investment decision-making skills.",
    "description": "**What it does**\nInvestIQ is a full-stack, AI-powered platform delivering validated, commercially-ready financial mastery and compliance.\nValidated ML & i18n: Through a multi-language Beta Pilot Program (powered by active i18n), we validated the predictive accuracy of our Advanced ML model for personalized lesson sequencing using real user data.\nRegulatory Compliance & Audit: The platform includes a working Compliance Layer and has already completed Preliminary External Auditing in preparation for seed funding and future regulatory filing.\nData Simplification: Advanced Visualization tools translate complex sentiment-analysis and predictive-model outputs into clear insights.\nHolistic Application: Includes Paper Trading, Behavioral Finance Coaching, and debt-management guidance‚Äîsupporting the 47% of American\n\n**Inspiration**\nGeneric financial education fails learners, costing the average American an estimated $1,015 each year in poor decisions. The result is an equity crisis‚ÄîGen Z scores only 38% on financial-literacy assessments. We built InvestIQ to fix this using Generative AI: a dynamic, personalized mentor that turns low confidence into real financial capability.\n\n**How we built it**\nWe designed an enterprise-grade system for security, validation, and global scalability.\nValidation: Ran a full Pilot Program using the active i18n framework to test platform stability and predictive-model accuracy against live interactions.\nCompliance & Security: Built the Regulatory Compliance Layer and engaged in External Auditing to secure the codebase and prepare for market entry.\nAI/ML Core: Integrated Generative AI for content and Advanced ML for predictive sequencing, supported by visualization tools that render complex outputs simply.\nAPI & Integration: Integrated with brokerage APIs and developed an NLP sentiment-analysis model.\n\n**Challenges**\nAudit & Pilot Data: Preparing for External Auditing and securely managing multi-language pilot data for ML validation.\nPredictive Accuracy: Achieving high predictive accuracy required extensive real-world data and multiple retraining cycles.\nThe Jargon Trap: Simplifying technical financial concepts while maintaining accuracy proved consistently challenging.\n\n**Accomplishments**\nMarket Validation: Successfully launched a multi-language Beta Pilot Program, proving viability, stability, and global scalability.\nInvestment Readiness: Achieved External Audit readiness and built Advanced Visualization to turn complex ML outputs into actionable insights.\n\n**What we learned**\nFinTech innovation must be validated and secure before launch. Through the Pilot Program and External Auditing, we proved that our GenAI-driven personalization is not only effective but stable and ready for regulatory scrutiny‚Äîdemonstrating true market readiness.\n\n**What's next**\nSeed Funding & Full Regulatory Filing: Leverage audit results and pilot data to raise seed capital and pursue formal regulatory approval for real-money transactions.\nStrategic Partnerships: Target global banks and financial institutions to deploy our compliance-ready, multi-language platform.\nAdvanced Psychometrics: Expand behavioral-finance coaching with deeper psychometric testing to further personalize user pathways.\n\n",
    "prize": "Winner [T. Rowe Price] Investor Education Challenge",
    "techStack": "alpha-vantage-api, big-data, cloudflare, css4, financial-modeling-prep-api, gemini, i18, llms, nextjs, pnpm, python, react19, reacthook, recharts, sec-edgar-api, tailwind, typescript, vultr",
    "github": "https://github.com/amansahu205/Technica-2025",
    "youtube": "https://www.youtube.com/watch?v=lZ9JGw4fMuU",
    "demo": "https://75ae0832.technica-2025.pages.dev/login/",
    "team": "Aman Sahu, Dhanush P Sukruth, Supriya Kadam, Nikhil Mulgir",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/ohmyfunds-xyz"
  },
  {
    "title": "It's Getting Fishy...",
    "summary": "\"It's Getting Fishy...\" is an innovative project that likely combines music and interactive gameplay, utilizing Ableton for audio production, Lua for scripting, and Playdate as the gaming platform. The project aims to create a unique experience that engages users through audio-visual interactions, possibly involving rhythm or sound-based challenges.",
    "description": "**Inspiration**\nWe wanted to take advantadge of the unique crank mechanism in the Playdate. While thinking of different ideas, we decided to go with a fishing game, as we thought it would be fun with the crank and very intuitive.\n\n**Challenges**\nNone of us had touched Lua before, so it was quite the learning curve!\nAt first, we couldn't load the game into the console! We realized the console was running an older software version, so we had to find a way to update the software.\nOur game was having very poor performance when running on actual hardware, we had to go back and refactor some of the code to make it more smooth and performant\n\n**Accomplishments**\nWe made everything ourselves in a very short amount of time. The code, the music, the art, the sound effects. All from us.\n\n**What we learned**\nWe learned how to make games, a very different approach to development as what we are all accustomed to. \nWe learned to code with specific hardware in mind, being aware of its limitations, its constraints, etc\nWe used our foundations of programming to quickly learn a new language (Lua) and apply it to a challenge.\n\n**What's next**\nWe want to see how we can release the game to the public! We will look into releasing the game on itch.io and seeing if other Playdate owners can play it on their consoles!\n\n",
    "prize": "Winner 1st Place",
    "techStack": "ableton, lua, playdate",
    "github": "https://github.com/santipadron/codejam15",
    "youtube": "https://www.youtube.com/watch?v=0IwvcU246H8",
    "demo": null,
    "team": null,
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/it-s-getting-fishy"
  },
  {
    "title": "Arbitrage",
    "summary": "Arbitrage is a project designed to facilitate real-time market transactions by leveraging arbitrage opportunities across different platforms. It utilizes advanced technologies to simulate an agent marketplace, optimizing trade decisions and maximizing profit margins through efficient data analysis and execution.",
    "description": "**Inspiration**\nThe project began with a simple question: what happens when negotiation is treated as an conversational process rather than a numerical optimization problem?\nMarket simulations typically rely on scoring functions, game-theoretic formulas, or engineered heuristics. Real human bargaining rarely works that way. Sellers hide motives, buyers infer intentions, and decisions emerge from dialogue, not equations. This gap‚Äîbetween real negotiation and algorithmic negotiation‚Äîinspired the creation of Arbitrage, a system where autonomous agents engage in realistic, chat-based bargaining driven entirely by LLM reasoning.\n\n",
    "prize": "Winner Visa Challenge - Agent Marketplace Simulation; Winner Best Use of Qualcomm EdgeAI",
    "techStack": "docker, fastapi, git, lm-studio, openrouter, postgresql, python, react, sqlalchemy, typescript, websockets",
    "github": "https://github.com/rohan-g0re/Hack_NYU/tree/main",
    "youtube": "https://www.youtube.com/watch?v=z2_8AWR_qU8",
    "demo": null,
    "team": "Pradeep Kulkarni, Sahil Sarnaik, Rohan Gore",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/multi-agent-marketplace-simulator"
  },
  {
    "title": "RotLocker",
    "summary": "TECHNICAL HIGHLIGHTS: Built using Android Studio and Kotlin, RotLocker leverages modern mobile development techniques for an intuitive user interface and experience. The integration of Claude CLI enhances its functionality, potentially allowing for seamless command-line interactions and automated processes, which could improve user efficiency.",
    "description": "**Inspiration**\nDevice addiction has quietly but steadily crept into our daily lives‚Äîand it‚Äôs only getting worse. As of 2025, the average American spends over 5 hours and 16 minutes on their phone each day. This growing dependence is closely linked to rising rates of depression, anxiety, and declining physical and emotional well-being. Looking to proven strategies for overcoming addiction, we turned to one of the most effective tools: accountability. When people support each other, meaningful change becomes possible. RotLocker is built on that principle.\n\n**What's next**\nRight now, RotLocker is functional but early in its development. The UI is still being refined, and the app currently works only on Android. In the coming months, we plan to polish the design and bring RotLocker to iOS as well.\n\n",
    "prize": "Winner [MLH] Best Domain Name from GoDaddy Registry",
    "techStack": "android-studio, claude-cli, github, kotlin",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=7gBWeZ2DXOc",
    "demo": "https://block-the-rot-for.us/",
    "team": "Ilya Shchelokov, qwack",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/rotlocker"
  },
  {
    "title": "./space_re:ACT",
    "summary": "The project ./space_re:ACT is an immersive technology experience that leverages interactive storytelling within a 3D environment. It likely combines elements of virtual reality or augmented reality to engage users, allowing them to explore and interact with a digital universe in innovative ways.",
    "description": "**What it does**\n./space_re:ACT is an immersive, interactive, and intuitive game that tests a player's ability to react to verbal and visual instructions in a high-pressured environment. Their task to push buttons and pull levers  to successfully reach the destination of the Space Mission. Tasks become increasingly difficult, with more colors and directions to track and memorize. Failure to accurately maintain the ship exposes the player to dangerous debris, such as meteorites, that collides with the ship and ultimately ends the game.\n\n**Inspiration**\nAs avid VR/AR game players, athletes, and CS students, we saw VR has the potential to not only entertain, but the train. While current VR applications exist to help develop and maintain coordination and quick reactions, most on the market serve little purpose other than sight-seeing. Those that serve as clinical tools lack engagement, and frame recovery as a chore, not self-improvement.\n\n**How we built it**\nIn the front-end, we built most of the models and assets using Blender. We personally model everything from scratch. For the game engine, we are using Unity with the XR toolkit. While importing all the models and assets into Unity, we use C# to implement the back end, creating physics, buttons, and levers. As a four-person team, we are using GitHub Desktop to collaborate and achieve vision control.\n\n**Accomplishments**\nWe actually finished the project in 42 hours! We implement everything from scratch, and as we mentioned before, we encountered many challenges. However, we created a cool VR game in just 42 hours.\n\n**What we learned**\nProblems Solving Skills\nCollaboration with a new team\nUnity\nC#\nModeling in Blender\n## What's next for ./space_re:ACT\nModes/difficulty levels to help players GROW\ndifferent modes/versions for different targets - one more on memory, one more on speed\nmore missions to increase immersive engagement and curiosity\nsome tasks outside of the Rocketship\nmore interactable objects in the Rocketship\n\n",
    "prize": "Winner Creative Immersive Technology 1st Place Prize; Winner Best Business Potential - S",
    "techStack": "blender, c#, elevenlabs, github, unity",
    "github": "https://github.com/rayyyrayy/DandyHacks-SpaceSimulator",
    "youtube": "https://www.youtube.com/watch?v=izFmv4YLVLk",
    "demo": null,
    "team": "Earvin Chen, Daniel Lin, Rain Xia",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/roket"
  },
  {
    "title": "US Law Reference",
    "summary": "US Law Reference is a mobile application designed to provide users with easy access to legal information and resources related to U.S. laws. By utilizing a user-friendly interface, the app aims to empower individuals to better understand their legal rights and navigate the complexities of the legal system.",
    "description": "**What it does**\nUS Law Reference provides an offline, searchable library of the U.S. Code, the Constitution, and DMV practice tests. Users can browse titles, read full legal sections, study amendments, take quizzes, and track their progress‚Äîall in one app.\n\n**Inspiration**\nStudents often struggle to access U.S. legal information in a simple, organized way. Existing apps are cluttered, online-only, or incomplete. I wanted a fast, offline tool that makes learning U.S. laws and DMV rules easy for everyone.\n\n**How we built it**\nI built the app using Flutter and Dart, with a local SQLite database for storing law sections, constitutional text, quiz history, and DMV questions. The UI uses Material Design 3 and follows a clean MVVM structure. All legal content is embedded for offline access.\n\n**Challenges**\nExtracting and formatting real U.S. legal text into a structured database\nManaging large datasets efficiently on-device\nFixing quiz randomization bugs and answer duplication\nDesigning a clean UI that still handles long legal sections\nEnsuring good performance without requiring internet access\n\n**Accomplishments**\nFully offline access to 38 U.S. Code titles and the entire Constitution\nA clean, intuitive UI that makes dense legal text easier to read\nA complete DMV practice test system with progress tracking\nSmooth search functionality across all legal sections\nSuccessfully publishing a working APK usable on real Android devices\n\n**What we learned**\nHow to structure and optimize large SQLite datasets in Flutter\nHow to manage state and navigation in a growing mobile app\nHow to extract, clean, and format public legal data\nThe importance of UI/UX when displaying long-form text\nHow to design quizzes, timers, and statistics tracking systems\n\n**What's next**\nExpanding to all 54 U.S. Code titles\nAdding case law summaries and examples for easier learning\nAdding bookmarks, favorites, and note-taking\nImproving search with filters and keyword highlighting\nBringing the app to iOS\nPublishing to Google Play for easy installation\n\n",
    "prize": "Winner Participation Prize; Winner Top 24-85",
    "techStack": "dart, flutter, sqlite",
    "github": "https://github.com/AndrewTr0612/USLawReferenceAndroidApp",
    "youtube": "https://www.youtube.com/watch?v=eml56tZ-PgA",
    "demo": null,
    "team": null,
    "date": "2025-11-17",
    "projectUrl": "https://devpost.com/software/us-law-reference"
  },
  {
    "title": "Pomodoro Park Casino - Productivity Paradise",
    "summary": "TECHNICAL HIGHLIGHTS: Built using React and TypeScript with Vite for fast development and performance, the project likely showcases responsive design, real-time task tracking, and interactive gaming elements. The use of TypeScript would enhance code quality and maintainability, while Vite's rapid build times would contribute to a smooth development experience.",
    "description": "**What it does**\nPomodoro Park is a gamified study timer that blends the productivity of the Pomodoro Technique with the fun of a casino. Here's how it works: Set Your Session: The user inputs their desired study time (e.g., 2 hours), a study block duration (e.g., 25 minutes), and a base break time (e.g., 5 minutes).\nStudy Mode: The app runs the study timer. The user focuses, knowing a unique break is coming.\nBreak Time!: When the study timer hits zero, the break timer begins, and the user is presented with the \"Pomodoro Park\" casino.\nPlace Your Bets: The user can choose to play one of three casino games: Blackjack, Roulette, or a Slot Machine.\nWin or Lose: The outcome of the game directly impacts their break! A win might add time to the break, while a loss will shorten the break. (Or, they can just take t\n\n**Inspiration**\nThe Pomodoro Technique is a time management method that uses a timer to break down work into focused intervals, traditionally 25 minutes in length, separated by short breaks. While it is a proven productivity powerhouse, but we've always felt the break component was boring. The standard 5 minute break leads endless doomscrolling on your phone too often. So, we asked ourselves: how could we make the break feel like a real reward? We wanted to build a system where you not only earned your break, but also holds you accountable. When your break timer ends, so does your ability to play at the Pomodoro Park casino.  The idea of a high-energy, fun \"casino\" where you could \"gamble\" for more break time lead to the creation of \"Pomodoro Park.\"\n\n**How we built it**\nThis project is built with: Vite\nTypeScript\nReact\nshadcn-ui\nTailwind CSS\n\n**Challenges**\nOur biggest challenge was scope and time management. We were passionate about the idea and initially wanted to add even more games and features. We had to focus and cut back to what was achievable.\n\n**Accomplishments**\nWe're also proud of the UI polish. We created an app that is clean, intuitive, and, most importantly, fun to use. The overall look of the application feels smooth and rewarding.\n\n**What's next**\nMore Games: We'd love to add more quick, casino games, to provide users with maximum enjoyment of the casino.\nUser Accounts & Stats: We plan to add a proper backend to allow users to create accounts, track their study hours, and see stats on their game winnings.\nCustomization: More themes! Imagine a customizable casino where you can choose the background, colours, etc., to your liking.\n\n",
    "prize": "Winner Finalist",
    "techStack": "react, typescript, vite",
    "github": "https://github.com/jamesjhjung/study-break-casino.git",
    "youtube": "https://www.youtube.com/watch?v=KX9xhb2YUuE",
    "demo": "https://pomodoro-casino.lovable.app/",
    "team": "James Jung, Lester Cheng, Jacky Zhong, Denis Chen",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/productivity-paradise"
  },
  {
    "title": "CrisisShield",
    "summary": "TECHNICAL HIGHLIGHTS: Notable technical implementations include the use of TypeScript for enhanced code reliability and maintainability, and Next.js for optimized server-side rendering and performance. The AI component likely leverages machine learning algorithms to process and interpret data from various sources, providing users with actionable insights.",
    "description": "**What it does**\nCrisisShield is a comprehensive AI-powered crisis management platform that helps small businesses predict, prepare for, and recover from disasters. Core Capabilities: AI Threat Prediction: Analyzes real-time weather patterns, economic indicators, and regional risks to provide personalized threat assessments with probability scores and early warnings\nIntelligent Emergency Planning: Generates customized crisis response plans in seconds using AI, tailored to specific business types, locations, and situations with step-by-step actions\nRecovery Tracking: Guides businesses through 6 recovery stages with visual progress monitoring, milestone tracking, and operational capacity metrics\nCrisis Management: Provides real-time AI guidance during active emergencies with context-aware recommendations and\n\n**Inspiration**\nSmall businesses are the backbone of economies worldwide, yet they're the most vulnerable when disaster strikes. We were inspired by the devastating reality that 90% of small businesses in developing countries have no disaster preparedness plan, and 60% never reopen after a crisis. We saw families losing their life's work to floods, pandemics, and economic crashes‚Äînot because they didn't care, but because enterprise-grade crisis management tools were too expensive, too complex, and simply not designed for them. We wanted to change that. CrisisShield was born from a simple belief: every business deserves a fighting chance. Crisis preparedness shouldn't be a luxury reserved for large corporations. With AI and modern cloud technology, we can democratize access to life-saving tools and level t\n\n**How we built it**\nWe built CrisisShield using a modern, scalable tech stack designed for speed, reliability, and real-time performance: Frontend & Framework: Next.js 15 with React 18 and TypeScript for type-safe, server-side rendered pages\nReact Bootstrap and Bootstrap 5 for responsive, accessible UI components\nCustom CSS for brand-specific styling and animations AI & Intelligence: Cerebras Cloud SDK for lightning-fast AI processing and threat analysis\nCustom AI prompts for emergency plan generation and crisis guidance\nReal-time data aggregation from multiple threat intelligence sources Backend & Database: Supabase (PostgreSQL) for real-time database with row-level security\nCustom database schema with 8+ interconnected tables\nOptimized queries for fast threat detection and plan retrieval Authentication & Se\n\n**Challenges**\n1. AI Response Speed vs. Quality Challenge: Balancing fast AI responses with comprehensive, actionable plans\nSolution: Implemented Cerebras for ultra-fast inference while optimizing prompts for quality output 2. Real-time Threat Data Integration Challenge: Aggregating data from multiple sources with different formats and reliability levels\nSolution: Built a flexible data aggregation layer with fallback mechanisms and confidence scoring 3. Database Schema Complexity Challenge: Designing a schema that handles multiple crisis types, recovery stages, and business profiles\nSolution: Created a normalized schema with flexible JSON fields for extensibility while maintaining query performance 4. User Experience During Crisis Challenge: Making the interface simple enough to use during high-stress em\n\n**Accomplishments**\nTechnical Achievements: ‚úÖ Built a full-stack, production-ready application in hackathon timeframe\n‚úÖ Integrated cutting-edge AI (Cerebras) for real-time threat analysis and plan generation\n‚úÖ Implemented 8+ major features with real-time database synchronization\n‚úÖ Created a scalable architecture that can handle millions of users\n‚úÖ Achieved sub-second AI response times for emergency plan generation\n‚úÖ Designed and implemented a complex database schema with proper relationships User Experience: ‚úÖ Created an intuitive interface that works during high-stress situations\n‚úÖ Built mobile-responsive designs that work on any device\n‚úÖ Implemented accessibility features for inclusive design\n‚úÖ Designed clear visual progress tracking for recovery stages Impact Potential: ‚úÖ Addressed a real problem affecting\n\n**What we learned**\nTechnical Learnings: How to integrate and optimize AI models for real-time applications\nBest practices for real-time database design with Supabase\nEffective patterns for Next.js 15 server and client components\nStrategies for handling complex state management in crisis scenarios\nTechniques for building responsive dashboards with Bootstrap Product Learnings: The importance of simplicity during high-stress situations\nHow to balance feature richness with usability\nThe value of visual progress indicators for user motivation\nThe need for offline capabilities in disaster scenarios (future work) Domain Learnings: Deep understanding of crisis management workflows\nThe specific needs of small businesses in developing regions\nThe complexity of disaster recovery processes\nThe importance of funding acce\n\n",
    "prize": "Winner Finalist Certificate + Custom .xyz Domain",
    "techStack": "ai, bootstrap, next, react, typescript",
    "github": "https://github.com/AqilaRifti/CrisisShield",
    "youtube": "https://www.youtube.com/watch?v=oJmFA3dTxM0",
    "demo": "https://crisisshield.netlify.app/",
    "team": null,
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/crisisshield"
  },
  {
    "title": "SynapseHub",
    "summary": "TECHNICAL HIGHLIGHTS: SynapseHub employs a diverse tech stack, including Flask for backend development, Llama-3.3-70b for advanced AI processing, and Google Spreadsheets for dynamic data management. The integration of SMTP for communication and SQLite for local data storage further strengthens its functionality, while the responsive design built with HTML5, CSS3, and JavaScript ensures a smooth user experience.",
    "description": "**What it does**\nSynapseHub is a comprehensive platform where student entrepreneurs can transform ideas into reality through visibility, collaboration, and expert guidance.\n\n**Inspiration**\nI'm a 15-year-old student entrepreneur. I've experienced the frustration firsthand: üí≠ Hidden Potential: Ideas remain trapped in classrooms and small friend groups\nüîá Zero Visibility: No platform to reach beyond immediate circles\n‚ùå No Validation: Students can't tell if their ideas are viable or needed\nüë• Isolation: Finding co-founders with complementary skills is nearly impossible\nüìö Learning Gap: No structured way to develop entrepreneurship skills\nüòû Lost Motivation: Without feedback and support, brilliant concepts die silently The truth is simple: 90% of student startup ideas fail not because they‚Äôre bad, but because there‚Äôs no infrastructure to nurture them. We‚Äôre told to ‚Äúinnovate,‚Äù yet given no tools to actually do it.\n\n**Challenges**\nStyling Consistency: Fast development caused spacing and color mismatches; fixed in final hours\nTime Management: Prioritized core features over a full mentorship module\nResponsive Layout: With limited time, we focused on desktop stability; mobile required more restructuring\nFiltering Bugs: Filters weren‚Äôt updating dynamically ‚Üí JS rewrite\nRouting Conflicts: Flask template paths broke and required refactoring\nTime Pressure: We dropped several planned features to ship a solid MVP\n\n**What's next**\n‚óè Move from SQLite to PostgreSQL for larger data sets.\n‚óè Implement real-time chat using Flask-SocketIO for team collaboration.\n‚óè Deploy AI-powered idea matching to connect students with similar interests.\n‚óè Mobile-first design ‚Üí native Flutter app in phase 2.\n‚óè Long-term ‚Üí Partnership with edtech companies and incubators for funding &\ninternships.\n\n",
    "prize": "Winner Participation Prize; Winner Top 4-23",
    "techStack": "css3, flask, google-spreadsheets, groq, html5, javascript, llama-3.3-70b, python, smtp, sqlite, vs-code",
    "github": "https://github.com/aditya3272-arya/SynapseHub-Student-Entrepreneur-Platform/tree/main",
    "youtube": "https://www.youtube.com/watch?v=AQj3iniSYs8",
    "demo": "https://docs.google.com/document/d/1Ps2Ec83H0Q0rgds43Tf3m43ixBJ5lRnfdVX2XJGVA-k/edit?usp=sharing",
    "team": null,
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/synapsehub-w2gl6z"
  },
  {
    "title": "Pancake Pals",
    "summary": "TECHNICAL HIGHLIGHTS: Built with Dart and Flutter, Pancake Pals likely showcases smooth cross-platform functionality, allowing for a seamless user experience on both iOS and Android devices. Key implementations may include real-time communication features for virtual gatherings and an intuitive interface for recipe sharing and exploration.",
    "description": "**What it does**\nPancake Pals tracks your screen time and turns it into a daily challenge. The less time you spend on your phone, the more breakfast points you earn. You can level up your pancake buddy, check your friends‚Äô progress, and compete to stay off your phone the longest.\n\n**Inspiration**\nWe wanted to make screen time fun instead of guilty. Everyone has tried to ‚Äúuse their phone less‚Äù and failed ‚Äî so we thought, why not turn it into a game? Breakfast characters, competition with friends, and a cozy vibe felt like the perfect mix to motivate people without pressure.\n\n**How we built it**\nWe built the app using Flutter + Dart so it could run cross-platform. We used Android Usage Stats to read screen time, and Firebase for storing user progress and friend leaderboard data. All UI assets and icons were custom-designed to match the breakfast theme.\n\n**Challenges**\nIt was our first time working with flutter and dart, so it took us longer than expected to get set up with it. Screen time permissions were way harder than expected ‚Äî dealing with Android SDK versions, NDK errors, and manifest permissions took a long time. Syncing friend data without lag and getting the emulator to cooperate were also big battles.\n\n**Accomplishments**\nWe got real screen-time tracking working and fully connected it with the reward system and characters. The UI turned out super cozy and satisfying, and people who tested the app said it actually motivated them to stay off their phones.\n\n**What we learned**\nWe learned a ton about Android permission APIs, Flutter build configurations, and debugging emulator chaos. We also learned that small UI details make a huge difference in user motivation ‚Äî a cute pancake really does make you use your phone less.\n\n**What's next**\nWe want to add more breakfast characters, weekly tournaments, and streak bonuses. Long term, we‚Äôd love to launch on both Android and iOS and maybe bring in features like calming mini-games or phone-free quests.\n\n",
    "prize": "Winner Best Mobile Hack",
    "techStack": "dart, flutter",
    "github": "https://github.com/ashleychching/pancake_pals",
    "youtube": "https://www.youtube.com/watch?v=OJWKgV2G9w4",
    "demo": "https://www.figma.com/proto/IIV5EcP56f20XcrtNjz9Pe/Pancake-Pals?node-id=1-4&t=fINy9LKeaYhgkVtP-0&scaling=scale-down&content-scaling=fixed&page-id=0%3A1&starting-point-node-id=1%3A4",
    "team": "Ashley Chan, Eddie Matthews, Yasmin",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/pancake-pals"
  },
  {
    "title": "Night Shift",
    "summary": "TECHNICAL HIGHLIGHTS: Night Shift utilized advanced libraries such as Gymnasium for creating the training environment, NumPy for numerical operations, and Stable-Baselines3 for implementing state-of-the-art reinforcement learning algorithms. The integration of Pygame likely provided an engaging visual interface for the simulation, enhancing user interaction and feedback.",
    "description": "**What it does**\nNight Shift is a \"Digital Twin\" of an ICU in crisis. It has two modes: \"Retro\" Mode: A top-down Pygame simulator where you, the human, try to manage the ICU. You must manually allocate nurses, beds, and ventilators to save patients as their severity increases.\n \"Modern\" AI Mode: We run the exact same simulation, but this time it's controlled by an AI we trained using reinforcement learning. It acts as a Decision Support System, executing the optimal strategy it learned from millions of simulations to save the maximum number of lives. It's a \"flight simulator for nurses\" that doubles as a powerful planning tool for hospital administrators.\n\n**Inspiration**\nWe were inspired by the intense, high-stakes decisions that hospital staff make every single night. A charge nurse on the \"Night Shift\" is constantly playing a game of high-stakes triage: who gets the last ventilator? Who gets the only available nurse? This human-driven optimization is incredibly difficult. We wanted to see if a modern AI, trained on millions of scenarios, could discover optimal, life-saving strategies that even the most experienced human might miss.\n\n**How we built it**\nThe project is built entirely in Python with two main components: The Simulation Engine: A core SimICU class that models all the logic: patient arrivals (Poisson process), severity degradation, and resource constraints.\nThe AI Environment: We wrapped our engine in a Gymnasium-compatible environment. This included a normalized observation_space (patient severity, wait times, free resources) and a MultiDiscrete action space (patient_id, action_type).\nThe AI Agent: We used Stable-Baselines3 to train a Proximal Policy Optimization (PPO) agent on our environment.\nThe \"Retro\" UI: We used Pygame to build the human-playable \"Retro\" game, which calls the same underlying simulation engine.\n\n**Challenges**\nThe AI did not work. At all. Our first agent had a 1.1% success rate‚Äîit was acting randomly and everyone was lost. The biggest challenge was debugging the environment (the teacher), not the agent (the student). State Normalization: The AI saw numbers like tick=1000 and free_beds=5 and thought the tick count was 200x more important. We had to normalize all state inputs to a [0, 1] range so it could understand the relative importance of each factor.\nReward Shaping: A single +100 reward for a save wasn't enough; the AI didn't know which of its 500 actions led to the win. We had to add frequent, small penalties for every tick a patient waited, teaching it a sense of urgency.\nInvalid Actions: The agent wasted thousands of steps trying to act on non-existent patients. We had to add large, immedi\n\n**Accomplishments**\nWe're incredibly proud of turning that 1.1% success rate into a high-performing agent. Debugging an RL environment is notoriously difficult, and we successfully navigated state normalization and complex reward shaping. Most importantly, we're proud of building a project that is more than a \"game\"‚Äîit's a high-fidelity \"Digital Twin\" that can be directly applied to a real-world healthcare problem.\n\n**What we learned**\nWe learned that \"DigitalTwins\" are incredibly powerful. Our \"Retro\" sim isn't just a game; it's a baseline for human performance. Our \"Modern\" AI isn't just an opponent; it's a Decision Support System. By building this, we learned how to model a complex, real-world system and, more importantly, how to teach an AI to master it. The challenges taught us that the quality of the environment and the clarity of the reward are far more important than the agent's algorithm.\n\n**What's next**\nThis is just the beginning. The next steps are focused on real-world deployment: Explainable AI (XAI): Build a dashboard that shows why the AI is making its recommendations in plain English. A doctor will never trust a black box, so we need to add transparency.\n Higher Fidelity: Deepen the model by adding patient archetypes (e.g., cardiac vs. respiratory) and specialized resources, making the simulation even more realistic.\n EMR Integration: Create a mock-up showing how Night Shift could plug into a real hospital's EMR system and provide live, actionable recommendations to staff.\n\n",
    "prize": "Winner Best Machine Learning Hack",
    "techStack": "gymnasium, numpy, pygame, python, stable-baselines3",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=nKaOxXwdDYI",
    "demo": null,
    "team": "Justin Zhang, courteneyS Sit",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/night-shift"
  },
  {
    "title": "AccessiGesture",
    "summary": "TECHNICAL HIGHLIGHTS: AccessiGesture utilizes advanced libraries such as Mediapipe for real-time hand tracking and OpenCV for image processing, ensuring accurate gesture recognition. The integration of PyInstaller allows for easy deployment of the application, while Tkinter provides a user-friendly interface, making the technology accessible to a broader audience.",
    "description": "**What it does**\nAccessiGesture runs as a lightweight, unobtrusive application that translates a user's hand movements into direct, real-time cursor control. We meticulously designed a set of gestures to be both ergonomic and comprehensive, allowing for the full spectrum of mouse operations. Cursor Movement: An open hand gesture ([1, 1, 1, 1, 1]) puts the system into \"mouse mode.\" The application maps the hand's position within a user-defined \"active zone\" on the screen to the entire desktop, allowing for smooth, 1-to-1 cursor movement.\nLeft Click & Drag: A pinch between the thumb and index finger triggers a \"left-click-down\" event. This was a critical design choice: by holding this pinch, the user can naturally drag files, highlight text, or interact with any drag-and-drop interface. Releasing the pinch i\n\n**Inspiration**\nIn a world that runs on digital interaction, the mouse and keyboard remain a fundamental barrier for millions. We were inspired by the daily challenge faced by individuals with motor disabilities, arthritis, repetitive strain injuries (RSI), or other conditions that make using a traditional mouse difficult or even painful. We asked ourselves: why should digital accessibility be a luxury, gated behind expensive, specialized hardware? Our project, AccessiGesture, is our answer. It‚Äôs built on the philosophy that accessibility should be built-in, not bolted-on. We envisioned a tool that could transform any standard webcam‚Äîa device already built into most computers‚Äîinto a high-fidelity, intuitive, and ergonomic gesture controller. We wanted to create something that didn't just work, but felt em\n\n**How we built it**\nThe project was built entirely in Python, which served as the \"glue\" for a stack of powerful, specialized libraries, each chosen for a specific purpose to create a seamless, real-time experience. OpenCV: This was our high-speed, low-latency \"eye.\" We used it for the initial camera capture, frame-by-frame processing, and image manipulation. Crucially, OpenCV also served as our visual debugging tool, allowing us to draw the hand landmarks and our program's current \"state\" (e.g., \"PINCH\") directly onto the video feed.\n Google MediaPipe (Hands): This is the core machine-learning engine and the \"brain\" of our recognition. We leveraged its incredibly high-fidelity, pre-trained model, which provides 21 distinct 3D landmarks for the hand in real-time. This level of detail is what allowed us to mov\n\n**Challenges**\nOur development process was a story of hitting walls and finding breakthroughs. Our first challenge was Framework Selection. We initially explored a sophisticated web-based solution using Next.js, drawn to the idea of a modern, browser-based tool. However, we quickly ran into the sandboxed limitations of the web. Gaining the simple, direct, low-latency access to the webcam and, more importantly, the system-level cursor control we needed was a massive, complex hurdle. We made the hard pivot to a native Python application, which proved to be the right decision, offering us the raw power and simplicity we needed. Our primary technical hurdle was Gesture-Clash Resolution. Our first prototypes were a mess of false positives. A \"fist\" might be misread as a \"thumbs-up.\" A hand moving into positio\n\n**Accomplishments**\nThis project was a journey of firsts, but a few accomplishments stand out: The Visual Debugger: We are incredibly proud of the real-time diagnostic overlay. By drawing our program's state (gesture_detected, fingers_list) onto the camera feed, we created an essential engineering tool that allowed us to see our logic working (or failing) and iterate with incredible speed.\nThe Standalone App: We didn't just build a script. We built a full desktop application with a robust UI, packaged as a standalone executable. This was a huge step in learning how to deliver a real product to users.\nMastering the Stack: We dove headfirst into a professional-grade development environment, learning to integrate complex libraries like OpenCV and MediaPipe within VS Code.\nThe Ultimate Test: Beating *osu!* We kne\n\n**What we learned**\nThis project was a masterclass in the architecture of real-time computer vision applications. Stateful Program Design: We learned the critical importance of managing \"state.\" A user's intent is not just in a single frame; it's a process. We had to build logic to track is_pinching vs. was_pinching to distinguish between a discrete \"click\" and a continuous \"drag.\"\n Human-Computer Interaction (HCI): We learned that gesture selection is a deep, complex trade-off. A gesture must be intuitive for the user, ergonomic to hold, and mathematically unambiguous for the machine. This core conflict is the central challenge of HCI design.\n Performance Profiling: We learned that in real-time apps, \"good enough\" code isn't. Every line matters. We learned how to hunt for performance bottlenecks and understa\n\n**What's next**\nTrainable Models: Our next big leap is to move from our hard-coded (heuristic) gesture rules to a proper machine-learned model. This would not only make detection more robust but would also allow users to train the app to recognize their own unique set of custom gestures.\nExpanding the Vocabulary: Right now, we only speak \"mouse.\" We want to expand AccessiGesture's features to include keyboard shortcuts (like Ctrl-C, Ctrl-V) and a pop-up virtual keyboard, transforming it from a mouse replacement into a complete, hands-free operating system navigator.\n\n",
    "prize": "Winner Future Startup",
    "techStack": "mediapipe, opencv, pyinstaller, python, tkinter",
    "github": "https://github.com/dlt87/Accesigesture",
    "youtube": "https://www.youtube.com/watch?v=bQuyfCt01F8",
    "demo": null,
    "team": "nathan liou, David Ting, Payton Yu, Alejandro Imperial",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/accessigesture"
  }
]